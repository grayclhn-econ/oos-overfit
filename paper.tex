\documentclass[11pt]{article}
%\linespread{1.2}\selectfont
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{fit}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage[round,comma]{natbib}
\usepackage{setspace,amsmath,amssymb,amsthm,ifthen,subfig,url,multicol}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\thepage}\chead{}\rhead{}
\lfoot{}\cfoot{}\rfoot{}
\renewcommand\headrulewidth{0pt}
\onehalfspacing
\usepackage[margin=1in]{geometry}
\usepackage{nath}

\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{claim}[thm]{Claim}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{asmp}{Assumption}
\newtheorem{example}{Example}
\newtheorem{defn}{Definition}

\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator*{\plim}{plim}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\atan}{atan*}

\newcommand{\citepos}[1]{\citeauthor{#1}'s \citeyearpar{#1}}
\newcommand{\Citepos}[1]{\Citeauthor{#1}'s \citeyearpar{#1}}

%% from http://blog.blubinc.net/
% Define a command for a given term. This will create a command which gives
% the long name, with a parenthesised acronym, on the first call.
% Additionally it will redefine itself to call the acronym function instead
% on subsequent executions.
%
% Takes:
%	1, command name
%	2, long name
%	3, acronym
% ie \newacronym{ols}{Ordinary Least Squares}{OLS} will define the
% command \ols 
\newcommand{\newacronym}[3]{%
\expandafter\newcommand\expandafter{\csname #1\endcsname}{#2 \textsc{(\MakeLowercase{#3})}%
\expandafter\renewcommand\expandafter{\csname #1\endcsname}{\textsc{\MakeLowercase{#3}}}}}

\newcommand{\oosA}{\bar{D}_{R}}
\newcommand{\oosB}{\bar{D}_{T}}

\newcommand{\oost}{\ensuremath{\operatorname{oos-t}}}
\newcommand{\oosSum}[2]{\ensuremath{\sum_{#1=R-\h+#2}^{T-\h}}}

\newcommand{\h}{h}
\newcommand{\InnerBlockL}[1]{\ensuremath{R+(#1-1)b_T + 1}}
\newcommand{\InnerBlockU}[1]{\ensuremath{R+#1 b_T}}
\newcommand{\OuterBlockU}{\lfloor \frac P{b_T} \rfloor}
\newcommand{\SumOuterBlock}[1]{\ensuremath{\sum_{#1=1}^{\OuterBlockU}}}
\newcommand{\SumInnerBlock}[2]{\ensuremath{\sum_{#1=\InnerBlockL{#2}}^{\InnerBlockU{#2}}}}

\newcommand{\CEOuterBlock}[2]{\ensuremath{\E_{\InnerBlockU{(#2-1)}} #1}}

\newcommand{\CenteredAverage}{\frac{\oosA - \E_T\oosT}{\hat\sigma / \sqrt{P}}}
\newcommand{\CenteredAverageI}{\ensuremath{\frac{1}{\hat\sigma
    \sqrt{P}}\oosSum{t}{1} (D_{tR} - \E_R D_{tR})}}
\newcommand{\CenteredAverageII}{\ensuremath{\frac{\sqrt{P}}{\hat\sigma}
    (\E_R\oosA - \E_T\oosB)}}

\newcommand{\MixingaleArray}[1]{\ensuremath{}}

\newcommand{\ZSummand}{D_{sR} - \E_R D_{sR}}
\newcommand{\ZDef}{ \frac1{\sqrt{P}} \SumInnerBlock{s}{i} [\ZSummand]}
\newcommand{\ZSqCE}{\ensuremath{\CEOuterBlock{Z_i^2}{i}}}
\newcommand{\ZTrunc}{\ensuremath{C\sqrt{b_{T}/P}}}
\newcommand{\PScaleTerm}{\ensuremath{\frac{C \sqrt{b_T}}{P}}}

\newcommand{\vWeight}{W((t-s)/\gamma)}
\newcommand{\vSummand}{\ensuremath{(D_{tR} - \E_R D_{tR})(D_{sR} - \E_R D_{sR}) \vWeight}}
\newcommand{\kernelBDefn}[1]{\ensuremath{\frac1{\delta\sqrt{2\pi}}e^{-\frac{#1^2}{2\delta^2}}}}
\newcommand{\kernelB}[1]{\ensuremath{\eta_{\delta}(#1)}}

\newcommand{\vtSum}{\ensuremath{\sum_{t=-P+1}^{2P}}}
\newcommand{\vtSumr}{\ensuremath{\sum_{i=1}^r}}
\newcommand{\vtSuma}{\ensuremath{\sum_{t=(2i-2)b_T-P+1}^{(2i-1)b_T-P}}}
\newcommand{\vtSumb}{\ensuremath{\sum_{t=(2i-1)b_T-P+1}^{2ib_T-P}}}
\newcommand{\vttLower}{\ensuremath{\max(1-t,-b_T)}}
\newcommand{\vttUpper}{\ensuremath{\min(P-t,b_T)}}

\newcommand{\varianceTermIDefn}{\ensuremath{\frac1{P}\oosSum{s,t}{1}
    \vSummand}}
\newcommand{\varianceTermI}{\ensuremath{\sigma_{0\delta}^2}}
\newcommand{\varianceTermII}{\ensuremath{\sigma_{1\delta}^2}}
\newcommand{\varianceTermIIa}{\ensuremath{\frac1{\sqrt{P\gamma}}
    \sum_{l=\max(1-t,-P)}^{\min(P-t,P)} (D_{t+l+R,R} - \E_R D_{t+l+R,R})
    W(l/\gamma)}}
\newcommand{\varianceTermIIb}{\ensuremath{\frac1{\sqrt{P\gamma}}
    \sum_{j=1-t}^{P-t} (D_{t+j+R,R} - \E_R D_{t+j+R,R})
    \kernelB{j/\gamma}}}
\newcommand{\varianceTermIII}{\ensuremath{\sigma_{2\delta}^{2}}}
\newcommand{\varianceTermIIIa}{\ensuremath{\frac1{\sqrt{P\gamma}}
    \sum_{l=\vttLower}^{\vttUpper} (D_{t+l+R,R} - \E_R D_{t+l+R,R})
    W(l/\gamma)}}
\newcommand{\varianceTermIV}{\ensuremath{\sigma_{3\delta}^{2}}}
\newcommand{\varianceTermIVb}{\ensuremath{\frac1{\sqrt{P\gamma}}
    \sum_{j=\vttLower}^{\vttUpper} (D_{t+j+R,R} - \E_R D_{t+j+R,R})
    \kernelB{j/\gamma}}}
\newcommand{\vtIIIsummand}{\ensuremath{(Z_{1t}Z_{2t} - \E_RZ_{1t}Z_{2t})}}

\newcommand{\varianceDiffA}{\ensuremath{\frac1{P} \oosSum{s,t}{1}
    \lvert (D_{tR} - \E_R D_{tR})(\E_R D_{sR} - \oosA) \rvert
    \vWeight}}
\newcommand{\varianceDiffAi}{\ensuremath{\frac1P \oosSum{s}{1}(\E_R
    D_{sR} - \oosA )^2}}
\newcommand{\varianceDiffAii}{\ensuremath{\frac1P \oosSum{s}{1}
    (D_{sR} - \E_R D_{sR})^2}}

\newcommand{\rhoExp}{\ensuremath{\frac{\rho-2}{2\rho}}}
\newcommand{\absReg}{\ensuremath{\frac\rho{\rho-2}}}

\newcommand{\couplingConstant}{\ensuremath{2^{\frac{1+\rho}{\rho}} B_L}}
\newcommand{\couplingBeta}[1]{\ensuremath{\beta^{\frac{\rho-2}{2\rho}}_{#1}}}
\newcommand{\couplingBetaSq}[1]{\ensuremath{\beta^{\frac{\rho-2}{\rho}}_{#1}}}
\newcommand{\couplingBound}[1]{\couplingConstant \couplingBeta{#1}}

\newcommand{\OCoef}{\ensuremath{O_p(\frac{\sqrt{P}}{T})}}

\newcommand{\bh}[1]{\ensuremath{\hat\theta_{#1}}}
\newcommand{\ep}[1]{\ensuremath{\boldsymbol{\varepsilon}_{#1}}}

\newcommand{\olsMatrix}[2]{\ensuremath{(#1'#1)^{-1}#1'#2}}

\newcommand{\dmw}{\textsc{dmw}}
\newcommand{\ols}{\textsc{ols}}
\newcommand{\mds}{\textsc{mds}}
\newcommand{\clt}{\textsc{clt}}
\newcommand{\lln}{\textsc{lln}}
\newacronym{VAR}{Vector Autoregression}{VAR}
\newacronym{oos}{out-of-sample}{OOS}
\newcommand{\dgp}{\textsc{dgp}}
\newcommand{\hac}{\textsc{hac}}
\newacronym{ned}{Near Epoch Dependent}{NED}
\newacronym{mse}{Mean Squared Error}{MSE}
\newcommand{\sfield}{$\sigma$-field}
\newcommand{\sfields}{\sfield s}
\renewcommand{\Re}{\ensuremath{\mathbb{R}}}

\frenchspacing \title{Out-of-Sample Comparisons of Overfit Models}
\author{Gray Calhoun\thanks{email: \texttt{gcalhoun@iastate.edu}. I
    would like to thank Julian Betts, Helle Bunzel, Marjorie Flavin,
    Nir Jaimovich, Lutz Kilian, Michael McCracken, Seth Pruitt, Ross
    Starr, Yixiao Sun, Allan Timmermann, participants at the 2010
    Midwest Economics Association Annual Meetings, the 2010
    International Symposium on Forecasting, the 2010 Joint
    Statistical Meetings, and in many seminars at \textsc{ucsd}, and
    espeically Graham Elliott for their valuable suggestions, feedback
    and advice in writing this paper.  I would also like to thank Amit
    Goyal for providing computer code and data for his 2008 RFS paper
    with Ivo Welch \citep{GoW:08}.} \\ Iowa State University,
  Economics Department}
\date{October 24, 2011}

\begin{document}
\maketitle

\begin{abstract}\thispagestyle{empty}\noindent
  This paper uses dimension asymptotics to study why overfit linear
  regression models should be compared out-of-sample; we let the
  number of predictors used by the larger model increase with the
  number of observations so that their ratio remains uniformly
  positive.  Under this limit theory, the naive Diebold-Mariano-West
  out-of-sample test can test hypotheses about a key quantity for
  evaluating forecasting models---a time series analogue to the
  generalization error---as long as the out-of-sample period is small
  relative to the total sample size.  Moreover, tests that are
  designed to reject if the larger model is true, such as the usual
  in-sample Wald and \textsc{lm} tests and also Clark and McCracken's
  (2001, 2005a), \citepos{Mcc:07}, and Clark and West's (2006, 2007)
  out-of-sample statistics, will choose the larger model too often
  when the smaller model is more accurate.

\noindent \textsc{jel} Classification: C01, C12, C22, C52, C53

\noindent Keywords: Generalization Error, Forecasting, Model
Selection, t-test, Dimension Asymptotics
\end{abstract}
\newpage

\section{Introduction}\label{sec:introduction}
Consider two sequences of length $P$ of prediction errors, the result
of forecasting the same variable with two different estimated models.
Both models are estimated with $R$ observations, collectively called
the {\em estimation window}, and are used to forecast an additional
$P$ observations, called the {\em test sample}.  There are $T$
observations in all, and $R+P=T$.  This paper introduces a new limit
theory for statistics constructed from these prediction errors
designed to approximate the behavior of the statistics when one of the
models is overfit.  In doing so, we provide a theoretical
justification for forecasters to use \oos\ instead of
in-sample comparisons.

Although \oos\ comparisons have been popular in macroeconomics and
finance since \citepos{MeR:83} seminal study of exchange rate models,
it is unclear from a theoretical perspective whether or not the
statistics are useful.  Empirical researchers often cite ``overfit''
or ``instability'' as reasons for using \oos\ comparisons, as in
\citet{StW:03}, but neither term is precisely defined or formalized.
Compounding this problem, the asymptotic distributions of these
statistics are derived under conditions that rule out either
instability or overfit and allow a researcher to use a conventional
in-sample comparison---a variation of the F-test, for example.  As
\citet{InK:04} argue, the statistics themselves are designed to test
hypotheses that can be tested by these in-sample statistics.  For
example, \citet{DiM:95} and \citet{Wes:96} derive the limiting
distributions of many popular \oos\ test statistics under conditions
that would justify these full-sample tests.\footnote{In this paper, we
  will refer to their basic \oos\ $t$-test as the \dmw\ test.}  Much
of the subsequent research by \citet{Mcc:00, Mcc:07}, \citet{CCS:01},
\citet{ClM:01,ClM:05}, \citet{CoS:02,CoS:04}, \citet{ClW:06,ClW:07},
\citet{Ana:07}, and others relaxes several of Diebold and Mariano's
and West's assumptions, but maintains the stationarity and dependence
conditions that permit in-sample comparisons.\footnote{Like us,
  \citet{Ana:07} allows the number of regressors to increase with $T$.
  But in that paper, they increase slowly enough that the \ols\
  coefficients are consistent and asymptotically normal.}
\citet{GiW:06}, and \citet{GiR:09, GiR:10} are exceptions.  Instead of
focusing on hypotheses that can be tested by in-sample comparisons,
\citet{GiW:06} derive an \oos\ test for the null hypothesis that the
difference between two models' \oos\ forecasting performance is
unpredictable, a martingale difference sequence; \citet{GiR:09} test
whether the \oos\ forecasting performance of a model suffers from a
breakdown relative to its in-sample performance; and \citet{GiR:10}
test whether the forecasting performance is stable. However, those
papers focus on a particular \oos\ estimation strategy and do not
address why naive \oos\ comparisons might be generally useful.

Since in-sample and \oos\ statistics require similar assumptions and
test similar hypotheses, one might expect that they would give similar
results.  They do not.  Generally in-sample analysis supports more
complicated theoretical models and \oos\ analysis supports simple
benchmarks, as seen in \citet{MeR:83}, \citet{StW:03}, and
\citet{GoW:08}.  Since these different approaches strongly influence
the outcome of research, it is important to know when each is
appropriate.  The explanations in favor of \oos\ comparisons claim
that they should be more robust to unmodeled instability
\citep{ClM:05,GiW:06,GiR:09,GiR:10} or to overfit
\citep{Mcc:98,Cla:04}.  Both explanations presume that the in-sample
comparison is invalid and the \oos\ comparisons are more reliable.  Of
course, as \citet{InK:04,InK:06} point out, both in-sample and \oos\
methods could be valid, but the \oos\ methods could have lower power.

In this paper, we study the ``overfit'' possibility and leave
``instability'' to future research. This paper uses dimension
asymptotics to study the behavior of \oos\ comparisons when at least
one of the models is overfit---when the number of its regressors
increases with the number of observations so that their ratio remains
positive.  We focus on linear regression models estimated with a fixed
training window, but our basic conclusions should be true for other
estimation strategies and models as well.  Under this asymptotic
theory, the \ols\ coefficient estimator is no longer consistent or
asymptotically normal \citep{Hub:73} and has positive variance in the
limit.  Even so, we show that the usual \oos\ average is
asymptotically normal and can consistently estimate the difference
between the models' expected loss in periods $T+1,T+2,\dots$
conditional on the on the data available in period $T$, a time-series
analogue to the generalization error.\footnote{See, for example,
  \citet{HTF:08}.}  Under these asymptotics, the generalization error
does not converge to the expected performance of the pseudo-true
models, so the in-sample and \oos\ comparisons measure different
quantities and should be expected to give different results for
reasons beyond simple size and power comparisons.  Under our theory,
the model that is closer to the true \dgp\ in population can forecast
worse.  In such a situation, an in-sample comparison would correctly
reject the null hypothesis that the benchmark is true, and an \oos\
comparison would correctly fail to reject the null that the benchmark
is more accurate.\footnote{In a pair of papers similar to ours,
  \citet{ClM:09,ClM:09b} study in-sample and \oos\ tests that the
  larger model has nonzero coefficients that are too close to zero to
  expect the model to forecast more accurately.  Like this paper, they
  argue that the larger model can be true but less accurate.  However,
  they focus on an aspect of the \dgp\ that makes this phenomenon
  likely, while we focus on the coefficient estimates that produce
  less accurate forecasts.}

This result justifies \oos\ comparisons when researchers want to
choose a model for forecasting.  Although there has been little
emphasis on hypothesis testing in this setting, testing is usually
appropriate: there is usually a familiar benchmark model in place, and
the cost of incorrectly abandoning the benchmark for a less accurate
alternative model is higher than the cost of incorrectly failing to
switch to a more accurate alternative.\footnote{These costs can be
  monetary, in the form of computer upgrades, software installation,
  and training, or they can be reputational costs.}
We show that the \dmw\ test lets the forecaster control the
probability of the first error, just as with conventional hypothesis
testing.  

Since the models' coefficients are imprecisely estimated in the limit,
the test sample must be small to ensure that the model estimated over
the training sample is similar to the one that will be estimated over
the full sample.  In particular, $P/T \to 0$ is required for
consistent estimation, and $P^2/T \to 0$ is required for valid
confidence intervals and inference.  For larger $P$, the \oos\
comparisons remain asymptotically normal, but are centered on the
forecasting performance associated with the period-$R$ estimates.  In
practice, researchers typically use large values of $P$, so these
studies may be too pessimistic about their models' future accuracy if
they use the \dmw\ test.  Section~\ref{sec:oostheory} lays out the
asymptotic behavior of the \dmw\ test under this limit theory.

Since the \ols\ coefficient estimator is no longer consistent or
asymptotically normal, the F-test and Wald tests are no longer
asymptotically valid.  Moreover, \citet{Efr:86,Efr:04} shows that
naive comparisons of in-sample loss overestimate models' accuracy by a
factor proportional to $K/T$, $K$ being the number of regressors.  But
modifications of the F-test are valid under this asymptotic theory, as
shown by \citet{BoB:95}, \citet{AkA:00}, \citet{AkP:04}, and
\citet{Cal:10}, among others.  Such results establish that hypotheses
about the parameters of the \dgp\ can be tested in-sample even if one
or both models are overfit.  For that very reason, these tests can not
reliably choose a model for forecasting and will reject the benchmark
too often.  We show this analytically in Section~\ref{sec:insample}.
Moreover, under this asymptotic theory, many recent \oos\ test
statistics, such as those derived by \cite{ClM:01,ClM:05},
\citet{Mcc:07}, and \citet{ClW:06,ClW:07} should behave like the
F-test.  These tests are also designed to reject the benchmark when
the alternative model is true, and so they may reject too often when
the benchmark is false but more accurate.  Obviously, since the
distribution of these statistics converges to the normal when $P/T \to
0$ (with the number of regressors fixed), these statistics behave like
the \dmw\ test when $P$ is small, but should over-reject the benchmark
when $P$ is large.  Section~\ref{sec:mc} presents simulations that
support these claims.

Finally, this paper also introduces a new method of proof for \oos\
statistics.  We use a coupling argument (Berbee's Lemma, 1979) to show
that sequences of \oos\ loss behave like mixingales when the
underlying series are absolutely regular, even if the forecasts depend
on non-convergent estimators.  Moreover, transformations of these
processes also behave like mixingales, so asymptotic results for \ned\
functions of mixing processes can be used for these \oos\ processes
with only slight modification.  Although we only present a proof that
this mixingale result holds for the fixed-window scheme, it is simple
to extend it to other estimation windows.

The rest of the paper proceeds as follows.
Section~\ref{sec:assumptions} introduces our notation and assumptions.
Section~\ref{sec:theory} gives the main theoretical results for the
\dmw\ \oos\ test and shows that the F-test rejects the benchmark model
too often when it is false but more accurate than the
alternative. Section~\ref{sec:mc} presents a Monte Carlo study
supporting our theoretical results.  Section~\ref{sec:empirics}
applies the \oos\ statistic to \citepos{GoW:08} dataset
for equity premium prediction, and Section~\ref{sec:conclusion}
concludes.  Proofs and supporting results are listed in the Appendix.

\section{Setup and Assumptions}\label{sec:assumptions}
We will set up our models and notation first, then list the assumptions
required for our results.  There are two competing linear models that
give forecasts for the target, $y_{t+\h}$:
\[
y_{t+\h} = x_{1t}'\theta_1 + \varepsilon_{1,t+h}
\]
and
\[
y_{t+\h} = x_{2t}'\theta_2 + \varepsilon_{2,t+h};
\]
$\h$ is the forecast horizon, and the variables $y_t$, $x_{1t}$, and
$x_{2t}$ are all known in period $t$.  Let
\begin{equation*}
  \mathcal{F}_t = \sigma(y_1, x_1, \dots, y_t, x_t)
\end{equation*}
with $x_t$ the vector of all elements of $x_{1t}$ and $x_{2t}$ after
removing duplicates, and let $\E_t$ and $\var_t$ denote the
conditional mean and variance given $\mathcal{F}_t$.  The first model
uses $K_1$ regressors, and the second uses $K_2$.  Without loss of
generality, assume that $K_1 \leq K_2$.  We will allow $K_1$ and $K_2$
to vary with $T$, so a stochastic array underlies all of our
asymptotic theory, but we suppress that notation to make the
presentation more clear.

Although our theory can apply to many different \oos\ statistics,
we'll only present results for the \dmw\ \oos\ $t$-test.  Both models
are estimated using \ols\ over the training sample:
\[
\hat y_{i,t+\h} = x_{it}'\bh{iR}, \quad i=1,2, \quad t = R+1,\dots,T-h
\]
with
\[
\bh{it} = (\sum_{s=1}^{t-\h} x_{is}x_{is}')^{-1} \sum_{s=1}^{t-\h} x_{is} y_{s+\h},
\quad i=1,2.
\]
The \oos\ average is denoted $\oosA$:
\[
\oosA \equiv P^{-1} \oosSum{t}{1} D_{tR}
\quad \text{with} \quad D_{tS} \equiv L(y_{t+\h} - x_{1t}'\bh{1S}) -
L(y_{t+\h} - x_{2t}'\bh{2S}),
\]
$P = T - R$ and $L$ a known loss function.  The \oos\ statistic
of interest is 
\[
\oost \equiv \frac{\sqrt{P} \oosA}{\hat\sigma},
\]
for $\hat\sigma^2$ some (possibly Heteroskedasticity- and
Autocorrelation-Consistent, or \hac) estimator of the asymptotic
variance of $\oosA$.

We'll show that \oost\ can be used to test whether the models will
forecast equally accurately in the future.  Specifically, assume that
whichever model is chosen will be used for an additional $Q$ periods
in the future and let $\oosB$ denote the difference in average
performance over those periods:
\[
\oosB \equiv Q^{-1} \sum_{t=T+1}^{T+Q} D_{tT}.
\]
Note that $\oosA$ depends on the training sample estimators,
$\hat\theta_{iR}$, and $\oosB$ depends on the full sample
estimators, $\hat\theta_{iT}$.  A forecaster has access to the
information set $\mathcal{F}_T$ when deciding between the two
forecasting models, but $\mathcal{F}_T$ does not contain $\oosB$.
So a forecaster should decide between the models based on the
conditional expectation, $\E_T \oosB$. If this quantity is
positive, the first model is expected to perform worse in the future,
and if it is negative the second is.

When the underlying series is i.i.d., this random variable, $\E_T
\oosB$, is equal to the difference in the models' generalization
error.  The generalization error of model $i$ is defined as
\begin{equation*}
  \E_{T} L(y_{T+\h}^{*} - x_{iT}^{*\prime}\hat{\theta}_{iT})
\end{equation*}
where $(y_{T+\h}^{*},x_{iT}^{*})$ has the same joint distribution as
$(y_{T+\h}, x_{iT})$ but is independent of $\mathcal{F}_T$.  Note that
$\hat{\theta}_{iT}$ is known in period $T$, so the generalization
error conditions on the observed coefficient estimates.  With
dependence, the coefficient estimates may be informative about future
values of $y_{t+\h}$ and $x_{it}$, but this influence dies out as $Q$
grows.  Conditional performance measures like these are especially
appropriate in areas like time-series economics where the forecaster
knows the coefficient values when choosing a model, but would be less
appropriate if the models will be re-estimated over an entirely new
dataset before forecasting.

Before stating the main theoretical assumptions, we define some more
notation.  The $l_v$-norm for vectors in $\Re^p$ (with $p$ arbitrary)
is denoted $\lvert \cdot \rvert_v$, and the $L_v$-norm for
$L_v$-integrable random variables is $\lVert \cdot \rVert_v$.  The
functions $\lambda_i(\cdot)$ take a square-matrix argument and return
its $i$th eigenvalue (with $\lambda_{i}(A) \leq \lambda_{i+1}(A)$ for
any matrix $A$).
All limits are taken as $T \to \infty$ unless stated otherwise.
 
We assume the next conditions hold throughout the paper.  The first
assumption controls the dependence of the underlying random array.
The second prevents the sequence of experiments from becoming
degenerate as $T$ increases.  The third assumption controls the
smoothness of the loss function and bounds the moments of the
difference in the models' performance; the fourth assumption describes
the behavior of the estimation and test windows.  The last assumption
describes the kernel used to estimate the \oos\ average's asymptotic
variance.

\begin{asmp}\label{asmp-1} 
  The random array $\{y_t,x_t\}$ is stationary and
  absolutely regular with coefficients $\beta_j$ of size
  $-\rho/(\rho-2)$; $\rho$ is greater than two and discussed further
  in Assumption \ref{asmp-3}.  Also, $K_1$ and $K_2$ are less than $R$,
  and $\frac{K_2}{T}$ and $\frac{K_2-K_0}{T}$ are uniformly positive;
  $K_0$ is the number of regressors shared by the two models ($K_1/T$
  can be uniformly positive as well, but is not required to be).
\end{asmp}

The first assumption is a standard condition on the dependence of the
underlying stochastic array.  We assume that the array is absolutely
regular instead of strong mixing, which is more common in
econometrics, because absolute regularity admits a particular coupling
argument \citep{Ber:79} that is unavailable for strong mixing
sequences.  Please see, for example, \citet{Dav:94} for a discussion
of these mixing conditions.

Assumption \ref{asmp-2} rules out uninteresting cases where the
forecast error vanishes.
\begin{asmp}\label{asmp-2}
  The variance of $y_{t+\h}$ given $\mathcal{F}_t$ is uniformly
  positive and finite, and all of the eigenvalues of the covariance
  matrix of $x_t$ are uniformly positive and finite as well.  The
  Euclidean norms of the pseudo-true coefficients, $\theta_1$ and
  $\theta_2$, satisfy $|\theta_1|_2 = O(1)$ and $|\theta_2|_2 = O(1)$.
  Moreover, there exist a $\delta > 0$ and finite $\Delta$ such that
  \begin{equation*}
    \lVert \lambda_{\max}(X_{iS_{T}}'X_{iS_{T}}) \rVert_{3+\delta}
    \leq S_{T}\, \Delta \\
    \lVert
    \lambda_{\max}(X_{iS_{T}}'\ep{iS_{T}}\ep{iS_{T}}'X_{iS_{T}})
    \rVert_{3+\delta} \leq S_{T}\, \Delta \\
    \lVert \lambda_{\max}((X_{iS_{T}}'X_{iS_{T}})^{-1}) \rVert_{3+\delta} \leq \Delta/S_{T} \\
    \lVert
    \lambda_{\max}((X_{iS_{T}}'\ep{iS_{T}}\ep{iS_{T}}'X_{iS_{T}})^{-1})\rVert_{3+\delta}
    \leq \Delta/S_{T}
  \end{equation*}
  for all $S_{T} \geq K_{2T}$ and large enough $T$, with $i =
  1,2$,
  \[ X_{iT} \equiv [x_{i1} \quad \dots \quad x_{i,T-\h}]' \]
  and
  \[ \ep{iT} = (\varepsilon_{i,1+\h}, \dots, \varepsilon_{i,T})'.\]
\end{asmp}

The assumption that $y_{t+\h}$ and $x_t$ have positive and finite
variance is straightforward.  The conditions on the eigenvalues are
technical and control the behavior of the \ols\ estimator as the
number of regressors gets large.  The restrictions on the pseudo-true
coefficients ensure that the regression model doesn't dominate the
variance of $y_{t+\h}$ in the limit.  

\begin{asmp}\label{asmp-3}
  The loss function $L$ is continuous and has finite left and right
  derivatives almost everywhere.  Also, $L(0) = 0$, and there is a
  constant $B_L$ such that
  \begin{equation*}
    \|D_{R+j,R}\|_\rho \leq B_L \quad\text{and}\quad
    \|D_{T+j,T}\|_\rho \leq B_L.
  \end{equation*}
  for all $j \geq 1$.
\end{asmp}

This third assumption establishes basic moment and smoothness
conditions for the \oos\ loss.  The differentiability
condition is weak and allows the loss function itself to be
non-differentiable; for example, absolute error and many asymmetric loss functions
satisfy this assumption.

\begin{asmp} \label{asmp-4} $P\to\infty$,
  $R\to\infty$, $Q\to\infty$, $P^2/T \to 0$, and $P = O(Q)$ as $T \to \infty$.
\end{asmp}

The requirements that $P$ and $R$ grow with $T$ are common.  Parts of
the assumption are new, in particular the requirement that $P^2/T \to
0$.  See Lemma \ref{res-convergence} for a discussion of its
implications.  In practical terms, this assumption requires that the
forecaster plan to use the model for many periods in the future, that
the test sample be large, and that the training sample be much larger,
by enough that including or excluding the test sample does not affect
the estimates of $\theta_1$ or $\theta_2$ very much.

The next assumption restricts the class of variance estimators we will
consider.  We use the same class of estimators studied by
\citet{DeD:00} (their class $\mathcal{K}$); see
their paper for further discussion.

\begin{asmp}
  \label{asmp-5} $W$ is a kernel from 
$\Re$ to $[-1,1]$ such that $W(0) = 1$, $W(x) = W(-x)$ for all $x$,
\begin{equation*}
    \int_{-\infty}^{\infty} \lvert W(x) \rvert dx < \infty, \quad
    \int_{-\infty}^{\infty} \lvert \psi(x) \rvert dx < \infty
\end{equation*}
with
\begin{equation*} \psi(x) = \frac1{\sqrt{2\pi}} \int_{-\infty}^{\infty} W(z) e^{ixz}dz,
\end{equation*}
and $W(\cdot)$ is continuous at 0 and all but a finite number of
points.
\end{asmp}

Assumptions~\ref{asmp-1}--\ref{asmp-5} are broadly similar to
\citepos{Wes:96} assumptions, except with regard to model complexity
and the size of the test sample.  West assumes that $K_1$ and $K_2$
are both finite, while we let them grow rapidly with $T$
(Assumption~\ref{asmp-1}), and West lets $P/R$ converge to any value
in $[0,\infty]$, while we require $P^2/R \to 0$
(Assumption~\ref{asmp-4}).\footnote{\citet{Wes:96} does not have a
  second \oos\ period, so there is no analogue in his paper to our
  restriction on $Q$.}  Obviously, these differences are
related---allowing for complex models requires tighter constraints on
the size of the test sample.

Our other assumptions are similar.  The loss function must be smooth
almost everywhere, which is slightly less restrictive than
\citet{Wes:96} requires but is not new; \citet{Mcc:00} extends West's
results to cover a similar case.  West relaxes our assumption that the
forecasts come from a linear regression model; we could relax our
assumption to include other linear estimators, but there would be
little new insight.  We assume full stationarity and absolute
regularity of the underlying random array, while West assumes
covariance stationarity and strong mixing of the transformed forecast
errors; and the details our moment conditions are different, but seem
that they would hold in similar environment to West's.  Finally, since
West's is a more standard environment, he assumes the existence of a
consistent estimator of the variance, while we construct it
explicitly.  Only West's assumptions governing model complexity and
the size of the test sample are substantially different than ours.

\section{Theoretical Results}\label{sec:theory}
The main result of the paper is Theorem
\ref{res-confidence-intervals}, which shows that the \oos\ average is
asymptotically normal and centered on $E_T \oosB$.  This result
holds even for nested models and can be used to construct confidence
intervals or test hypotheses.  After presenting Theorem
\ref{res-confidence-intervals}, we explain and present two key Lemmas
used in its proof that add some insight into the nature of \oos\
comparisons, and then show that standard \hac\ \oos\ estimators can be
used to consistently estimate the asymptotic variance of the \oos\
average.  We then show that the F-test does not let researchers test
hypotheses about $E_{T} \oosB$ when the models are overfit.  In
fact, in the limit it rejects a more-accurate but false benchmark
model almost surely.

\subsection{Behavior of DMW Test with Overfit}\label{sec:oostheory}
Our first theorem justifies using the \dmw\ $t$-test with a a short
test sample.

\begin{thm}\label{res-confidence-intervals}
  Suppose that Assumptions \ref{asmp-1}--\ref{asmp-4} hold, that
  $\hat\sigma$ is an estimator satisfying
  \[
    \hat\sigma^2 = \sigma^2 + o_p(1), \quad \text{with}\quad 
    \sigma^2 \equiv \var_R(\sqrt{P} \oosA),
  \]
  and that $\sigma^2$ is uniformly a.s. positive.  Then
  \[
  \frac{\oosA - \E_T\oosB}{\hat\sigma / \sqrt{P}}
  \to^d N(0,1).
  \]
  Consequently, each of the usual Gaussian confidence intervals,
  \begin{eqnarray}
  &&[\oosA - z_{\alpha/2} \, \hat\sigma /
      \sqrt{P}, \oosA + z_{\alpha/2} \hat\sigma / \sqrt{P}],\label{interval-twosided} \\
  &&[\oosA - z_{\alpha} \, \hat\sigma / \sqrt{P}, +\infty), \label{interval-greater}
  \end{eqnarray}
  and
  \begin{equation} (-\infty, \oosA + z_{\alpha}
      \, \hat\sigma / \sqrt{P}],
  \end{equation}
  contains $\E_T\oosB$ with probability $\alpha$ in the limit,
  with $z_{\alpha}$ the $1-\alpha$ quantile of the standard normal
  distribution.
\end{thm}
The requirement that $\sigma^2$ be uniformly a.s. positive is not
restrictive under our asymptotic theory.  Since the models'
coefficients are estimated with uncertainty in the limit, the two
models give different forecasts even if they both nest the \dgp.  This
is similar to \citepos{GiW:06} rolling-window result, but comes from
different asymptotics.

This result allows forecasters to use the \dmw\ test to conduct
inference about the models' accuracy.  Since the \oos\ average,
$\oosA$, is asymptotically normal and centered on the expected
future performance, $t$-tests constructed from the \oos\ errors
necessarily test hypotheses about $\E_T \oosB$ and not the
performance of the pseudo-true models or the unconditional
performance of the models.

The application to testing can be expressed as a corollary:
\begin{cor}\label{res:oostest}
Suppose that the conditions of Theorem \ref{res-confidence-intervals}
hold.  Then
\begin{equation}
  \Pr[\sqrt{P}\oosA/\hat\sigma \geq z_{\alpha} \mid \E_{T}
  \oosB \leq 0] \to^p \alpha.
\end{equation}
\end{cor}

Two intermediate Lemmas will help us understand Theorem
\ref{res-confidence-intervals} better.  Its proof consists of two
steps: we can write
\begin{equation}
  \sqrt{P} (\oosA - \E_T \oosB) = \sqrt{P} (\oosA - \E_R
  \oosA) + \sqrt{P} (\E_R \oosA - \E_T \oosB),
\end{equation}
so Theorem~\ref{res-confidence-intervals} holds if
\begin{equation}\label{normalpart}
  \sqrt{P}\frac{\oosA - \E_R \oosA}{\sigma} \to^d N(0,1)
\end{equation}
and
\begin{equation}\label{coefpart}
  \E_R \oosA = \E_T \oosB + o_p(P^{-\frac12}).
\end{equation}
The first Lemma first shows that the process $\{D_{tR} - \E_R \oosA\}$
is an $L_2$-mixingale of size $-1/2$ that satisfies a \clt,
ensuring~\eqref{normalpart}.  The second shows that~\eqref{coefpart}
holds under Assumption~\ref{asmp-4}.

\begin{lem}\label{res-mixingale}
  Part 1: If Assumptions~\ref{asmp-1}--\ref{asmp-3} hold then the inequality
  \begin{equation}\label{mixingaleR}
    \lVert \E_{R+j-l}(D_{R+j,R} - \E_R D_{R+j,R}) \rVert_2 \leq
    \couplingConstant \;
    \zeta_l
  \end{equation}
  with $\zeta_l = O(l^{-\frac12 - \delta})$ holds for some positive
  $\delta$, any positive $j$, and any $l$ between 0 and $j$.  As a
  consequence, $\{D_{t,R} - \E_R D_{t,R}, \mathcal F_t\}$ is an
  $L_2$-mixingales of size $-1/2$.  

  \noindent Part
  2: Moreover,~\eqref{normalpart} holds under
  Assumptions~\ref{asmp-1}--\ref{asmp-3} if $P \to \infty$ as $T \to
  \infty$ and $\sigma^2$ is a.s. uniformly positive.
\end{lem}

Mixingales satisfies a weak-dependence condition similar to \mds es
and were introduced and studied extensively by
\citet{Mcl:74,Mcl:75,Mcl:75b,Mcl:77}.  This lemma shows that the
estimation uncertainty introduces dependence into the \oos\ forecast
errors, but that additional dependence is contained in the term $\E_R
\oosA$ and can be effectively removed by subtraction.  The second
part of the Lemma shows that the basic mixingale \clt\ established by
\citet{Dej:97} applies in this setting.\footnote{De Jong proves two
  \clt s: one for mixingales and one for \ned\ functions of mixing
  processes.  His mixingale result makes assumptions about the
  behavior of the sample second moment that arise as consequences of
  \ned.  The proof of Lemma~\ref{res-mixingale} shows that the
  arguments used for de Jong's \ned\ \clt\ continue to hold for our
  \oos\ process---in effect, that the \oos\ process has slightly more
  structure than required for it to be a mixingale.}

\begin{lem} \label{res-convergence} Suppose that Assumptions
  \ref{asmp-1}--\ref{asmp-4} hold.  Then
\begin{equation*}
  \E_T \oosB - \E_R \oosA =  O_p(\sqrt{P/T}) +
  o_p(P^{-\frac12}) + o_p(Q^{-\frac12}).
\end{equation*}
\end{lem}

The justification for Lemma~\ref{res-convergence} is intuitive.  In
the limit, the difference between $\E_T \oosB$ and $\E_R \oosA$
comes from the coefficient estimators used: $\E_T\oosB$ is the
expected performance using the full-sample estimate, $\hat{\theta}_T$,
and $\E_R \oosA$ is the performance using the period-$R$ estimate,
$\hat{\theta}_R$.  When $P$ and $Q$ are large, the dependence between
the test and training samples vanishes, so the estimators of $\theta$
are the only variables affected by the conditioning $\sigma$-fields,
$\mathcal{F}_R$ and $\mathcal{F}_T$.  Since $L$ is smooth, the
difference between $\E_R \oosA$ and $\E_T \oosB$ is of order
$\lvert \hat\theta_R - \hat\theta_T \rvert_2$, which itself is of
order $\sqrt{\frac{P}{T}}$.

Finally, for Theorem \ref{res-confidence-intervals} to be useful, we
need a consistent estimator of $\sigma^2$. Lemma
\ref{res-variance-estimator} establishes that the usual \oos\ \hac\
variance estimators are consistent.\footnote{As with
      Lemma~\ref{res-mixingale}, the proof of this lemma takes
      \citepos{DeD:00} existing consistency results
      for \ned\ processes and shows that the same arguments can be
      applied to our \oos\ mixingale.}

\begin{lem}
  \label{res-variance-estimator}  Suppose Assumptions \ref{asmp-1},
  \ref{asmp-2}, \ref{asmp-3}, and \ref{asmp-5} hold, that $\gamma, P$
  and $R \to \infty$ as $T \to \infty$, and that $\gamma = o(P)$.  Then
  \begin{equation*}
    \frac1P \oosSum{s,t}{1} (D_{tR} - \oosA)(D_{sR} - \oosA)
    \vWeight = \sigma^2 + o_p(1).
  \end{equation*}
\end{lem}


\subsection{Failure of In-Sample Tests}\label{sec:insample}
This subsection establishes that the F-test can over-reject when the
benchmark is more accurate than the alternative.  We use stronger
assumptions than before, i.i.d. data, normal errors, \mse\ loss, and
nested models to ensure that the F-test performs as well as possible.
Indeed, we know its exact distribution.  But the basic idea of this
argument is much more general and should apply to other related tests,
including \oos\ tests.

\begin{thm}\label{res:ftest}
  Let $\h = 1$ and suppose that $x_{1t}$ and $x_{2t}$ are
  deterministic vectors such that $x_{2t} = (x_{1t}, z_t)$ and
\[
T^{-1} \sum_{t=1}^T x_{2t} x_{2t}'  = Q^{-1} \sum_{t=T+1}^{T+Q} x_{2t} x_{2t}' =
I.
\]
Also, suppose that $\theta_2 = (\theta_1, \psi)$, 
$\varepsilon_{2,t+1} \sim i.i.d.\ N(0,\tau^2)$,
$|\theta_1|_2 = \phi_1$, $|\psi|_2 = \phi_2$, $K_1/T = c_1 > 0$,
$K_2/T = c_2 > 0$, and $K_2 \geq 2$, and let $F$ be the F-statistic
for the test of the null hypothesis that $\psi = 0.$
Then 
\begin{equation}\label{eq:ftestreject1}
  \Pr[F \geq q_{K_2 - K_1, T - K_2}(1-\alpha) \mid E_T \oosB \leq 0
  ] \to 1\mbox{ as $\phi_2 \to \infty$ ($T$ fixed)}
\end{equation}
and if $\phi_2 \geq \tau \sqrt{c_2 - c_1}$, then
\begin{equation}\label{eq:ftestreject2}
  \Pr[F \geq q_{K_2 - K_1, T - K_2}(1-\alpha) \mid E_T \oosB \leq 0
  ] \to 1 \text{ as $T \to \infty$},
\end{equation}
where $q_{K_2-K_1, T-K_2}(1-\alpha)$ is the
$(1-\alpha)$-quantile of the $F_{K_2-K_1, T-K_2}$ distribution.
\end{thm}
The conditions on the regressors simplify the proof but can be
relaxed.  As in Corollary~\ref{res:oostest}, the event that we are
conditioning on in~\eqref{eq:ftestreject1} and~\eqref{eq:ftestreject2}
is the event that the benchmark model is expected to be more accurate
in the future.  Since the probability of this event converges to zero
as $\phi_2 \to \infty$, \eqref{eq:ftestreject1} is not an immediate
consequence of the power of the F-test increasing with $\phi_2$.

The first result,~\eqref{eq:ftestreject1}, can be understood through the case $K_2 = 2$ and
$K_1 = 0$, so
\begin{equation*}
  E_T \oosB = \psi_1^2 + \psi_2^2 - (\hat{\psi}_1 - \psi_1)^2 - (\hat\psi_2 - \psi_2)^2,
\end{equation*}
where $\hat{\psi}$ is the \ols\ estimator.  Under these conditions,
the F-test rejects if $\hat{\psi}$ lies outside the circle centered at
the origin with radius $\hat{\tau} (c_2 - c_1)^{0.5} q_{K_2-K_1,
  T-K_2}^{0.5}(1-\alpha)$, and $E_T \oosB \leq 0$ holds for $\hat\psi$
on or outside a circle centered at $\psi$ passing through the origin;
$\hat{\tau}^2$ is the usual estimator of $\tau^2$.
Figure~\ref{fig:rreject} depicts these regions.  Since the \ols\
estimator $\hat\psi$ is normal with mean $\psi$, the probability that
$\hat{\psi}$ falls in the acceptance region given $E_T \oosB \leq 0$
vanishes as $\phi_2 \to \infty$, even though the full model will give
worse forecasts than the restricted model.

\newcommand{\circlefigA}[4]{
  \begin{tikzpicture}
    \fill[lightgray] (-#3,-#3) rectangle (#4,#4);
    % rejection region for F-test
    \filldraw[fill=white,draw=black] (0,0) circle (#1);
    % circle of equal generalization error
    \filldraw[fill=white,draw=black] (1,1) let \p1=(1,1) in circle({veclen(\x1,\y1)});
    \draw (1,1) let \p1=(1,1) in circle({veclen(\x1,\y1)});
    \fill [black] (1,1) circle (2pt) node[right] {$(\psi_1,\psi_2)$};
    \draw (0,0) circle (#1);
    \draw (1,1)--(0,0);
    % axes
    \draw[->] (0,0)--(#2,0) node[right] {$\hat\psi_1$};
    \draw[->] (0,0)--(0,#2) node[above] {$\hat\psi_2$};
  \end{tikzpicture}
}
\newcommand{\circlefigB}[4]{
  \begin{tikzpicture}
    \fill[white] (-#3,-#3) rectangle (#4,#4);
    % rejection region for F-test
    \fill[lightgray] (0,0) circle (#1);
    % circle of equal generalization error
    \fill[white] (1,1) let \p1=(1,1) in circle({veclen(\x1,\y1)});
    \draw (0,0) circle (#1);
    \draw (1,1) let \p1=(1,1) in circle({veclen(\x1,\y1)});
    \fill [black] (1,1) circle (2pt) node[right] {$(\psi_1,\psi_2)$};
    \draw (1,1)--(0,0);
    % axes
    \draw[->] (0,0)--(#2,0) node[right] {$\hat\psi_1$};
    \draw[->] (0,0)--(0,#2) node[above] {$\hat\psi_2$};
  \end{tikzpicture}
}

\begin{figure}
  \centering
  \begin{tabular}{cc}
  \subfloat[]{\circlefigA{1}{2.5}{1.4}{3.2}\label{fig:circleA}} &
  \subfloat[]{\circlefigB{1}{2.5}{1.4}{3.2}\label{fig:circleB}}  \\
  \subfloat[]{\circlefigA{.3}{2.5}{1.4}{3.2}\label{fig:circleC}}  &
  \subfloat[]{\circlefigB{.3}{2.5}{1.4}{3.2}\label{fig:circleD}} 
  \end{tabular}
  \caption{Graphs indicating the rejection region and region of equal
    generalization error for the models discussed in Section
    \ref{sec:insample}.  The smaller model has no estimated parameters
    and the larger has two coefficients.  The shaded regions in
    Figures (a) and (b) show the rejection region and the acceptance
    region respectively of the F-test given $E_T \oosB \leq 0$ when
    the true elements of $\psi$ are moderately small.  Figures (c) and (d)
    show the same regions when the true $\psi$ is large.}
\label{fig:rreject}
\end{figure}

The second part of Theorem \ref{res:ftest},~\eqref{eq:ftestreject2},
is more interesting and implies that the F-test rejects the benchmark
too often for forecasting whenever we have an overfit model (unless,
of course, $\psi = 0$).  The intuition is similar to the previous
case.  As $K_2$ increases, the volume that $\hat{\psi}$ can occupy
increases as well.  So the probability that $\hat{\psi}$ is close to
the origin and therefore inside the acceptance region of the F-test
decreases.  In the limit, the probability that $\hat{\psi}$ is outside
the acceptance region is one.\footnote{If $\phi_2 \geq \tau \sqrt{c_2
    - c_1}$ does not hold, then $\psi$ lies inside the acceptance
  region of the F-test for large enough $T$.  In that case, the
  rejection probability does not converge to one, but will converge to
  a probability greater than $\alpha$.}

It is important to remember that the F-test is valid for the null
$\psi = 0$ in this analysis.  However, the F-test is poorly suited for
choosing an accurate forecasting model, and performs worse when the
larger model is overfit or explains more of the variation of the
dependent variable.  Other valid unbiased tests of the null hypothesis
that $\psi = 0$ will suffer the same problem.

\section{Monte Carlo}\label{sec:mc}
\newcommand{\thetanorm}{\ensuremath{\lvert \theta \rvert_2}} This
section presents two simulations that investigate the accuracy of our
theory in small samples.\footnote{Simulations were conducted in R
  \citep{Rde:10} using the Lattice \citep{Sar:10} and RSQLite
  \citep{Jam:10} packages for graphics and data management
  respectively.}  The first looks at the asymptotic normality of the
\dmw\ statistic centered on either $E_R \oosA$ or $E_T \oosB$.
The previous section's results imply that centering on the first
should give an approximately normal statistic and centering on the
second should give a normal statistic when $P$ is small relative to
$T$.  The second simulation looks at the size and power of several
statistics when conducting inference about the difference in the
models' generalization error.

\subsection{Setup}\label{sec:simulation-design}
The Monte Carlo experiment is intentionally very simple so that we can
isolate the influence of the models' complexity.  In particular, we do
not include some features that are common in forecasting
environments---serial dependence, heteroskedasticity, and complicated
\dgp s.  We simulate data for both studies from the equation
\begin{equation}\label{eq:6}
  y_t = x_t'\theta + \varepsilon_t,\quad \varepsilon_t \sim N(0,1),
  \quad t=1,\dots,T.
\end{equation}
The first element of $x_t$ is 1 and the remaining $K_2-1$ elements are
independent Standard Normal.  The benchmark model is
\begin{equation}
  \label{eq:1}
  y_{1t} = \sum_{j=1}^{K_1} x_{jt}\theta_j + \varepsilon_t
\end{equation}
and the alternative model is the \dgp\ \eqref{eq:6}.  We let
$(K_1,K_2)$ equal either $(2,3)$ or $(T/20,T/10)$ to study our theory
in its intended application as well as for more parsimonious models.
We let $T$ equal 100, 250, 500, or 1000.  We also vary $\theta$, and do 
so giving the benchmark and the alternative model comparable weight in
predicting $y_t$.  Specifically, we set
\begin{equation*}
  \theta_j = 
\begin{cases} \frac{c}{\sqrt{K_1}} & j = 1,\dots,K_1 \\
\frac{c}{\sqrt{K_2 - K_1}} & j = K_1 + 1,\dots,K_2 \end{cases}  
\end{equation*}
with $c=0$ or 1.  When $c$ is large, we're more likely to draw
values of $X$ and $Y$ that make the estimated larger model more
accurate than the benchmark, and when $c$ is zero we're unlikely to
draw such values of $X$ and $Y$.  For all of the studies, $L(x) =
x^2$.  

The first set of simulations study how well the theory in
Section~\ref{sec:oostheory} works in practice.  For each draw of $X$
and $Y$, we construct a one-sided and two-sided \oos\ interval of the
form
\begin{equation*}
  [ \oosA - 1.28 \hat{\sigma}, \infty) \quad\text{and}\quad
  [ \oosA - 1.64 \hat{\sigma}, \oosA + 1.64\ \hat{\sigma}], \\
  \hat\sigma^2 = \frac1P \sum_{t=R+1}^T (D_t - \oosA)^{2}
\end{equation*}
for $P$ set to every 10th value between $T$ and $\frac{2T}{3}$.  We
calculate the percentage of these intervals that contain $E_R \oosA$
and that contain $E_T \oosB$.  Since the data are i.i.d., both of
these quantities are easy to calculate:
\begin{equation*}
  E_R \oosA = [\sum_{i=1}^{K_1} (\tilde{\theta}_{iR} - \theta_i)^2 +
  \sum_{i=K_1+1}^{K_2}\theta_i^2] - \sum_{i=1}^{K_2} (\hat{\theta}_{iR} -
  \theta_i)^2
\end{equation*}
and
\begin{equation*}
  E_T \oosB = [\sum_{i=1}^{K_1} (\tilde{\theta}_{iT} - \theta_i)^2 +
  \sum_{i=K_1+1}^{K_2}\theta_i^2] - \sum_{i=1}^{K_2} (\hat{\theta}_{iT} -
  \theta_i)^2
\end{equation*}
where $\tilde{\theta}$ indicates that the coefficient is estimated
using only the regressors in the benchmark model and $\hat{\theta}$
the full model.  We draw 2000 samples for each combination of the
design parameters.

The second simulation looks at the size and power of the test
statistics when conducting inference about the models' generalization
error.  The null hypothesis we test is 
\begin{equation}\label{eq:9}
  H_0:\quad E_T \oosB \leq 0,
\end{equation}
We look at four different statistics---the full-sample F-test, the
\dmw\ $t$-test, the
\oos\ $t$-test using McCracken's (2007)
critical values,\footnote{These critical values are not published for
  $K_2-K_1>10$, so we do not report them for $K_2 = T/10$.}
and Clark and West's (2006, 2007) Gaussian out-of-sample
statistic.\footnote{Clark and West (2006, 2007) derive their statistic
using the rolling window estimation scheme.  Here we use the same
statistic, but with a fixed window scheme.}
For the F-test, we simply test whether the coefficients on the larger
model are nonzero.  For the out-of-sample tests, we conduct a
one-sided test of out-of-sample performance for every 10th value of
$P$ as before.

To estimate each test's size for the hypothesis \eqref{eq:9}, we draw
samples from the \dgp\ and discard those for which \eqref{eq:9} does
not hold, until we have 2000 for each choice of the design parameters.
The estimated size is the fraction of those samples in which the test
statistic rejects.
\subsection{Results}
We next discuss results for the confidence intervals first.
Figures~\ref{fig:interval-R} and~\ref{fig:interval-T} show the
coverage probability of these intervals as a function of $P$ for each
combination of $T$, $K_1$ and $K_2$, and $c$.  The gray horizontal
line shows the intervals' nominal coverage probability.  Each panel
displays the coverage for a different choice of interval, centering
term ($E_R \oosA$ or $E_T \oosB$) and combination of design
parameters.

Figure~\ref{fig:interval-R} gives the results for $E_R \oosA$.  For
both the one and two-sided intervals, the actual coverage is very
close to the nominal coverage except when $P$ is small.  The poor
behavior for small $P$ is unsurprising, as it simply means that the
\clt\ is a poor approximation when the test sample is small.  The
intervals' good coverage holds even for the parsimonious models
($K_1=1$ and $K_2= 3$) which our theory does not necessarily apply to.

Figure~\ref{fig:interval-T} gives the results for $E_T \oosB$.  In
rows 2, 4, 6, and 8, the overfit models, the coverage is near nominal
coverage for moderately small values of $P$.  As $P$ increases to $2T/3$, the
coverage either increases to 1 (for the one-sided interval) or fall to
zero (the two-sided).  With the parsimonious model, the coverage is near
nominal coverage for all $P$ for the one-sided interval with $c=1$,
but only for moderately small $P$ for the two-sided interval or when
$c=0$.

The behavior for $K_2 = T/10$ is exactly what our theory predicts.
When $P^2/T$ is small, the coverage is near nominal levels.  The
behavior as $P$ increases, combined with the results for $E_R
\oosA$, indicate that $E_T \oosB \geq E_R \oosA$ in general.
Since
\[E_{T} \oosB = E_{R} \oosA + (E_{T} \oosB - E_{R} \oosA), \] and the
interval is approximately centered at $E_R \oosA$, the difference
$E_{T} \oosB - E_{R} \oosA$ adds a substantial positive quantity when
$P^2/T$ is not near zero, increasing the coverage of the one-sided
interval but taking $E_T \oosB$ outside the two-sided interval.

We next present the size simulations,
Figures~\ref{fig:ftest}--\ref{fig:ttest-power}.  For the \oos\ tests,
the conditional rejection probability is plotted for each combination
of $T$, $K_1$, $K_2$, and $c$ as a function of $P$.  The F-test does
not depend on $P$, so a single value is presented for each
combination.

We'll look at the F-test first.  To make comparisons easier, the size
is displayed as a dot plot in Figure~\ref{fig:ftest}.  Different
panels display a different combination of $K_1$, $K_2$, and $T$, and
each individual plot shows the empirical size for each choice of $c$.
We see immediately that the actual and nominal size are essentially
equal for $c = 0$, which is unsurprising.  For $c = 0$, the F-test is
exact; moreover, the larger model will almost always be less accurate
than the smaller one, so conditioning on $\E_T \oosB \leq 0$ is
almost unrestrictive.  When $c$ increases, though, the F-test
over-rejects badly---rejecting at roughly 50\% when $c = 1$ for the
parsimonious model and from 70\% to 100\% for the overfit model.  As
our discussion in Section~\ref{sec:insample} predicts, the rejection
probability increases with $T$ for the overfit model but does not
depend on $T$ for the small model.

Figure~\ref{fig:ttest-size} presents the size estimates for the \dmw\
$t$-test.  Again, different panels display results for different
combinations of the design parameters.  Each graph plots the rejection
probability against $P/T$.  For $K/T=10$, the rejection probability
falls as $P/T$ increases, from near nominal size when $P/T$ is small
to zero when $P/T$ is near $2/3$.  Moreover, the rejection probability
falls faster when $T$ is large, as our theory predicts.  When $K=3$,
the rejection probability stays closer to nominal size, but falls with
$P/T$ for $c=0$, under-rejecting by about 5pp when $P/T = 2/3$, and
rises with $P/T$ for $c=1$, over-rejecting by about 10pp when
$P/T=2/3$.  For small $P$, the rejection probability is near 10\% for
all simulations (the farthest is $K=T/10$, $c=0$, where the rejection
probability is about 5\%; the other simulations are much closer).

We observe the following patterns.  The
\dmw\ test has close to nominal size when $P$ is small for every
combination of design parameters.  In most cases, the rejection
probability decreases as $P/T$ increases---the exception is for $K_2 =
3$ and $c=1$.  For the large-$K$ simulations, the rejection
probability drops to zero for most of the simulations as $P/T$
increases.  The rejection probability increases with $c$, but the
rejection probability still is near nominal probability for small $P$
with $c=1$.

Clark and West's (2006, 2007) statistic, presented in
Figure~\ref{fig:clarkwest}, behaves quite differently.  For $c=0$ the
test is correctly sized for both the overfit and parsimonious studies,
as we saw for the F-test.  When $c=1$, the rejection probability
increases rapidly with $P/T$.  For $K_2=3$, the rejection probability
is near 10\% when $P$ is small but about 40\% when $P/T = 2/3$.  For
$K=T/10$, the rejection probability is even higher and increases with
$T$ as well, from a maximum over 50\% when $T=100$ to a maximum of
nearly 100\% when $T=1000$.

Results using \citepos{Mcc:07} critical values are
presented in Figure~\ref{fig:mccracken} and are similar to those using
Clark and West's test.  For $c=0$ the rejection probability is nearly
the test's nominal size.  For $c=1$, the rejection probability
increases with $P/T$, from close to the nominal size when $P/T$ is
small to over 25\% when $P/T = 2/3$.  Note that all of the simulations
use the parsimonious model.  McCracken's statistic over-rejects here
by slightly less than Clark and West's, but still by a substantial
amount.

Since the \dmw\ test tends to have low rejection probability, the
test's power is a concern.  We'll present some power results,
simulating from \eqref{eq:1} with $c = 1$ or 2 subject to the
constraint that $E_T \oosB > 0$.\footnote{Draws of $X$ and $Y$ with
  $E_T \oosB > 0$ are very rare when $c=0$, so we do not present
  results for that value of $c$.}  Figure~\ref{fig:ttest-power} plots
the power for the \dmw\ test; since the other test statistics greatly
over-reject, we do not present their power.  For $c=1$, the power is
never greater than nominal size and decreases to zero as $P/T$
increases for the overfit model.  For $c=2$ the power is better,
increasing with $P/T$ for a stretch and then decreasing as $P/T$ grows
beyond approximately 1/4 for the overfit model.  Larger values of $T$
give a higher peak and greater power overall, but the power still
falls to nearly zero if $P/T$ is too large (approximately 2/3 in our
simulations).  The power with the parsimonious model is typically
quite low but greater than nominal size for $c = 2$.

Both sets of simulations support our theoretical results.  The first
simulation confirms that the \dmw\ \oos\ $t$-test is centered at $E_R
\oosA$ for all choices of $P$ and $R$ and is centered on $E_T \oosB$
only when $P$ is small.  The second simulation confirms that the \dmw\
test has correct size for the null hypothesis that $E_T \oosB \leq 0$
when $P$ is small and that tests designed to test whether the
benchmark is true, like the F-test and Clark and West's (2006, 2007)
and McCracken's (2007) \oos\ tests can reject by much more than their
nominal size when testing the null $E_T \oosB \leq 0$.  Moreover,
these simulations demonstrate that the restriction $P^2/T \to 0$ is
binding in practice, as the \dmw\ test under-rejects and has very low
power when it is not satisfied.
\section{Empirical Exercise}\label{sec:empirics}
This section presents an analysis of equity premium predictability
similar to \citet{GoW:08}.\footnote{All of the calculations in this
  section are done in R \citep{Rde:10}.  We also use code provided by
  \citet{Zeh:02}, \citet{Zei:04}, and \citet{Har:10}.}  We estimate
the expected forecasting performance of the largest model they
consider, a model with 13 regressors, using 81 observations (annual
data from 1928 to 2009).  We construct confidence intervals for the
difference in performance between this model and a prevailing mean
benchmark, and do so for different values of $R$ and $P$ to examine
the effect of the training and test sample choice on this \oos\
statistic.  We find that the estimate of the larger model's \mse\
decreases relative to the benchmark as $R$ increases, but it is never
smaller than the benchmark's.  This result, combined with the larger
model's in-sample significance, suggests that there may be a real
relationship between the equity premium and some of these predictors,
but that the relationship can not be estimated accurately enough to be
useful for forecasting and supports \citepos{GoW:08}
conclusions.  These results hold even when imposing
\citepos{CaT:08} restriction that the equity
premium forecast be non-negative.

We'll start with a very brief review.  \citet{GoW:08}
study the \oos\ forecasting performance of different variables thought
to predict the equity premium (calculated as the difference between
the return on the S\&P 500 index and the T-bill rate).  Some of these
variables are listed in
Table~\ref{tab:equity}.\footnote{Table~\ref{tab:equity} only lists the
  variables used in \citepos{GoW:08} ``kitchen sink''
  model.  Some of the variables that they use in bivariate models are
  excluded from this model either because the series are too short or
  because the variables are linear combinations of other variables.}
\citet{GoW:08} estimate the \oos\ forecasting
performance of bivariate models of the form
\[
r_{t+1} = \beta_0 + \beta_1 x_t + \varepsilon_{t+1}
\]
using \ols\ with a recursive window, where $r_{t+1}$ is the equity
premium and $x_t$ is a generic predictor, as well as some larger
models.  They find that these models forecast no better than the
prevailing mean of the equity premium.
\citet{CaT:08} find that nonlinear extensions of
Goyal and Welch's models are more accurate, either imposing sign
restrictions on the estimated coefficients or imposing that the
forecast be non-negative, but do not test for significance.

Obviously, \citepos{GoW:08} bivariate models do not
match the asymptotic theory of this paper, so we restrict our
attention here to their ``kitchen sink'' model, which includes all of
the variables, except for a few dropped because of data availability or
(perfect) multicollinearity.  Formally, the benchmark and alternative models are
\begin{eqnarray}
  \label{eq:3}
  r_{t+1} &=& \mu + \varepsilon_{1,t+1} \\
  \label{eq:2}
r_{t+1} &=& \beta_0 + \sum_{i=1}^K \beta_{i} x_{it} + \varepsilon_{2,t+1}  
\end{eqnarray}
respectively and are estimated by \ols\ using the fixed-window scheme.
The predictors are listed in Table~\ref{tab:equity}; for a detailed
description of each predictor, please see Goyal and Welch's original
paper.  In this model, there are 13 regressors (including the constant
term) and we estimate the coefficients using annual data from 1928 to
2009.  This makes the ratio $K/T$ equal to about 0.16.  We also
present results for one of \citepos{CaT:08}
restricted forecasting models.  For this model, we estimate
\eqref{eq:3} and \eqref{eq:2}, but impose that $\hat r_{t+1}$ be
non-negative for each forecast.  We then calculate the \oos\ test just
as for the unconstrained model.

Table~\ref{tab:gwinsample} gives the results of the full sample
regression for Equation~\eqref{eq:2}, using Newey-West standard errors
with two lags \citep{NeW:87}.\footnote{We compare the test statistic
  to critical values from the $F$-distribution.}  The $p$-value is
very small (less than 0.01), indicating that the coefficients are
nonzero in population and at least one of these predictors is
correlated with the equity premium.  As we argue earlier, this result
does not imply that the model will forecast well, which we look at
next.

To study the effect of the training sample size on the \dmw\ statistic, we
calculate the one-sided confidence interval for $E_T \oosB$ given
by \eqref{interval-greater} corresponding to the null and alternative
hypotheses
\[ H_0: \quad E_T \oosB \leq 0 \qquad
H_A: \quad E_T \oosB > 0
\]
for each value of $R$ between 20 and $T-10$.  The standard deviation
is estimated using a Newey-West estimator with $\lfloor
P^{\frac14}\rfloor$ lags.  For small values of $R$, the \oos\ average
is expected to underestimate the performance of the larger model
relative to the smaller, but this may not hold in this particular
dataset.

Figures~\ref{fig:empirics1} plots the \ols\ results and
Figure~\ref{fig:empirics2} imposes \citepos{CaT:08} restriction.  The
solid line in each figure shows the \oos\ average, $\oosA$, and the
shaded region indicates the 95\% one-sided confidence interval implied
by the \dmw\ test.  Negative numbers indicate that the kitchen sink
model has higher out-of-sample loss.  We can see that the same
patterns hold for both models: the performance difference decreases as
$R$ grows, but the kitchen sink model is never more accurate.  We also
see that the performance difference decreases suddenly over the period
$R=29$ to $R=34$ (corresponding to the years 1956--1961).
Figure~\ref{fig:empirics3} plots the accuracy of the individual
forecasts (only for the linear models) and shows that this change is
the result of a sudden improvement in the kitchen sink model.  This
change may indicate instability in the underlying relationship, as
\citet{GoW:08} propose.

In summary, we fail to reject the null that the benchmark prevailing
mean model is more accurate than \citepos{GoW:08} kitchen sink.  This
result is consistent with Goyal and Welch's original analysis.  Unlike
Goyal and Welch, we attribute this result, at least in part, to
parameter uncertainty---the full sample results indicate that the
larger model could predict better than the benchmark with enough data.
Obviously, this is an illustrative exercise only and is not meant to
be comprehensive.  In this dataset, there is likely parameter
instability that we have not addressed, and we have not dealt with
variable persistence at all.
\section{Conclusion}\label{sec:conclusion}
This paper gives a theoretical motivation for using \oos\ comparisons:
the \dmw\ \oos\ test allows a forecaster to conduct inference about
the expected future accuracy of his or her models when one or both is
overfit.  We show analytically and through Monte Carlo that existing
in-sample test statistics can not test hypotheses about this
performance.

Our paper also shows that popular test and training sample sizes may
give misleading results if researchers are concerned about
overfit.\footnote{As we mention in the introduction, forecasters also
  use these statistics if they are concerned about unmodeled
  instability, which this paper does not address.}  We show that
$P^2/T$ must converge to zero for the \dmw\ test to give valid
inference about the expected forecast accuracy, otherwise the test
measures the accuracy of the estimates constructed using only the
training sample.  In empirical research, $P$ is typically much larger
than this.  Our simulations indicate that using large values of $P$
with the \dmw\ test gives undersized tests with low power, so this
practice may favor simple benchmark models too much.  Existing
corrections, proposed by \citet{ClM:01,ClM:05}, \citet{Mcc:07} and
\citet{ClW:06,ClW:07}, seem to correct too much, though, and reject
too often when the benchmark model is more accurate.

More work remains.  Our requirement that $\frac{P^2}{T}$ converge to
zero is limiting, as it implies that in typical macroeconomic
datasets, only a handful of observations should be used for testing.
This restriction could be addressed by directly estimating $\E_R \oosA
- \E_T \oosB$ or by deriving its asymptotic distribution, which would
allow larger values of $P$.  It could also be mitigated by extending
our results to cross-validation or other resampling strategies.  It
would also be useful to extend our results to other estimation windows
and forecasting models, and to explore how stationarity could be
relaxed.

\appendix
\section{Supporting results}
The results in this paper rely heavily on a coupling argument for
absolutely regular sequences, Berbee's Lemma \citep{Ber:79}.  In fact,
many of the results of this paper (\ref{res-confidence-intervals},
\ref{res-mixingale}, and \ref{res-variance-estimator}) are
modifications of existing results for \ned\ functions of mixing
processes by \citet{Dej:97} and
\citet{DeD:00}; this coupling argument is used to
explicitly derive inequalities that arise naturally for \ned\
processes.  Lemma \ref{lem-basic-coupling} establishes these
inequalities, which are based on a proposition of
\citet{MeP:02}.

We present \citeauthor{MeP:02}'s
(\citeyear{MeP:02}) statement of Berbee's Lemma for
the reader's reference.  In the following Lemma, $\beta(X,Y)$ is the
coefficient of absolute regularity:
\[
\beta(X,Y) = \sup_{A \in \sigma(Y)} \E \lvert \Pr(A \mid \sigma(X))
  - \Pr(A) \rvert.
\]
\begin{quotation}
\begin{lem}\label{lem-berbee}
  Let $X$ and $Y$ be random variables defined on a probability space
  $(\Omega, \mathcal{T}, \mathbf{P})$ with values in a Polish space
  $S$.  Let $\sigma(X)$ be a $\sigma$-algebra generated by $X$ and let
  $U$ be a random variable uniformly distributed on $[0,1]$
  independent of $(X,Y)$.  Then there exists a random variable $Y^{*}$
  measurable with respect to $\sigma(X) \vee \sigma(Y) \vee
  \sigma(U)$, independent of $X$ and distributed as $Y$, and such that
  $\mathbf{P}(Y \neq Y^{*}) = \beta(X,Y)$.
\end{lem}
\noindent\citep{MeP:02}
\end{quotation}

The advantage of this result over coupling arguments that use other
forms of weak dependence is that the difference between the original
variable, $Y$, and the new variable, $Y^{*}$, does not depend on their
dimension.  Similar results for strong mixing sequences depend on the
dimension of $Y$, which makes them unsuitable for our applications.
The remaining results in this section use Berbee's Lemma to establish
some basic inequalities.

\begin{lem}\label{lem-extend-mp}
  Suppose that $X$ and $X^*$ are $L_p$-bounded random variables, with
  $p > 2$, that satisfy ${\Pr[X \neq X^*] = c}$.  Then
  \[
    \lVert X - X^* \rVert_2 \leq 2^{1/p} (\lVert X \rVert_p + \lVert
    X^* \rVert_p) c^{(p-2)/2p}
  \]
\end{lem}

The proof is virtually identical to the proof of Proposition 2.3 in
\citet{MeP:02} and is omitted.

\begin{lem}\label{lem-basic-coupling}
  Suppose Assumptions \ref{asmp-1}--\ref{asmp-3} hold.  Then, for any
  T, $s$, $t$, $u$, and $w$, with $t \geq s > u \geq w$, there exist random
  variables $\tilde D_s,\dots,\tilde D_t$ such that
  \begin{equation}\label{eq:coupling1}
    P[(\tilde D_s,\dots,\tilde D_t) \neq (D_{sw},\dots,D_{tw})] \leq \beta_{s-u}
  \end{equation}
  and
  \begin{equation}\label{eq:coupling2}
    \E(\phi(\tilde D_s,\dots, \tilde D_t) \mid \mathcal{F}_u ) = 
    \int
    \phi(D_{sw},\dots,D_{tw}) f(\mathbf{x}, \mathbf{y})\ d\mathbf{x}\ d\mathbf{y}
  \end{equation}
  almost surely for all measurable functions $\phi$ such that the
  expectations are finite, where 
  \[ \mathbf{x} = (x_{s}', \dots, x_{t}')', \\
  \mathbf{y} = (y_{s+\h},\dots,y_{t+\h})'\] and $f$ is the
  joint density of $(\mathbf{x}, \mathbf{y})$.  Moreover,
 \begin{equation}\label{eq:coupling3}
   \| \tilde D_v - D_{vw} \|_2 \leq 2^{1+1/\rho} B_L
   \beta_{s-u}^\rhoExp, \qquad v = s,\dots,t.
 \end{equation}
\end{lem}

\begin{proof}
  The proof follows as a consequence of Lemmas \ref{lem-berbee} and
  \ref{lem-extend-mp}.  Let $l = t-s$.  For any fixed values of $l$
  and $T$, the sequence of vectors
  \[ V_s = (y_{s+\h}, x_{s}', \dots, y_{s+l+\h}, x_{s+l}') \] is
  absolutely regular of size $\rho/(\rho-2)$.  Berbee's Lemma implies
  that there is a random vector $V^*$ that is independent of
  $\mathcal{F}_u$, equal to $V_s$ in distribution, and satisfies
  \[\Pr[V^* \neq V_s] = \beta_{s-u}.\]

  Now define
  \[ \tilde D_v \equiv L(y_{v+\h}^* - x_{1v}^* \cdot
  \hat{\theta}_w) - L(y_{v+\h}^* - x_{2v}^* \cdot
  \hat{\theta}_w), \quad v = s,\dotsc, t,
  \]
  with $y_{v+\h}^*$ and $x_{iv}^*$ denoting the elements of $V^*$
  corresponding to $y_{v+\h}$ and $x_{iv}$ in $V_s$.  Equations
  (\ref{eq:coupling1}) and (\ref{eq:coupling2}) are satisfied by
  construction, and (\ref{eq:coupling3}) follows from Lemma
  \ref{lem-extend-mp}.
\end{proof}

\section{Proofs of results in main text}
\begin{proof}[Proof of Theorem \ref{res-confidence-intervals}]
It suffices to prove asymptotic normality.  We can rewrite the
centered \oos\ average as
  \[
    \CenteredAverage = \CenteredAverageI + \CenteredAverageII.
  \]
Lemmas \ref{res-mixingale} and 
\ref{res-convergence} ensure that the first term is asymptotically
normal and the second term vanishes.
\end{proof}

\begin{proof}[Proof of Lemma \ref{res-mixingale}, Part 1.]
Fix $R$, $j$ and $l$ and use Lemma
\ref{lem-basic-coupling} to define $\tilde{D}_{R+j}$ so that
\[\E_R\tilde D_{R+j} = \E_{R+j-l} \tilde D_{R+j} \quad a.s. \]
and
\[\lVert D_{R+j,R} - \tilde D_{R+j} \rVert_2 \leq \couplingBound{l}.\]
Then
\[
\lVert \E_{R+j-l} D_{R+j,R} - \E_R D_{R+j,R} \rVert_2 \wall \leq 
\lVert \E_{R+j-l} D_{R+j,R} - \E_{R+j-l} \tilde{D}_{R+j} \rVert_2 \\
\quad + \lVert
\E_{R+j-l} \tilde{D}_{R+j} - \E_R D_{R+j,R} \rVert_2 \\
\leq 2 \lVert D_{R+j,R} - \tilde D_{R+j} \rVert_2 \\
\leq 2^{2 + \frac1\rho}\ B_L\ \couplingBeta{l}.\return
\]
Now $\couplingBeta{l} = O(l^{-\frac12 - \delta})$ for some
positive $\delta$, completing the proof.
\end{proof}

\begin{proof}[Proof of Lemma \ref{res-mixingale}, Part 2.]
  A modification of \citepos{Dej:97} \clt s for
  mixingale and \ned\ arrays will complete the proof.

Define
\[\label{eq:z1}
  Z_i = \ZDef
\]
where $b_T$ is a sequence that satisfies $b_T\leq P$,
$b_T\to\infty$, and $b_T/P\to 0$.  The same arguments used in
\citepos{Dej:97} Theorem 1 show that
\begin{equation*}
   \SumOuterBlock{i} Z_i = \frac1{\sqrt{P}} \oosSum{s}{1} (D_{tR} - \E_R D_{tR}) + o_p(1).
\end{equation*}
and
\begin{equation*}
  \SumOuterBlock{i} Z_i = \SumOuterBlock{i} (Z_i - E_{R + (i-1)b_T}
  Z_i) + o_p(1).
\end{equation*}
Note that $\{Z_i - E_{R + (i-1)b_T},\mathcal{F}_{R + i b_T}\}_i$ is an
\mds\ by construction, so \citepos{HaH:80} Theorem 3.2 and Corollary
3.1 ensure that\footnote{Note that $\sigma^2 \in \mathcal{F}_t$ for
  all $t \geq R$, so Hall and Heyde's condition (3.21) is
  unnecessary---see the remarks after their result.}
\[
\frac1{\sigma} \SumOuterBlock{i} Z_i \to^p
N(0,1)
\]
as long as
\begin{equation}\label{eq:cltvar2}
  \sigma^2 - \SumOuterBlock{i} \E_R Z_i^2 \to^p 0,
\end{equation}
and
\begin{equation}\label{eq:cltvar1}
  \SumOuterBlock{i} \E_R Z_i^2 - \SumOuterBlock{i} \ZSqCE \to^p 0.
\end{equation}
Equation (\ref{eq:cltvar2}) holds as in \citet{Dej:97} (see the proof
of his Theorem 2); to see that (\ref{eq:cltvar1}) holds, we'll use an
argument based on his Lemma 5.  Define
\[
\tilde Z_i = \frac1{\sqrt{P}} \SumInnerBlock{s}{i} \tilde D_s -
\E_R \tilde D_s,
\]
with $\tilde D_{R+(i-1)b_T + 1},\dots,\tilde D_{R+ib_T}$ new random
variables that satisfy
\[
\lVert \tilde D_s - D_{sR} \rVert_2 \leq 2^{1 + \frac1\rho} B_L \beta_{s -
  (i-1)b_T + R}^{\rhoExp}, \quad s = R+(i-1)b_T + 1,\dots,R+i b_T
\]
and
\[
\E_R \phi(\tilde D_s) = \E_{R+(i-1)b_T} \phi(\tilde D_s) \quad \text{a.s.}, \quad s = R+(i-1)b_T + 1,\dots,R+i b_T
\]
for all measurable $\phi$ such that the expectations are finite; the
existence of these variables is ensured by Lemma
\ref{lem-basic-coupling}.  Then $\tilde Z_i$ satisfies
\[
\E_R \tilde Z_i^2 = \CEOuterBlock{\tilde Z_i^2}{i} \quad \text{a.s.},
\]
giving the inequality
\[
\lVert \ZSqCE - \E_R Z_i^2 \rVert_1 \wall= \lVert \wall\CEOuterBlock{(Z_i^2 -
  \tilde Z_i^2)}{i} \\ - (\E_R Z_i^2 - \CEOuterBlock{\tilde Z_i^2}{i})
\rVert_1 \return\\
\leq 2 \lVert Z_i^2 - \tilde Z_i^2 \rVert_1.
\return
\]

Also define 
\[
V_{i,C} = \begin{cases} Z_i & \text{if } |Z_i| < \ZTrunc \\ 
  \ZTrunc & \text{if } Z_i \geq \ZTrunc \\
- \ZTrunc & \text{if } Z_i \leq -\ZTrunc
  \end{cases}
\]
and
\[
\tilde V_{i,C} = \begin{cases} 
 \tilde Z_i & \text{if } |\tilde Z_i| < \ZTrunc \\ 
  \ZTrunc & \text{if } \tilde Z_i \geq \ZTrunc \\
- \ZTrunc & \text{if } \tilde Z_i \leq -\ZTrunc.
\end{cases} 
\]
Both $Z_i\sqrt{\frac{P}{b_T}}$ and $\tilde Z_i\sqrt{\frac{P}{b_T}}$
are uniformly integrable, so $C$ can be chosen to make
\[
\lVert \SumOuterBlock{i} [(Z_i^2 - \tilde Z_i^2) - (V_{i,C}^2 - \tilde V_{i,C}^2)] \rVert_1
\]
arbitrarily small.  As a result, \eqref{eq:cltvar1} holds if we can
prove that
\[
\lVert \SumOuterBlock{i} (V_{i,C}^2 - \tilde V_{i,C}^2) \rVert_1 \to 0
\]
for arbitrary $C$, which we will show next.

Note that 
\[\lvert V_{i,C} - \tilde V_{i,C} \rvert \leq \lvert Z_{i,C} - \tilde
Z_{i,C} \rvert \quad \text{and} \quad \lvert V_{i,C} + \tilde V_{i,C}
\rvert \leq 2 C \sqrt{b_T/P} \quad \text{a.s.}\] 
It follows that
\[
\SumOuterBlock{i} \lVert V_{i,C}^2 - \tilde V_{i,C}^2 \rVert_1 
\wall
 \leq 2 \ZTrunc \SumOuterBlock{i} \lVert Z_i - \tilde Z_i \rVert_2 \\
 \leq \frac{2 C \sqrt{b_T}}{P}\SumOuterBlock{i} \SumInnerBlock{s}{i} \lVert \wall \ZSummand \\
  - (\tilde D_s - \E_R \tilde D_s) \rVert_2 \return \\ 
 = O(\frac{\sqrt{b_T}}{P}) \SumOuterBlock{i} \SumInnerBlock{s}{i} \lVert D_{sR} - \tilde D_s \rVert_2 \\
 = O(b_T^{-\frac12}) \sum_{s=1}^{b_T} \couplingBeta{s}
 \return
 \]
Finally, $\couplingBeta{s} = O(s^{-1/2-\delta})$ for some positive
$\delta$, making the last term $o(1)$ and completing the proof.
\end{proof}

\begin{proof}[Proof of Lemma \ref{res-convergence}]
\newcommand{\resConvgRHS}[1]{\ensuremath{\E(L(y^* - x_1^{*\prime}\bh{1#1}) - L(y^{*} -
x_2^{*\prime}\bh{2#1}) \mid \bh{#1})}}
\newcommand{\resConvgEstDiff}[1]{\ensuremath{\E(L(y^{*} -
    x_{#1}^{*\prime}\bh{#1R}) \mid \bh{R}) - 
\E(L(y^{*} - x_{#1}^{*\prime}\bh{#1T}) \mid \bh{T})}}
\newcommand{\resConvgEstDiffRV}[1]{\ensuremath{L(y^{*} -
    x_{#1}^{*\prime}\bh{#1R}) - L(y^{*} - x_{#1}^{*\prime}\bh{#1T})}}
We'll first show that 
\begin{equation}\label{EQ-oosaverage1}
\E_R \oosA = \resConvgRHS{R} + o_p(P^{-\frac12})
\end{equation}
and
\begin{equation}\label{EQ-oosaverage2}
\E_T \oosB = \resConvgRHS{T} + o_p(Q^{-\frac12})
\end{equation}
where $\hat\theta_R = (\hat\theta_{1R}, \hat\theta_{2R})$,
$\hat\theta_T = (\hat\theta_{1T}, \hat\theta_{2T})$, and $y^{*}$,
$x_1^{*}$ and $x_2^{*}$ are random variables drawn from the joint
distribution of $(y_{t+\h},x_{1t},x_{2t})$ independently of
$\mathcal{F}_T$.  We'll then show that
\begin{equation}\label{EQ-thetaconvg1}
\resConvgEstDiff{1} = O_p(\sqrt{P/T})
\end{equation}
and
\begin{equation}\label{EQ-thetaconvg2}
\resConvgEstDiff{1} = O_p(\sqrt{P/T})
\end{equation}
to complete the proof.

To prove (\ref{EQ-oosaverage1}), define $\tilde D_t$ for each $t = R+1,\dots,T-\h$ as
in Lemma \ref{lem-basic-coupling} so that
\[
\lVert \tilde D_t - D_{tR} \rVert_2 \leq \couplingBound{t-R}
\]
and 
\[
\E_R \tilde D_t = \resConvgRHS{R} \quad\text{a.s.}
\]
for all $t$.  Then 
\[
\lVert \E_R \oosA - \wall \resConvgRHS{R} \rVert_2  \\
\leq \frac1P \oosSum{t}{1} \lVert \E_R (D_{tR} - \tilde D_t) \rVert_2 \\
\leq \frac{\couplingConstant}{P} \oosSum{s}{1} \couplingBeta{s} \\
= o(P^{-\frac12}).
\return
\]
The proof of (\ref{EQ-oosaverage2}) is identical.

Equations (\ref{EQ-thetaconvg1}) and (\ref{EQ-thetaconvg2}) both
follow the same argument, so we'll just prove (\ref{EQ-thetaconvg1}).
Since 
\[ \lVert \resConvgEstDiff{1} \rVert_1 \\ \quad= \| \E(L(y^{*} -
x_1^{*\prime} \hat{\theta}_{1R}) - L(y^{*} - x_2^{*\prime}
\hat{\theta}_{dR}) \mid \hat{\theta}_{1R}, \hat{\theta}_{1T}) \|_1 \\
\quad\leq \lVert \resConvgEstDiffRV{1} \rVert_1, \] it suffices to
prove
\[ \resConvgEstDiffRV{1} = O_p(\sqrt{P/T}). \]
Moreover, smoothness of the loss function ensures that
\[ \resConvgEstDiffRV{1} = O_p(1) x_1^{*\prime} (\bh{1R} -
\bh{1T}), \]
and it suffices to show that
\begin{equation*}
  \lVert x_1^{*\prime}(\bh{1R} - \bh{1T}) \rVert_2 = O(\sqrt{P/T}).
\end{equation*}

Manipulating the formula for the \ols\ estimator gives us
\begin{equation} \label{eq-theta-convergence-1}
\lVert x_1^{*\prime} (\bh{1R} - \bh{1T}) \rVert_2 \wall \leq
\lVert x_1^{*\prime} [(X_{1R}'X_{1R})^{-1} - (X_{1T}'X_{1T})]
X_{1T}'\ep{1T} \rVert_2  \\ \quad +
\lVert x_1^{*\prime} (X_{1R}'X_{1R})^{-1}\oosSum{t}{1} x_{1t}\varepsilon_{i,t+\h}
\rVert_2.
\end{equation}
The first term on the right side of \eqref{eq-theta-convergence-1}
satisfies the relationship
\[
\lVert x_{1}^{*\prime}[(X_{1R}'X_{1R})^{-1} -
(X_{1T}'X_{1T})^{-1} ]
X_{1T}'\ep{1T} \rVert_2^2 \\ = O(1)
\E\{\lambda_{\max}(X_{1T}' \ep{1T} \ep{1T}'
    X_{1T}) \sum_{i=1}^{K_1}
    \lambda_i^2([X_{1R}'X_{1R}]^{-1} -
        [X_{1T}'X_{1T}]^{-1} )\}.
\]
By assumption, $\lambda_{\max}(X_{1T}'\ep{1T} \ep{1T}' X_{1T}) =
O_p(T)$.  The matrix $(X_{1R}'X_{1R})^{-1} - (X_{1T}'X_{1T})^{-1}$ has
rank $P$ and its eigenvalues are bounded by those of
$(X_{1R}'X_{1R})^{-1}$, so
  \[\sum_{i=1}^{K_1}\lambda_i^2((X_{1R}'X_{1R})^{-1} -
  (X_{1T}'X_{1T})^{-1} )  \wall \leq
  \sum_{i=K_1 - P + 1}^{K_1}\lambda_i^2((X_{1R}'X_{1R})^{-1})  \\ = O_p(P/T^2).
  \]
The moment bounds in Assumption~\ref{asmp-2} guarantee that
convergence in probability implies convergence in $L_2$, so
  \[
    \lVert x_1^{*\prime} [(X_{1R}'X_{1R})^{-1} -
        (X_{1T}'X_{1T})^{-1} ]
      X_{1T}'\ep{1T} \rVert_2 = O(\sqrt{P/T}).
  \]
  A similar argument shows that
  \[
    \lVert x_1^{*\prime} (X_{1R}'X_{1R})^{-1}\oosSum{t}{1} x_{1t}\varepsilon_{1T}
    \rVert_2 =   O(\sqrt{P/T}),
  \]
  completing the proof.
\end{proof}

\begin{proof}[Proof of Lemma \ref{res-variance-estimator}]
  The proof follows \citepos{DeD:00} Theorem 2.1
  closely.  We start by defining similar quantities to theirs,
  borrowing their notation when possible to make the similarities
  apparent.  For positive $\delta$, define $b_T \equiv \lfloor
  \frac\gamma\delta \rfloor$,
\begin{equation*}
  \kernelB{x} \equiv \kernelBDefn{x},
\end{equation*}
\begin{equation*}
  \varianceTermI \equiv \varianceTermIDefn,
\end{equation*}
\begin{equation*}
  \varianceTermII \equiv \vtSum \wall \varianceTermIIa\\ \times
  \varianceTermIIb,\return
\end{equation*}
\begin{equation*}
  \varianceTermIII \equiv \vtSum \wall \varianceTermIIIa\\ 
  \times \varianceTermIIb, \return 
\end{equation*}
\begin{equation*}
  \varianceTermIV \equiv \vtSum \wall \varianceTermIIIa \\
  \times \varianceTermIVb, \return
\end{equation*}
giving us the inequalities
\begin{equation*}
  \lVert \hat\sigma^2 - \sigma^2 \rVert_1 \wall 
  \leq \lVert \hat\sigma^2 - \varianceTermI \rVert_1 
  + \lVert \varianceTermI - \varianceTermII \rVert_1 
  + \lVert \varianceTermII - \varianceTermIII \rVert_1 
  + \lVert \varianceTermIII - \varianceTermIV \rVert_1\\\quad
  + \lVert \varianceTermIV - \E_R \varianceTermIV \rVert_1 
  + \lVert \E_R\varianceTermIII - \E_R\varianceTermIV \rVert_1
  + \lVert \E_R\varianceTermII - \E_R\varianceTermIII \rVert_1\\\quad 
  + \lVert \E_R\varianceTermI - \E_R\varianceTermII \rVert_1 
  + \lVert \E_R \varianceTermI - \sigma^2 \rVert_1
  \\ 
  \leq  \lVert \hat\sigma^2 - \varianceTermI \rVert_1 
  + 2(\lVert \varianceTermI - \varianceTermII \rVert_1 
      + \lVert \varianceTermII - \varianceTermIII \rVert_1 
      + \lVert \varianceTermIII - \varianceTermIV \rVert_1) \\ 
  \quad + \lVert \varianceTermIV - \E_R \varianceTermIV \rVert_1 
  + \lVert \E_R \varianceTermI - \sigma^2 \rVert_1.
\end{equation*}
De Jong and Davidson (2000) prove that
\begin{equation} \label{dd1}
\lim_{\delta\to0} \limsup_{T\to\infty} \lVert \varianceTermI -
\varianceTermII\rVert_1 = 0,
\end{equation}
\begin{equation} \label{dd2}
\lim_{\delta\to0} \limsup_{T\to\infty} \lVert \varianceTermII - \varianceTermIII
\rVert_1 = 0,
\end{equation}
\begin{equation} \label{dd3}
\lim_{\delta\to0} \limsup_{T\to\infty} \lVert \varianceTermIII - \varianceTermIV
\rVert_1 = 0,  
\end{equation}
and
\begin{equation} \label{dd4}
\lim \lVert \E_R \varianceTermI - \sigma^2 \rVert_1 = 0.
\end{equation}
Their proofs of \eqref{dd1}--\eqref{dd4} use the fact that \ned\
functions of mixing processes are also mixingale processes, and do not
use any properties specific to \ned\ processes, so they hold here as
well.  We do need to modify their proofs that
\begin{equation}
  \label{eq:4} \lVert \hat{\sigma}^2 - \varianceTermI \rVert_1 \to 0
\end{equation}
and
\begin{equation}
  \label{eq:5} \lVert \varianceTermIV - \E_R \varianceTermIV \rVert_1
  \to 0 \quad \text{for all positive $\delta$},
\end{equation}
though, since their proofs exploit \ned\ properties.

We'll start with (\ref{eq:4}). It follows from simple algebra that
\[
\lvert \hat\sigma^2 - \varianceTermI \rvert \leq \wall
\varianceDiffA \\ +
\frac1{P} \oosSum{s,t}{1} \lvert (D_{tR} - \oosA)(\E_R D_{sR} - \oosA) \rvert
\vWeight,
\]
so we'll show that each summation is $o_p(1)$.  The two
arguments are almost identical, so we'll only present the first.

Applying the Cauchy-Scwarz inequality twice and simplifying gives the
upper bound
\[
\varianceDiffA \\ \quad \leq O(1) [\varianceDiffAii]^{\frac12} [\varianceDiffAi]^{\frac12}.
\]
Now,
$\frac1P \oosSum{s}{1}(D_{sR} - \E_R D_{sR})^2 = O_p(1)$, and it suffices to prove
that \[\varianceDiffAi = o_p(1).\]  Observe that
\[
\varianceDiffAi \wall =  O_p(\frac1P \oosSum{s}{1}(\E_{R}D_{sR} -
\E_R\oosA)^2) \\
\quad + O_p(\oosA - \E_R\oosA)^{2} \\
= O_p(\frac1P \oosSum{s}{1} (\E_RD_{sR})^2 - (\E_R\oosA)^2) + o_p(1),    \return
\]
with the second term vanishing by Lemma \ref{res-mixingale} and
\citepos{Dav:93} mixingale \lln.

Now define $\tilde D_s$, $s = R+1,\dots,T-\h$, as in Lemma
\ref{lem-basic-coupling} so that $\E_{s-1} \tilde D_s = \E_R \tilde
D_s$ almost surely.  Note that we also have the almost sure equality
$\E_R \tilde D_s = \E_R \tilde D_{R+1}$ almost surely for all $s\geq R+1$,
and so
\[
\frac1P \oosSum{s}{1} (\E_R \tilde D_s)^2 = (\frac1P \oosSum{s}{1} \E_R
\tilde D_s)^2 \quad \text{a.s..}
\]

Consequently,
\[
\frac1P \oosSum{s}{1} (\E_R D_{sR})^2 - (\E_R\oosA)^2 \wall = 
\frac1P \oosSum{s}{1} [(\E_R D_{sR})^2 - (\E_R\tilde D_s)^2]\\
\quad+ (\frac1P \oosSum{s}{1} \E_R \tilde D_s)^2 - (\E_R\oosA)^2 \quad \text{a.s}.\\
= O_p(\frac1P \oosSum{s}{1} [(\E_R D_{sR})^2 - (\E_R \tilde D_s)^2]).
\]
Finally, 
\[
\lVert \frac1P \wall\oosSum{s}{1} [(\E_RD_{sR})^2 - (\E_R \tilde D_s)^2] \rVert_1
\\ \leq \frac1P \oosSum{s}{1} \lVert \E_RD_{sR} - \E_R \tilde D_s \rVert_2 \lVert
\E_RD_{sR} + \E_R \tilde D_s \rVert_2
\\ \leq \frac{4 B_L}P \oosSum{s}{1} \lVert \E_RD_{sR} - \E_R \tilde D_s \rVert_2,
\return
\]
and this last term vanishes as in the proof of Lemma
\ref{res-mixingale}.

\newcommand{\UFiltration}[1]{\ensuremath{\mathcal{F}_{(#1)b_{T}+R-P}}}%
Next, we'll prove (\ref{eq:5}). As in \citet{DeD:00}, let
\begin{equation*}
  Z_{1t} = \varianceTermIIIa,
\end{equation*}
and
\begin{equation*}
  Z_{2t} = \varianceTermIVb,
\end{equation*}
so
\begin{equation*}
  \varianceTermIV - \E_R \varianceTermIV = \vtSum (Z_{1t}Z_{2t} - \E_R Z_{1t}Z_{2t}),
\end{equation*}
and note that $\{Z_{1t}^2 \frac{P\gamma}{b_T}\}$ and $\{Z_{2t}^2
\frac{P\gamma}{b_T}\}$ are uniformly integrable.  As in the proof of Theorem
\ref{res-confidence-intervals}, we can assume that there is a constant
$C$ such that $Z_{1t}$ and $Z_{2t}$ are bounded in absolute value by
$C\sqrt{\frac{b_T}{P\gamma}}$; uniform integrability ensures that the
difference between the unbounded random variables and these truncated
versions is negligible for large enough values of $C$.  Let $r =
\lfloor \frac{3P}{2b} \rfloor$ and rewrite the summation as
\begin{equation*}
  \vtSum \vtIIIsummand \wall= \vtSumr \vtSuma \vtIIIsummand \\
  \quad+ \vtSumr \vtSumb \vtIIIsummand \\
  \quad+ \sum_{t=r b - P + 1}^{2P} \vtIIIsummand \\
  \equiv \vtSumr (U_i - \E_R U_i) + \vtSumr (U_i' - \E_R U_i') + o_{L_1}(1).\return
\end{equation*}
The proof then holds if we can show that both $U_i$ and $U_i'$ obey
\lln s.  We'll do so by proving that $\{U_i -\E_R U_i,
\UFiltration{2i-1}\}$ and $\{U_i' - \E_R U_i', \UFiltration{2i}\}$ are
$L_2$-mixingales of size $-1/2$ and using the bound $E(\vtSumr (U_i -
\E_R U_i))^2 = O(\vtSumr c_i^2)$ where the $c_i$ are the mixingale
magnitude indices \citep{Mcl:75}.

For non-negative $m$, we have 
\begin{equation*}
U_i - \E_R U_i \in \UFiltration{2i+2m-1},
\end{equation*}
establishing part of the mixingale result trivially.  Now fix $i$ and $m >
0$ and use Lemma \ref{lem-basic-coupling} to define $\tilde{D}_{ts}$
for each $t =(2i-2)b_T-P+1,\dots,(2i-1)b_T-P$ and $s =
\max(t+R-b_T,R-\h+1),\dots,\min(t+R+b_T,T)$ such that
\begin{equation*}
 \E_R \tilde D_{ts} = \E_{(2i-2m-1)b_T+R-P} \tilde D_{ts} \quad a.s.
\end{equation*}
and
\begin{equation*}
  \lVert \tilde D_{ts} - D_{sR} \rVert_2 \leq \couplingBound{s - (2i-2m-1)b_T-R+P}.
\end{equation*}
Also define
\begin{equation*}
  \tilde Z_{1t} = \frac1{\sqrt{P\gamma}} \sum_{l=\vttLower}^{\vttUpper}
  (\tilde D_{t,t+l+R} - \E_R\tilde D_{t,t+l+R})\ W(l/\gamma), \\
\end{equation*}
and
\begin{equation*}
\tilde Z_{2t} = \frac1{\sqrt{P\gamma}} \sum_{j=\vttLower}^{\vttUpper}
    (\tilde D_{t,t+l+R} - \E_R\tilde D_{t,t+l+R})\ \kernelB{j/\gamma}.
\end{equation*}

Now, we have the inequalities
\begin{equation*}
  \lVert \E( U_i - \wall \E_R U_i \mid \UFiltration{2i-2m-1}) \rVert_2
  \\ \leq
  \vtSuma \lVert \E(Z_{1t}Z_{2t} \mid \UFiltration{2i-2m-1}) - \E_R
  Z_{1t}Z_{2t} \rVert_2 \\
  = \vtSuma \lVert \wall \E(Z_{1t}Z_{2t} \mid \UFiltration{2i-2m-1}) \\
  - \E(\tilde{Z}_{1t}\tilde{Z}_{2t} \mid \UFiltration{2i-2m-1}) \\
  + \E(\tilde{Z}_{1t}\tilde{Z}_{2t} \mid \UFiltration{2i-2m-1})
  - \E_R Z_{1t}Z_{2t} \rVert_2 \return \\
%  \leq \vtSuma (\wall \lVert \wall \E(Z_{1t}Z_{2t} \mid \UFiltration{2i-2m-1}) \\
%  - \E(\tilde{Z}_{1t}\tilde{Z}_{2t} \mid \UFiltration{2i-2m-1}) \rVert_2\return
%  \\ + \lVert \E_R Z_{1t}Z_{2t} - \E_R \tilde{Z}_{1t}\tilde{Z}_{2t}
%  \rVert_2)\return \\
  \leq 2 \vtSuma \lVert Z_{1t} Z_{2t} - \tilde Z_{1t} \tilde{Z}_{2t}
  \rVert_2 \\
  \leq 2 \vtSuma (\lVert Z_{1t} - \tilde{Z}_{1t} \rVert_2 \lVert
  Z_{2t} \rVert_{\infty}
  + \lVert Z_{2t} - \tilde{Z}_{2t} \rVert_2 \lVert \tilde{Z}_{1t} \rVert_{\infty}) \\
  \leq 2 C \sqrt{\frac{b_T}{P\gamma}} \vtSuma (\lVert Z_{1t} - \tilde{Z}_{1t} \rVert_2 
  + \lVert Z_{2t} - \tilde{Z}_{2t} \rVert_2).
  \return
\end{equation*}

And we can finish the proof with the following inequalities:
\begin{equation*}
  2 C \wall \sqrt{\frac{b_T}{P \gamma}} \vtSuma \lVert Z_{1t} - \tilde{Z}_{1t} \rVert_2 \\ \leq
  \frac{4C \sqrt{b_T}}{P\gamma} \vtSuma \sum_{l=\vttLower}^{\vttUpper} \lVert
  D_{t+l+R,R} - \tilde{D}_{t,t+l+R} \rVert_2 W(l/\gamma) \\
  \leq O(\frac{\sqrt{b_T}}{P\gamma}) \vtSuma
  \sum_{l=\vttLower}^{\vttUpper} \couplingBeta{t+l-(2i-2m-1)b_T+P}\\
  = O(\frac{\sqrt{b_T}}{P\gamma}) O(b_T^{\frac32-u}\, m^{-\frac12-u})
\end{equation*}
for some positive $u$.  The same argument holds for $Z_{2t}$.
As a result,
\[
E(\vtSumr (U_i - \E_R U_i))^2 = o(\vtSumr \frac{b_T^2}{P\gamma}) =
o(\frac{3P}{2b_T} \frac{b^2}{P\gamma}) = o(\frac{b_T}{\gamma}) \to 0,
\]
as required.
\end{proof}


\begin{proof}[Proof of Theorem~\ref{res:ftest}]
  To clean up the notation, we'll write $\theta_1$ as $\theta$ and
  $q_{K_2-K_1,T-K_2}(1-\alpha)$ as $q_\alpha$.  Let
  $(\hat{\theta}, \hat{\psi})$ denote the \ols\ coefficient estimator
  using the full model and let 
  \[\hat\tau^2 = \frac{1}{T-K_2}\sum_{t=0}^{T-1} (y_{t+1} - x_{1t}'\hat{\theta} -
  x_{2t}'\hat{\psi})^2. \] Since the regressors are orthogonal,
  $\hat\theta$ also is the \ols\ estimator of $\theta$ using the
  restricted model.  The difference between the two models'
  generalization error is
\[
E_T \oosB = \sum_{j=K_1+1}^{K_2} [\psi_j^2 - (\hat{\psi}_j - \psi_j)^2].
\]
For any $\hat{\theta}$, $E_T \oosB$ equals zero when $\hat{\psi}$ lies
on the surface of the hypersphere centered at $\psi$ with radius
$|\psi|_2$, and $E_T \oosB$ and is greater than zero when $\hat{\psi}$
lies outside that sphere.  Also, the rejection region in $\Re^{K_2}$
for the F-test is the set of values of $\hat{\psi}$ on or beyond the
sphere
\[
\hat{\psi}'\hat{\psi} = \hat{\tau}^2 q_\alpha \frac{K_2-K_1}{T}.
\]

We can get a lower bound for the probability
\[\Pr[F \geq q_\alpha \mid E_T \oosB \leq 0]\]
by rewriting $\hat{\psi}$ in spherical coordinates re-centered at
$\psi$: $(\hat{r}, \hat{\omega}_1, \dots,
\hat{\omega}_{K_2-K_1-1})$, with 
\[\hat{r} = |\hat{\psi} - \psi|_2 \\
\hat{\omega}_{i} = \tan^{-1}(\frac{(\sum\nolimits_{j=K_1+i+1}^{K_2}(\hat{\psi}_j -
    \psi_j)^2)^{\frac12}}{\hat{\psi}_{K_1+i} - \psi_{K_1+i}}), \quad
  i=1,\dots,K_2-K_1-2, \\
\hat{\omega}_{K_2-K_1-1} =\atan(\hat{\psi}_{K_2-K_1} - \psi_{K_2-K_1},
  \hat{\psi}_{K_2-K_1-1}- \psi_{K_2-K_1-1} )
\]
and
\[
\atan(a, b) \equiv 
\begin{cases}
\tan^{-1}(a/b) & a > 0 \\
\pi + \tan^{-1}(a/b) & a < 0, b \geq 0 \\
-\pi + \tan^{-1}(a/b) & a < 0, b < 0 \\
\pi/2 & a = 0, b > 0 \\
- \pi/2 & a = 0, b < 0 \\
\text{undefined} & a = 0, b = 0.
\end{cases}
\]
Note that $\hat{\omega}_1,\dots,\hat{\omega}_{K_2-K_1 - 2} \sim
\operatorname{uniform}(-\pi/2, \pi/2)$ and
$\hat{\omega}_{K_2-K_1-1}\sim \operatorname{uniform}(-\pi, \pi)$.
Under this coordinate system, the acceptance region is centered at
$(|\psi|_2, \tilde{\omega}_1, \dots, \tilde{\omega}_{K_2-K_1-1})$ with
\[
\tilde{\omega}_i = \tan^{-1}([\sum_{j=i+1}^{K_2-K_1}
  (\psi_j/\psi_i)^2]^{\frac12}) \quad \text{for } i = 1,\dots,K_2-2,\\
\tilde{\omega}_{K_2-K_1-1} = \atan(\psi_{K_2-K_1}, \psi_{K_2-K_1-1}).
\]

If $\psi$ lies outside the acceptance region of the F-test, that
acceptance region is contained in the cone
\[\{(r, \omega_1,\dots,\omega_{K_2-1}) : r \geq 0,\, \tilde{\omega}_{i}
- \xi \leq \omega_i \leq \tilde{\omega}_i + \xi\}\]
with
\[
 \xi = \sin^{-1}(\frac{\hat{\tau}}{|\psi|_2}\sqrt{\frac{q_\alpha (K_2-K_1)}{T}})
\] 
for $\hat{\tau}^2$ given.\footnote{This setup assumes that
  $[\tilde\omega_i - \xi, \tilde\omega_i + \xi] \subset
  [-\pi/2,\pi/2]$ for each $i$.  If not, the region can be rotated
  without changing the results.}


As a result,
\[
\Pr[F \geq q_{\alpha}\wall \mid E_T \oosB \leq 0] = 
\Pr[\hat{\psi}'\hat{\psi} \geq \hat \tau^2 q_\alpha \frac{K_2 - K_1}{T}
\mid |\hat{\psi}-\psi|_2 \geq |\psi|_2] \\
\geq 1 - \E(\;\Pr[ \hat{\omega}_i \in [\tilde{\omega}_i - \xi, \tilde{\omega}_i +
\xi] \ \forall i  \mid \wall \hat{\tau}^2,
|\hat{\psi}-\psi|_2 \geq |\psi|_2] \\ 
\mid |\hat{\psi}-\psi|_2 \geq |\psi|_2)\quad a.s. \return \\ 
= 1 - \frac12 E ((2\xi / \pi)^{K_2-K_1-1}\mid
|\hat{\psi}-\psi|_2 \geq |\psi|_2)\quad a.s. \\
= 1 - \frac12 E (2\xi/\pi)^{K_2-K_1-1} \quad a.s..
\]
The second equality holds because the $\hat{\omega}_i$s are uniform
and the third holds because $\hat{\tau}$ is independent of
$\hat{\psi}$ under normality.

Now~\eqref{eq:ftestreject1} holds since $\xi \to 0$ almost surely as
$\phi_2 \to \infty.$  Moreover, as
$T\to\infty$, $\hat{\tau}^2\to^p\tau^2$ and $q_{\alpha}\to 1$, so
$\plim \xi < \frac{\pi}{2}$.  Consequently,
$(\frac{2\xi}{\pi})^{K_2-K_1-1}\to^p0$, proving~\eqref{eq:ftestreject2}.
\end{proof}
\bibliographystyle{abbrvnat}
\bibliography{AllRefs}

\begin{table}[h]
 \begin{multicols}{2}
  \begin{itemize}
  \item log Dividend Price Ratio
  \item log Earnings Price Ratio
  \item Stock Market Variance
  \item Book to Market Ratio
  \item Net Equity Expansion
  \item Percent Equity Issuing
  \item Treasury Bill
  \item Long Term Yield
  \item Long Term Rate
  \item Default Return Spread
  \item Default Yield Spread
  \item Inflation
  \end{itemize}
 \end{multicols}
 \caption{Variables used in Section \ref{sec:empirics} and by Goyal
   and Welch (2008) to predict the equity premium.  Please see Goyal
   and Welch's original paper
   for a detailed description of each variable.}
 \label{tab:equity}
\end{table}

\begin{table}[b]
 \begin{center}
 \begin{tabular}{lrrrr}\hline\hline
\multicolumn{1}{l}{waldtest}&\multicolumn{1}{c}{Res.Df}&\multicolumn{1}{c}{Df}&\multicolumn{1}{c}{F}&\multicolumn{1}{c}{Pr(\textgreaterF)}\tabularnewline
\hline
1&69&&&\\
2&81&$-$12&3.4&0.00066 \\
\hline
\end{tabular}
\caption{Wald test for the null hypothesis that all of the
  coefficients in Goyal and Welch's (2008) ``kitchen sink'' model are
  zero, except for the intercept, using the Newey-West
  variance-covariance matrix.  The p-value is calculated from the $F$
  distribution.\label{tab:gwinsample}}
\end{center}
\end{table}

\clearpage
\begin{figure}
\centering
\large{Simulated Coverage of One-Sided DMW Interval for $E_R\oosA$}
\includegraphics{mc/plots/interval-testerror1}
\large{Simulated Coverage of Two-Sided DMW Interval for $E_R\oosA$}
\includegraphics{mc/plots/interval-testerror2}
\caption{Simulated coverage of $E_R \oosA$ at 90\% confidence using a
  one- and two-sided intervals based on the \protect\textsc{dmw}
  \protect\textsc{oos} test, plotted as a function of the fraction of
  observations used in the test sample, $P/T$.  The solid horizontal
  line denotes the intervals' nominal coverage.}
 \label{fig:interval-R}
\end{figure}
\clearpage
\begin{figure}
\centering
\large{Simulated Coverage of One-Sided DMW Interval for $E_T\oosB$}
\includegraphics{mc/plots/interval-generror1}
\large{Simulated Coverage of Two-Sided DMW Interval for $E_T\oosB$}
\includegraphics{mc/plots/interval-generror2}
\caption{Simulated coverage of $E_T \oosB$ at 90\% confidence using a
  one- and two-sided intervals based on the \protect\textsc{dmw}
  \protect\textsc{oos} test, plotted as a function of the fraction of
  observations used in the test sample, $P/T$.  The solid horizontal
  line denotes the intervals' nominal coverage.}
\label{fig:interval-T}
\end{figure}
\clearpage

\begin{figure}
  \centering
\large{Simulated Rejection Probability of F-test Under $E_T
      \oosB \leq 0$}
  \includegraphics{mc/plots/ftest-001}
  \caption{Simulated rejection probabilities for the F-test given
    $\E_T \oosB \leq 0$ with nominal size 10\%.  Values greater than
    10\% indicate that the test rejects the benchmark model too often.
    See Section~\ref{sec:simulation-design} for a discussion of the
    simulation design.}
  \label{fig:ftest}
\end{figure}
\clearpage
\begin{figure}
  \centering
  \large{Simulated Rejection Probability of DMW Test Under $E_T
      \oosB \leq 0$}
  \includegraphics{mc/plots/size-dmwsize}  
  \caption{Simulated rejection probabilities for the \protect\textsc{dmw}
    \protect\textsc{oos} $t$-test given $\E_T \oosB \leq 0$ with nominal
    size 10\%.  Values greater than 10\% indicate that the test
    rejects the benchmark model too often.  The solid horizontal line
    indicates the nominal rejection probability.  See
    Section~\ref{sec:simulation-design} for a discussion of the
    simulation design.}
  \label{fig:ttest-size}
\end{figure}

\begin{figure}
  \centering
  \large{Simulated Rejection Probability of Clark and West
      (2006, 2007) Test \\ Under $E_T \oosB \leq 0$}
  \includegraphics{mc/plots/size-clarkwest}
  \caption{Simulated rejection probabilities for Clark and West's
    (2006, 2007) \protect\textsc{oos} test statistic given $\E_T \oosB
    \leq 0$ with nominal size 10\%.  Values greater than 10\% indicate
    that the test rejects the benchmark model too often.  The solid
    horizontal line indicates the nominal rejection probability.  See
    Section~\ref{sec:simulation-design} for a discussion of the
    simulation design.}
   \label{fig:clarkwest}
\end{figure}

\begin{figure}
  \centering
  \large{Simulated Rejection Probability of McCracken (2007)
      Test \\ Under $E_T \oosB \leq 0$}
  \includegraphics{mc/plots/size-mccracken}
  \caption{Simulated rejection probabilities for McCracken's (2007)
    \protect \textsc{oos} test statistic given $\E_T \oosB \leq 0$ with
    nominal size 10\%.  Values greater than 10\% indicate that the
    test rejects the benchmark model too often.  The solid horizontal
    line indicates the nominal rejection probability.  See
    Section~\ref{sec:simulation-design} for a discussion of the
    simulation design.}
  \label{fig:mccracken}
\end{figure}

\begin{figure}
  \centering
  \large{Simulated Rejection Probability of DMW Test Under $E_T
      \oosB > 0$}
  \includegraphics{mc/plots/size-dmwpower}
  \caption{Simulated rejection probabilities for the \protect \textsc{dmw}
    \protect\textsc{oos} $t$-test given $\E_T \oosB > 0$ with nominal
    size 10\%.  Values greater than 10\% indicate that the test
    rejects the benchmark model too often.  The solid horizontal line
    indicates the nominal rejection probability.  See
    Section~\ref{sec:simulation-design} for a discussion of the
    simulation design.}
  \label{fig:ttest-power}
\end{figure}

\begin{figure}
\centering
\large{Difference in OOS MSE of Prevailing Mean and\\ Kitchen
    Sink Models of Equity Premium (OLS)}
\includegraphics{empirics/plots/oos-mse-1}
\includegraphics{empirics/plots/oos-mse-1b}
\caption{\protect \textsc{oos} difference in the \protect\textsc{mse}
  of the prevailing mean benchmark and the kitchen sink model as a
  function of the test sample size, $R$.  Both models forecast the
  equity premium using annual data from 1928--2008.  The solid line
  gives the \protect \textsc{oos} average, and the shaded region indicates the
  one-sided 95\% confidence interval implied by the \protect
  \textsc{dmw} test.  The bottom panel is a detailed view of the top
  panel for $R \geq 50$.}
\label{fig:empirics1}
\end{figure}

\begin{figure}
\centering
\large{Difference in OOS MSE of Prevailing Mean and\\ Kitchen
    Sink Models of Equity Premium (CT)}
\includegraphics{empirics/plots/oos-mse-2}
\includegraphics{empirics/plots/oos-mse-2b}
\caption{\protect \textsc{oos} difference in the \protect\textsc{mse}
  of the prevailing mean benchmark and the kitchen sink model as a
  function of the test sample size, $R$.  Both models forecast the
  equity premium using annual data from 1928--2008.  The solid line
  gives the \protect \textsc{oos} average, and the shaded region indicates the
  one-sided 95\% confidence interval implied by the \protect
  \textsc{dmw} test.  The bottom panel is a detailed view of the top
  panel for $R \geq 50$.}
\label{fig:empirics2}
\end{figure}

\begin{figure}
\centering
\large{OOS MSE of Individual Forecasts of Equity Premium}
\includegraphics{empirics/plots/oos-ind-ks}
\includegraphics{empirics/plots/oos-ind-pm}
\caption{\protect{\textsc{oos} \textsc{mse} of the Prevailing Mean (\textsc{pm}) and
    Kitchen Sink (\textsc{ks}) models for equity premium prediction as
    a function of the size of the training sample, $R$.  Please note
    that the vertical scales are different in the two plots.}}
\label{fig:empirics3}
\end{figure}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "paper"
%%% End: 

% LocalWords: LM clark meese diebold chao corradi mccracken giacomini lm jel iR
% analyses inoue datasets huber efron anova akritas
% calhoun iR berbee th de nondifferentiable jong
% mixingales mcleish AIC Berbee's merlevede Heyde's ib
% mixingaleR TP eq dx dy tex tR tS tT iT sR sw tw vw
% reestimate davidson over-reject anatolyev Anatolyev's iS izx
% coefpart normalpart Jong's jt oos underrejects cltvar Scwarz JEL
% hansen lm McCracken's jel RSQLite elemstatlearning rproject rsqlite
% LocalWords:  McCracken's kilian anatolyev rossi REStud JAppliedEconometrics
% LocalWords:  goyal welch rfs reputational calhoun Berbee's mixingales tR tS
% LocalWords:  Heteroskedasticity Autocorrelation tT iT th berbee iS de dx ixz
% LocalWords:  DMW mcleish sR eq ftestreject lightgray veclen RSQLite zeileis
% LocalWords:  heteroskedasticity hothorn RNews JStatSoftware harrell campbell
% LocalWords:  thompson bivariate Welch's multicollinearity Newey newey tw vw
% LocalWords:  merlevede cltvar dR Scwarz davidson indices lrrrr waldtest Df jt
% LocalWords:  dmw McCracken MSE OLS mse inoue huber elemstatlearning efron sw
% LocalWords:  anova akritas jong normalpart coefpart Jong's rproject rsqlite
% LocalWords:  unrestrictive datasets Heyde's ib oos Mcc MeR StW InK DiM CCS
% LocalWords:  ClM CoS ClW GiW GiR GoW Cla HTF Efr BoB AkA AkP DeD Mcl Dej Rde
% LocalWords:  Sar Zeh Zei Har CaT NeW MeP HaH AllRefs
