\documentclass[11pt]{article}
\usepackage{tikz,setspace}
\usetikzlibrary{calc}
\usetikzlibrary{fit}
\usepackage[round]{natbib}
\usepackage[margin=1in]{geometry}
\onehalfspacing
\input{macros}

\newcommand{\e}{\varepsilon}
\newcommand{\eb}{\mathbf{\e}}
\DeclareMathOperator{\eigen}{\lambda}

\pagestyle{myheadings}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{cor}{Corollary}
\newtheorem{asmp}{Assumption}
\newtheorem{example}{Example}
\newtheorem{defn}{Definition}

\newcommand{\citepos}[1]{\citeauthor{#1}'s \citeyearpar{#1}}
\newcommand{\Citepos}[1]{\Citeauthor{#1}'s \citeyearpar{#1}}
\newcommand{\clws}{\citeauthor{ClW:06}'s \citeyearpar{ClW:06,ClW:07}}

\input{VERSION.tex}
\title{Out-of-Sample Comparisons of Overfit Models}
\author{Gray Calhoun\thanks{email: \texttt{gcalhoun@iastate.edu}. I
    would like to thank Julian Betts, Helle Bunzel, Marjorie Flavin,
    Nir Jaimovich, Lutz Kilian, Ivana Komunjer, Michael McCracken,
    Seth Pruitt, Ross Starr, Jim Stock, Yixiao Sun, Allan Timmermann,
    Hal White, several anonymous referees, participants at the 2010
    Midwest Economics Association Annual Meetings, the 2010
    International Symposium on Forecasting, the 2010 Joint Statistical
    Meetings, the 2011 \textsc{nber}/\textsc{nsf} Time-Series
    Conference, and in many seminars at \textsc{ucsd}, and espeically
    Graham Elliott for their valuable suggestions, feedback and advice
    in writing this paper.  I would also like to thank Amit Goyal for
    providing computer code and data for his 2008 \textsc{rfs} paper
    with Ivo Welch \citep{GoW:08}.} \\ Iowa State University}

\begin{document}
\maketitle

\begin{abstract}\thispagestyle{empty}\noindent
  This paper uses dimension asymptotics to study why overfit linear
  regression models should be compared out-of-sample; we let the
  number of predictors used by the larger model increase with the
  number of observations so that their ratio remains uniformly
  positive.  Our analysis gives a theoretical motivation for using
  \oos\ comparisons: the \dmw\ \oos\ test allows a forecaster to
  conduct inference about the expected future accuracy of his or her
  models when one or both is overfit.  We show analytically and
  through Monte Carlo that standard full-sample test statistics can
  not test hypotheses about this performance.  Our paper also shows
  that popular test and training sample sizes may give misleading
  results if researchers are concerned about overfit.  We show that
  $P^2/T$ must converge to zero for the \dmw\ test to give valid
  inference about the expected forecast accuracy, otherwise the test
  measures the accuracy of the estimates constructed using only the
  training sample.  In empirical research, $P$ is typically much
  larger than this.  Our simulations indicate that using large values
  of $P$ with the \dmw\ test gives undersized tests with low power, so
  this practice may favor simple benchmark models too much.

\noindent \textsc{jel} Classification: C12, C22, C52, C53

\noindent Keywords: Generalization Error, Forecasting, Model
Selection, $t$-test, Dimension Asymptotics
\end{abstract}
\newpage

\section{Introduction}\label{sec:introduction}
Consider two sequences of length $P$ of prediction errors, the result
of forecasting the same variable with two different estimated models.
Both models are estimated with $R$ observations, collectively called
the {\em estimation window}, and are used to forecast an additional
$P$ observations, called the {\em test sample}.  There are $T$
observations in all, and $R+P=T$.  This paper introduces a new limit
theory for statistics constructed from these prediction errors
designed to approximate the behavior of the statistics when one of the
models is overfit.  In doing so, we provide a theoretical
justification for forecasters to use \oos\ instead of in-sample
comparisons: the \dmw\ \oos\ test\footnote{In this paper, we will
  refer to the basic \oos\ $t$-test studied by \citet{DiM:95} and
  \citet{Wes:96} as the \dmw\ test.} allows a forecaster to conduct
inference about the expected future accuracy of his or her models when
one or both is overfit.  We show analytically and through Monte Carlo
that standard full-sample test statistics can not test hypotheses
about this performance.

Our paper also shows that popular test and training sample sizes may
give misleading results if researchers are concerned about overfit.
We show that $P^2/T$ must converge to zero for the \dmw\ test to give
valid inference about the expected forecast accuracy, otherwise the
test measures the accuracy of the estimates constructed using only the
training sample.  In empirical research, $P$ is typically much larger
than this.  Our simulations indicate that using large values of $P$
with the \dmw\ test gives undersized tests with low power, so this
practice may favor simple benchmark models too much.  Existing
corrections, proposed by \citet{ClM:01,ClM:05}, \citet{Mcc:07} and
\citet{ClW:06,ClW:07}, seem to correct too much, though, and reject
too often when the benchmark model is more accurate.


Although \oos\ comparisons have been popular in Macroeconomics and
Finance since \citepos{MeR:83} seminal study of exchange rate models,
it has been unclear from a theoretical perspective whether or not the
statistics are useful.  Empirical researchers often cite ``overfit''
or ``instability'' as reasons for using \oos\ comparisons, as in
\citet{StW:03}, but neither term is precisely defined or formalized.
Compounding this problem, the asymptotic distributions of these
statistics are derived under conditions that rule out either
instability or overfit and allow a researcher to use a conventional
in-sample comparison---a variation of the $F$-test, for example.  As
\citet{InK:04} argue, the statistics themselves are designed to test
hypotheses that can be tested by these in-sample statistics.  For
example, \citet{DiM:95} and \citet{Wes:96} derive the limiting
distributions of many popular \oos\ test statistics under conditions
that would justify these full-sample tests.  Much
of the subsequent research by \citet{Mcc:00, Mcc:07}, \citet{CCS:01},
\citet{ClM:01,ClM:05}, \citet{CoS:02,CoS:04}, \citet{ClW:06,ClW:07},
\citet{Ana:07}, and others relaxes several of Diebold and Mariano's
and West's assumptions, but maintains the stationarity and dependence
conditions that permit in-sample comparisons \citep[see][for a
review of this literature]{Wes:06}.\footnote{Like us, \citet{Ana:07}
  allows the number of regressors to increase with $T$.  But in that
  paper, they increase slowly enough that the \ols\ coefficients are
  consistent and asymptotically normal.}  \citet{GiW:06} and
\citet{GiR:09, GiR:10} are exceptions.  Instead of focusing on
hypotheses that can be tested by in-sample comparisons, \citet{GiW:06}
derive an \oos\ test for the null hypothesis that the difference
between two models' \oos\ forecasting performance is unpredictable, a
martingale difference sequence; \citet{GiR:09} test whether the \oos\
forecasting performance of a model suffers from a breakdown relative
to its in-sample performance; and \citet{GiR:10} test whether the
forecasting performance is stable. However, those papers focus on a
particular \oos\ estimation strategy and do not address why \oos\
comparisons might be useful as a general strategy.

Since in-sample and \oos\ statistics require similar assumptions and
test similar hypotheses, one might expect that they would give similar
results.  They do not.  Generally in-sample analysis supports more
complicated theoretical models and \oos\ analysis supports simple
benchmarks, as seen in \citet{MeR:83}, \citet{StW:03}, and
\citet{GoW:08}.  Since these different approaches strongly influence
the outcome of research, it is important to know when each is
appropriate.  The explanations in favor of \oos\ comparisons claim
that they should be more robust to unmodeled instability
\citep{ClM:05,GiW:06,GiR:09,GiR:10} or to overfit
\citep{Mcc:98,Cla:04}.  Both explanations presume that the in-sample
comparison is invalid and the \oos\ comparisons are more reliable.  Of
course, as \citet{InK:04,InK:06} point out, both in-sample and \oos\
methods could be valid, but the \oos\ methods could have lower power.

In this paper, we study the ``overfit'' possibility and leave
``instability'' to future research. This paper uses dimension
asymptotics to study the behavior of \oos\ comparisons when at least
one of the models is overfit---the number of its regressors
increases with the number of observations so that their ratio remains
positive.  Although overfit is sometimes used to describe the
situation where a forecaster chooses from many different models,
i.e. \textit{data-mining} or \textit{data-snooping}, we view these as
separate issues.  Procedures that account for the presence of many
models have been, and are continuing to be, developed \citep[see, for
example,][]{Whi:00,Han:05,RoW:05,HHK:10,ClM:12b}, but it is unclear
whether those procedures should themselves use in-sample or \oos\
comparisons.  Understanding the difference between in-sample and \oos\
comparisons in the context of a simple comparison between two models
is necessary before resolving any new issues that arise with multiple
comparisons.\footnote{Moreover, the empirical research that motivates
  this paper uses \textit{pseudo} \oos\ comparisons and not
  \textit{true} \oos\ comparisons.  Even if a true \oos\ comparison
  could account for some forms of data-snooping better than
  \citepos{Whi:00} \textsc{brc} or its extensions, in-sample and
  pseudo \oos\ comparisons would both be affected by the data-snooping.
  This point is also made by \citet{InK:04}.}

We focus on linear regression models for simplicity, but our basic
conclusions should be true for other models as well.  Under this
asymptotic theory, where the number of regressors $k$ increases with $T$
so that $\lim k/T$ is positive,
the \ols\ coefficient estimator is no longer
consistent or asymptotically normal \citep{Hub:73} and has positive
variance in the limit.  We show that, even so, the usual \oos\ average
is asymptotically normal and can consistently estimate the difference
between the models' generalization error, the expected loss in the
future conditional on the on the data available in period
$T$.\footnote{See, for example, \citet{HTF:08} for a discussion of
  generalization error.}  Under these asymptotics, the generalization
error does not converge to the expected performance of the pseudo-true
models, so existing in-sample and \oos\ comparisons measure different
quantities and should be expected to give different results for
reasons beyond simple size and power comparisons.  Under our limit theory,
the model that is closer to the true \dgp\ in population can forecast
worse.  In such a situation, a standard in-sample comparison would
correctly reject the null hypothesis that the benchmark is true, and
an \oos\ comparison would correctly fail to reject the null that the
benchmark is more accurate.\footnote{In a pair of papers similar to
  ours, \citet{ClM:12,ClM:12b} study in-sample and \oos\ tests that
  the larger model has nonzero coefficients that are too close to zero
  to expect the model to forecast more accurately.  Like this paper,
  they argue that the larger model can be true but less accurate.
  However, they focus on an aspect of the \dgp\ that makes this
  phenomenon likely, while we focus on the coefficient estimates that
  produce less accurate forecasts.  Moreover, the implications of our
  asymptotic theories are different and their papers do not provide
  reasons to do \oos\ comparisons.}
Also note that, in this situation, a model that performs well
in-sample can perform badly out-of-sample \emph{even if there are no
structural breaks or other forms of instability.}  Researchers
have argued that a breakdown of in-sample forecasting ability
indicates a structural break (see \citealp{BoH:99} and
\citealp{StW:03} among others), but this breakdown can be caused by overfit as
well.

Our results can justify \oos\ comparisons when researchers want to
choose a model for forecasting.  Although there has been little
emphasis on hypothesis testing in this setting, testing is usually
appropriate: there is usually a familiar benchmark model in place, and
the cost of incorrectly abandoning the benchmark for a less accurate
alternative model is higher than the cost of incorrectly failing to
switch to a more accurate alternative.  We show that the \dmw\ test
lets the forecaster control the probability of the first error, just
as with conventional hypothesis testing.

Since the models' coefficients are imprecisely estimated in the limit,
the test sample must be small enough that the model estimated over the
training sample is similar to the one that will be estimated over the
full sample.  In particular, $P/T \to 0$ is required for consistent
estimation of the difference in the two models' performance,
and $P^2/T \to 0$ is required for valid confidence
intervals and inference.  For larger $P$, the \oos\ comparisons remain
asymptotically normal, but are centered on the forecasting performance
associated with the period-$R$ estimates.  In practice, researchers
typically use large values of $P$, so these studies may be too
pessimistic about their models' future accuracy if they use the \dmw\
test.  Section~\ref{sec:oostheory} lays out the asymptotic behavior of
the \dmw\ test under this limit theory.

Since the \ols\ coefficient estimator is no longer consistent or
asymptotically normal, the $F$-test and Wald tests are no longer
asymptotically valid.  Moreover, \citet{Efr:86,Efr:04} shows that
naive comparisons of in-sample loss overestimate models' accuracy by a
factor proportional to $K/T$; $K$ is the number of regressors.  But
modifications of the $F$-test are valid under this asymptotic theory,
as shown by \citet{BoB:95}, \citet{AkA:00}, \citet{AkP:04},
\citet{Cal:11c}, and \citet{Ana:12}, among others.  These results
establish that hypotheses about the parameters of the \dgp\ can be
tested in-sample even if one or both models are overfit.  For that
very reason, these tests can not reliably choose a model for
forecasting and will reject the benchmark too often.  We show this
analytically in Section~\ref{sec:insample} and present analogous
results for model selection.  Moreover, under this asymptotic theory,
many recent \oos\ test statistics, such as those derived by
\cite{ClM:01,ClM:05}, \citet{Mcc:07}, and \citet{ClW:06,ClW:07} should
have the same problems as in-sample tests.\footnote{Our theoretical
  results apply directly to \citepos{Mcc:07} \oos\ $t$-test, since it
  simply proposes more liberal critical values for the same test
  statistic that we study.  Since \citet{ClW:06,ClW:07} use a finite
  length estimation window, our asymptotics are incompatible with
  theirs and prevent us from studying their test directly, as well as
  \citepos{GiW:06} and other tests based on \citepos{GiW:06}
  asymptotics.  But \clws\ test can be viewed as a stochastic
  adjustment to the critical values of the usual \oos\ $t$ test, so
  our conclusions should apply informally as well.  Specifically, we
  show that the \dmw\ test rejects with probability equal to nominal
  size when the estimated benchmark model is expected to be more
  accurate, so more liberal critical values result in overrejection.}
These tests are also designed to reject the benchmark when the
alternative model is true, and so they may reject too often when the
benchmark is false but more accurate.  Obviously, since the
distribution of these statistics converges to the normal when $P/T \to
0$ (with the number of regressors fixed), these statistics behave like
the \dmw\ test when $P$ is small, but should overreject the benchmark
when $P$ is large.  Section~\ref{sec:mc} presents simulations that
support these claims.

Finally, this paper also introduces a new method of proof for \oos\
statistics.  We use a coupling argument (Berbee's Lemma, 1979) to show
that sequences of \oos\ loss behave like mixingales when the
underlying series are absolutely regular, even if the forecasts depend
on non-convergent estimators.  Moreover, transformations of these
processes also behave like mixingales when recentered, so asymptotic
results for \ned\ functions of mixing processes can be used for these
\oos\ processes with only slight modification.

The rest of the paper proceeds as follows.
Section~\ref{sec:assumptions} introduces our notation and assumptions.
Section~\ref{sec:theory} gives the main theoretical results for the
\dmw\ \oos\ test, shows that standard in-sample tests reject the
benchmark model too often when it is false but more accurate than the
alternative, and shows that standard model selection methods choose the
larger model too often under similar conditions. Section~\ref{sec:mc}
presents a Monte Carlo study supporting our theoretical results.
Section~\ref{sec:empirics} applies the \oos\ statistic to
\citepos{GoW:08} dataset for equity premium prediction, and
Section~\ref{sec:conclusion} concludes.  Proofs and supporting results
are listed in the Appendix.

\section{Setup and Assumptions}\label{sec:assumptions}
We will set up our models and notation first, then list the assumptions
required for our results.

\subsection{Notation and Forecasting Environment}
We assume the following forecasting environment. There are two
competing linear models that give forecasts for the target,
$y_{t+\h}$:
\[
y_{t+\h} = x_{1t}'\theta_1 + \e_{1,t+h}, \quad\text{and}\quad
y_{t+\h} = x_{2t}'\theta_2 + \e_{2,t+h};
\]
$t = 1,\dots,T-h$, $\h$ is the forecast horizon, and the
variables $y_t$, $x_{1t}$, and $x_{2t}$ are all known in period $t$.
Let
\begin{equation*}
  \mathcal{F}_t = \sigma(y_1, x_1, \dots, y_t, x_t)
\end{equation*}
with $x_t$ the vector of all elements of $x_{1t}$ and $x_{2t}$ after
removing duplicates, and let $\E_t$ and $\var_t$ denote the
conditional mean and variance given $\mathcal{F}_t$.  The first model
uses $K_1$ regressors, and the second uses $K_2$.  Without loss of
generality, assume that $K_1 \leq K_2$.  We will allow $K_1$ and $K_2$
to vary with $T$, so a stochastic array underlies all of our
asymptotic theory, but we suppress that notation to simplify the
presentation.

The forecaster observes the data $(y_t,x_t)$ for periods 1 through $T$
and divides them into an estimation sample of the first $R$
observations and a test sample of the remaining $P$ observations.
After choosing one of the models, the forecaster will use that model
to make predictions for periods $T + h + 1$ through $T + h + Q$, where
$Q$ is a known positive integer.  The forecaster's goal is to choose
the model that will have lower average loss over periods $T+h$ through
$T+h+Q$, evaluated by the known loss function $L$.  We allow $Q = 1$,
so this setup generalizes the problem of choosing a model for a single
prediction.

Both models are estimated using \ols\ with a fixed window:
\[
\hat y_{i,t+\h} = x_{it}'\hat{\theta}_{it}, \qquad i=1,2, \qquad t
= R+1,\dots,T-h
\]
with
\begin{equation*}
  \bh{it} =
  \begin{cases}
    \big(\sum_{s=1}^{R-\h} x_{is}x_{is}'\big)^{-1} \sum_{s=1}^{R-\h}
    x_{is} y_{s+\h} & t \leq T \\
    \big(\sum_{s=1}^{T-\h} x_{is}x_{is}'\big)^{-1}
    \sum_{s=1}^{T-\h} x_{is} y_{s+\h} & t > T.
  \end{cases}
\end{equation*}
The observed \oos\ average is denoted $\oosA$: \[\oosA \equiv P^{-1}
\oosSum{t}{1} D_{Rt}\]  with
\[
D_{Rt} \equiv L(y_{t+\h} - x_{1t}'\bh{1t}) - L(y_{t+\h} - x_{2t}'\bh{2t}).
\]
The \oos\ statistic of interest is $\oost \equiv \sqrt{P} \oosA /
\hat\sigma_R$, for $\hat\sigma_R^2$ some (possibly Heteroskedasticity- and
Autocorrelation-Consistent, or \hac) estimator of the asymptotic
variance of $\oosA$.

As discussed above, the forecaster's goal is to choose the model with
lower average future loss; i.e. to determine whether $\oosB$ is
positive or negative,with
\[
\oosB \equiv Q^{-1} \sum_{t=T+1}^{T+Q} D_{Tt}, \quad
D_{Tt} \equiv L(y_{t+h} - x_{1t}' \bh{1t}) - L(y_{t+h} - x_{2t}' \bh{2t}).
\]
Note that the coefficients are reestimated with the full dataset.
Since the forecaster's information set, $\mathcal{F}_T$, does not
contain $\oosB$, he or she should decide between the models based on
the conditional expectation, $\E_T \oosB$. If this quantity is
positive, the first model is expected to perform the worst in the
future, and if it is negative the second is.

This conditional expectation is related to several well known measures
of model performance.  In the limit, $\E_{T} \bar{D}_{T}$ converges to
the difference in the models' generalization error and the two are
equal in finite samples for i.i.d. data.  Moreover, if the data are
i.i.d., the expectation of $\E_T \oosB$ equals \citepos{Aka:69} Final
Prediction Error (\textsc{fpe}).  Both generalization error and
\textsc{fpe} are defined by the model's performance on a new,
independent, data set; here the new data are not independent in finite
samples, but are asymptotically independent.  For the rest of the
paper, we will call $\E_T \oosB$ the difference in generalization
error with hopefully no risk of confusion.

Finally, define the following notation.  The $l_v$-norm for vectors in
$\Re^p$ (with $p$ arbitrary) is denoted $\lvert \cdot \rvert_v$, and
the $L_v$-norm for $L_v$-integrable random variables is $\lVert \cdot
\rVert_v$.  The functions $\eigen_i(\cdot)$ take a square-matrix
argument and return its $i$th eigenvalue (with $\eigen_{i}(A) \leq
\eigen_{i+1}(A)$ for any matrix $A$).  All limits are taken as $T \to
\infty$ unless stated otherwise.

\subsection{Assumptions}

We assume the next conditions hold throughout the paper.  The first
assumption controls the dependence of the underlying random array.
The second prevents the sequence of experiments from becoming
degenerate as $T$ increases.  The third assumption controls the
smoothness of the loss function and bounds the moments of the
difference in the models' performance; the fourth assumption describes
the behavior of the estimation and test windows.  And the last
assumption describes the kernel used to estimate the \oos\ average's
asymptotic variance.

\begin{asmp}\label{asmp-1}
  The random array $\{y_t,x_t\}$ is stationary and absolutely regular
  with coefficients $\beta_j$ of size $-\rho/(\rho-2)$; $\rho$ is
  greater than two and discussed further in Assumption \ref{asmp-3}.
  Also, $K_1$ and $K_2$ are less than $R$, and $K_2/T$ and
  $(K_2-K_0)/T$ are uniformly positive; $K_0$ is the number of
  regressors shared by the two models ($K_1/T$ can be uniformly
  positive as well, but is not required to be).
\end{asmp}

The first assumption is a standard condition on the dependence of the
underlying stochastic array.  We assume that the array is absolutely
regular instead of strong mixing, which is more common in
econometrics, because absolute regularity admits a particular coupling
argument \citep[reproduced in this paper as Lemma A.1]{Ber:79} that is
unavailable for strong mixing sequences.  Please see, for example,
\citet{Dav:94} for a discussion of these mixing conditions.

Assumption \ref{asmp-2} rules out uninteresting cases where the
forecast error vanishes.
\begin{asmp}\label{asmp-2}
  The variance of $y_{t+\h}$ given $\mathcal{F}_t$ is uniformly
  positive and finite, and all of the eigenvalues of the covariance
  matrix of $x_t$ are uniformly positive and finite as well.  The
  Euclidean norms of the pseudo-true coefficients, $\theta_1$ and
  $\theta_2$, satisfy $|\theta_1|_2 = O(1)$ and $|\theta_2|_2 = O(1)$.
  Moreover, there exists a finite $\Delta$ such that $\| x_{it}'x_{it}
  \|_3 \leq K_i \Delta$,
  \begin{align*}
    \lVert \eigen_{\max}(X_{iS_{T}}'X_{iS_{T}}) \rVert_{3} &\leq
    S_{T}\, \Delta, & \lVert
    \eigen_{\max}(X_{iS_{T}}'\ep{iS_{T}}\ep{iS_{T}}'X_{iS_{T}})
    \rVert_{3} &\leq S_{T}\, \Delta, \\
    \lVert \eigen_{\max}((X_{iS_{T}}'X_{iS_{T}})^{-1})
    \rVert_{3} &\leq \Delta/S_{T}, & \lVert
    \eigen_{\max}((X_{iS_{T}}'\ep{iS_{T}}\ep{iS_{T}}'X_{iS_{T}})^{-1})\rVert_{3}
    &\leq \Delta/S_{T},
  \end{align*}
  for all $S_{T} \geq K_{2T}$ and large enough $T$, with $i =
  1,2$,
  \[ X_{iT} \equiv [x_{i1} \quad \dots \quad x_{i,T-\h}]' \qquad
  \text{and} \qquad
  \ep{iT} = (\e_{i,1+\h}, \dots, \e_{i,T})'.\]
\end{asmp}

The assumption that $y_{t+\h}$ and $x_t$ have positive and finite
variance is straightforward.  The conditions on the eigenvalues are
technical and control the behavior of the \ols\ estimator as the
number of regressors gets large.  The restrictions on the pseudo-true
coefficients ensure that the regression model doesn't dominate the
variance of $y_{t+\h}$ in the limit.

\begin{asmp}\label{asmp-3}
  The loss function $L$ is continuous and has finite left and right
  derivatives almost everywhere.  Also, $L(0) = 0$, and there is a
  constant $B_L$ such that $\|D_{R,R+j}\|_\rho \leq B_L$ and
  $\|D_{T,T+j}\|_\rho \leq B_L$ for all $j \geq 1$ ($\rho$ is defined
  in Assumption~\ref{asmp-1}); $\| x_{it}' \hat{\theta}_{is}^R \|_2 \leq
  B_L$ and $\| x_{it}' \hat{\theta}_{is}^T \|_2 \leq B_L$; and
  \begin{equation*}
    \| L'(y_{t+h} - x_{it}'\tilde{\theta}_{it}) \|_2 \leq B_L
  \end{equation*}
  for $\tilde{\theta}_{it}$ a weighted average of any two estimates
  $\hat{\theta}_{is}^R$ and $\hat{\theta}_{is'}^R$ or
  $\hat{\theta}_{is}^T$ and $\hat{\theta}_{is'}^T$ with $s, s' \leq t$.
\end{asmp}

This third assumption establishes basic moment and smoothness
conditions for the \oos\ loss and forecasts.  The differentiability
condition is weak and allows the loss function itself to be
non-differentiable or even discontinuous; for example, absolute error
and many asymmetric loss functions satisfy this assumption.

\begin{asmp} \label{asmp-4} $P, R, Q \to\infty$ and $P^2/T \to 0$ as
  $T \to \infty$.
\end{asmp}

The requirements that $P$ and $R$ grow with $T$ are common.  Parts of
the assumption are new, in particular the requirement that $P^2/T \to
0$.  See Lemma \ref{res-convergence} for a discussion of its
implications.  In practical terms, this assumption requires that the
test sample be large and that the training sample be much larger, by
enough that including or excluding the test sample does not affect the
estimates of $\theta_1$ or $\theta_2$ very much.

The next assumption restricts the class of variance estimators we will
consider.  We use the same class of estimators studied by
\citet{JoD:00} (their class $\mathcal{K}$); see
their paper for further discussion.

\begin{asmp}
  \label{asmp-5} $W$ is a kernel from
$\Re$ to $[-1,1]$ such that $W(0) = 1$, $W(x) = W(-x)$ for all $x$,
\begin{equation*}
  \int_{-\infty}^{\infty} \lvert W(x) \rvert dx < \infty, \quad
  \int_{-\infty}^{\infty} \lvert \psi(x) \rvert dx < \infty
\end{equation*}
with
\begin{equation*}
  \psi(x) = \frac1{\sqrt{2\pi}} \int_{-\infty}^{\infty} W(z) e^{ixz}dz,
\end{equation*}
and $W(\cdot)$ is continuous at zero and all but a finite number of
points.
\end{asmp}

Assumptions~\ref{asmp-1}--\ref{asmp-5} are broadly similar to
\citepos{Wes:96} assumptions, except with regard to model complexity
and the size of the test sample.  West assumes that $K_1$ and $K_2$
are both finite, while we let them grow rapidly with $T$
(Assumption~\ref{asmp-1}), and West lets $P/R$ converge to any value
in $[0,\infty]$, while we require $P^2/R \to 0$
(Assumption~\ref{asmp-4}).\footnote{\citet{Wes:96} does not have a
  second \oos\ period, so there is no analogue in his paper to our
  restriction on $Q$.}  Obviously, these differences are
related---allowing for complex models requires tighter constraints on
the size of the test sample.  West allows more general estimation
methods, while we restrict attention to \ols; the advantage of using
\ols\ is that we can derive the inequality,
\begin{equation*}
  \| x_{it}'\theta_{i,T+1} - x_{it}'\theta_{iR} \|_2 \leq O(P/T)^{1/2}.
\end{equation*}
Our main results will hold for other models and estimation windows as
long as a similar inequality exists, but proving such inequalities is
left for future research.

Our other assumptions are similar.  The loss function must be smooth
almost everywhere, which is less restrictive than \citet{Wes:96}
requires but is not new; \citet{Mcc:00} extends West's results to
cover a similar case.\footnote{Our assumptions allow for discontinuous
  loss functions, such as those proposed by \citet{PeT:92}.
  \citet{Mcc:00} does not explicitly allow for such loss functions,
  but it appears that his conclusions hold for them as well under the
  restriction $P/T \to 0$.}  We assume full stationarity and absolute
regularity of the underlying random array, while West assumes
covariance stationarity and strong mixing of the transformed forecast
errors; and the details our moment conditions are different, but seem
that they would hold in similar environment to West's.  Finally, since
West's is a more standard environment, he assumes the existence of a
consistent estimator of the variance, while we construct it
explicitly.  Only West's assumptions governing model complexity and
the size of the test sample are substantially different than ours.

\section{Theoretical Results}\label{sec:theory}
The main result of the paper is Theorem
\ref{res-confidence-intervals}, which shows that the \oos\ average is
asymptotically normal and centered on $\E_T \oosB$.  This result holds
even for nested models and can be used to construct confidence
intervals or test hypotheses.  After presenting Theorem
\ref{res-confidence-intervals}, we explain and present two key Lemmas
used in its proof that add some insight into the nature of \oos\
comparisons, and then show that standard \hac\ \oos\ estimators can be
used to consistently estimate the asymptotic variance of the \oos\
average.  We next present an example that illustrates some of these
concepts, and then show that typical full-sample tests do not let
researchers test hypotheses about $\E_{T} \oosB$ when the models are
overfit.  In fact, in the limit they can reject a more accurate but
false benchmark model almost surely.

\subsection{Behavior of the DMW Test with Overfit}\label{sec:oostheory}
Our first theorem justifies using the \dmw\ $t$-test with a a short
test sample.

\begin{thm}\label{res-confidence-intervals}
  Suppose that Assumptions \ref{asmp-1}--\ref{asmp-4} hold, that
  $\hat\sigma_R^2$ is an estimator satisfying
  \[
    \hat\sigma_R^2 - \sigma_R^2 \to^p 0 \quad \text{with}\quad
    \sigma_R^2 \equiv \var_R(\sqrt{P} \oosA) \equiv P \E_R(\bar{D}_R -
    \E_R \bar{D}_R)^2,
  \]
  and that $\sigma_R^2$ is uniformly a.s. positive.  Then
  \[
  \frac{\oosA - \E_T\oosB}{\hat\sigma_R / \sqrt{P}}
  \to^d N(0,1).
  \]
  Consequently, each of the usual Gaussian confidence intervals,
  \begin{gather}
  [\oosA - z_{\alpha/2} \, \hat\sigma_R /
      \sqrt{P}, \oosA + z_{\alpha/2} \hat\sigma_R / \sqrt{P}],\label{interval-twosided} \\
  [\oosA - z_{\alpha} \, \hat\sigma_R / \sqrt{P}, +\infty), \label{interval-greater}
  \intertext{and}
(-\infty, \oosA + z_{\alpha}
      \, \hat\sigma_R / \sqrt{P}],
    \end{gather}
    contains $\E_T\oosB$ with probability $\alpha$ in the limit,
  with $z_{\alpha}$ the $1-\alpha$ quantile of the standard normal
  distribution.
\end{thm}
The requirement that $\sigma_R^2$ be uniformly a.s. positive is not
restrictive under our asymptotic theory.  Since the models'
coefficients are estimated with uncertainty in the limit, the two
models give different forecasts even if they both nest the \dgp.  This
is intuitively similar to \citepos{GiW:06} rolling-window result, but
comes from different asymptotic theory; Giacomini and White keep the
variance of the \oos\ average positive by letting $P \to \infty$ with
$R$ fixed; in this paper, $R \to \infty$ but the variance remains
positive since $K \to \infty$ (quickly) as well.

This result allows forecasters to use the \dmw\ test to conduct
inference about the models' accuracy.  Since the \oos\ average,
$\oosA$, is asymptotically normal and centered on the expected future
performance, $t$-tests constructed from the \oos\ errors necessarily
test hypotheses about $\E_T \oosB$ and not the performance of the
pseudo-true models or the unconditional performance of the models.
This application to testing can be expressed as its own theorem:
\begin{thm}\label{res:oostest}
Suppose that the conditions of Theorem \ref{res-confidence-intervals}
hold and that $\lim \Pr[\E_T \oosB \leq 0] > 0$.  Then
\begin{equation}
  \lim \Pr[P^{1/2}\oosA/\hat\sigma_R > z_{\alpha} \mid \E_T
  \oosB \leq 0] \leq \alpha.
\end{equation}
\end{thm}

Two intermediate Lemmas will help us understand Theorem
\ref{res-confidence-intervals} better.  Its proof consists of two
steps: we can write
\begin{equation}
  \sqrt{P} (\oosA - \E_T \oosB) = \sqrt{P} (\oosA - \E_R
  \oosA) + \sqrt{P} (\E_R \oosA - \E_T \oosB),
\end{equation}
so Theorem~\ref{res-confidence-intervals} holds if
\begin{equation}\label{coefpart}
  \E_R \oosA = \E_T \oosB + o_p(P^{-1/2}).
\end{equation}
and
\begin{equation}\label{normalpart}
  \sqrt{P}\frac{\oosA - \E_R \oosA}{\sigma_R} \to^d N(0,1)
\end{equation}
The first Lemma establishes~\eqref{coefpart} and the second shows that
the process $\{D_{Rt} - \E_R \oosA\}$ is an $L_2$-mixingale of size
$-1/2$ that satisfies a \clt, ensuring~\eqref{normalpart}.

\begin{lem} \label{res-convergence}
  Under Assumptions \ref{asmp-1}--\ref{asmp-4},
  \begin{equation}\label{eq:7}
    \E_T \oosB - \E_R \oosA = O_p(\sqrt{P/T}) + o_p(P^{-1/2} + Q^{-1/2}).
  \end{equation}
\end{lem}

Note that $\E_T \oosB - \E_R \oosA$ is noise introduced by
approximating the performance of the full-sample estimates with that
of the partial-sample estimates.  Moreover, the $O_p(\sqrt{P/T})$ term
is binding; it is straightforward to find examples for which
$\sqrt{T/P}(\E_T \oosB - \E_R \oosA)$ does not converge to
zero.\footnote{Notice that, if the observations are i.i.d. and
  $(y_{t+h}, x_{it}) \sim N(0,I)$, $(\hat{\theta}_{iR}^R -
  \hat{\theta}_{iT}^T) \sim N(0, (X_{iR}'X_{iR})^{-1} -
  (X_{iT}'X_{iT})^{-1})$ given $X_{iT}$, and so
  $\var(x_{i,T+1}'\hat{\theta}_{iR}^R - x_{i,T+1}'
  \hat{\theta}_{iT}^T) \gg P K_i / (R - K_2)(T - K_2)$.}
Consequently, there is little scope to relax our assumption that
$P^2/T \to 0$.  If $\lim P^2/T$ is positive and finite then there is
the possibility of adjusting the asymptotic distribution of the test
statistic to account for this noise term as in \citet{Wes:96}, but we
leave that to future research.  If $P^2/T \to \infty$, which is
closest to common practice in applied research, the noise term
dominates the asymptotic distribution of the \oos\ $t$-test and the
test has only trivial power in the limit if it has correct size.

\begin{lem}\label{res-mixingale}
  Part 1: If Assumptions~\ref{asmp-1}--\ref{asmp-3} hold then the
  inequality
  \begin{equation}\label{mixingaleR}
    \lVert \E_{R+j-l}(D_{R,R+j} - \E_R D_{R,R+j}) \rVert_2 \leq
    \couplingConstant \;
    \zeta_l
  \end{equation}
  with $\zeta_l = O(l^{-1/2 - \delta})$ holds for some positive
  $\delta$, any positive $j$, and any $l$ between 0 and $j$.  As a
  consequence, $\{D_{Rt} - \E_R D_{Rt}, \mathcal F_t\}$ is an
  $L_2$-mixingale of size $-1/2$.

  \noindent Part 2: Moreover,~\eqref{normalpart} holds under
  Assumptions~\ref{asmp-1}--\ref{asmp-4} if $\sigma_R^2$ is
  a.s. uniformly positive.
\end{lem}

Mixingales satisfy a weak-dependence condition similar to \mds es and
were introduced and studied extensively by
\citet{Mcl:74,Mcl:75,Mcl:75b,Mcl:77}.  This lemma shows that the
estimation uncertainty introduces dependence into the \oos\ forecast
errors, but the additional dependence is contained in the term $\E_R
\oosA$ and can be effectively removed by subtraction.  The second part
of the Lemma shows that the basic mixingale \clt\ established by
\citet{Jon:97} applies in this setting.\footnote{De Jong proves two
  \clt s: one for mixingales and one for \ned\ functions of mixing
  processes.  His mixingale result makes assumptions about the
  behavior of the sample second moment that arise as consequences of
  \ned.  The proof of Lemma~\ref{res-mixingale} shows that the
  arguments used for de Jong's \ned\ \clt\ continue to hold for our
  \oos\ process---in effect, that the \oos\ process has slightly more
  structure than required for it to be a mixingale.}  Note that
Lemma~\ref{res-mixingale} does not require $P^2/T \to 0$.

Finally, for Theorem \ref{res-confidence-intervals} to be useful, we
need a consistent estimator of $\sigma_R^2$. Lemma
\ref{res-variance-estimator} establishes that the usual \oos\ \hac\
variance estimators are consistent.\footnote{As with
      Lemma~\ref{res-mixingale}, the proof of this lemma takes
      \citepos{JoD:00} existing consistency results
      for \ned\ processes and shows that the same arguments can be
      applied to our \oos\ mixingale.}

\begin{lem}
  \label{res-variance-estimator} Suppose Assumptions
  \ref{asmp-1}--\ref{asmp-5} hold and that $\gamma \to \infty$ and
  $\gamma/P \to 0$ as $T \to \infty$, and define
  \begin{equation*}
    \hat{\sigma}_R^2 \equiv P^{-1} \oosSum{s,t}{1} (D_{Rt} - \oosA)(D_{Rs} - \oosA)
    \vWeight.
  \end{equation*}
  Then $\hat{\sigma}_R^2 - \sigma_R^2 \to^p 0$.
\end{lem}

The next subsection presents an example that clarifies the previous
results.

\subsection{An Example}\label{sec:example}

Let $L(e) = e^2$, let $h
= 1$, let the benchmark model have no regressors (i.e. $y_{t+1} \sim$
white noise), suppose that $(y_{t}, x_{2t}) \sim i.i.d.\ N(0,I)$, and
assume that $R > 2K_2 + 4$.\footnote{The restriction $R > 2 K_2 + 4$
  allows us to use a convenient formula for moments of the
  Inverse-Wishart distribution.}  Under these conditions,
\begin{equation*}
  D_t =
  \begin{cases}
    2 y_{t+1} x_{2t}'\hat{\theta}_{2R} -
    \hat{\theta}_{2R}' x_{2t} x_{2t}' \hat{\theta}_{2R} & t \leq T \\
    2 y_{t+1} x_{2t}'\hat{\theta}_{2T} -
    \hat{\theta}_{2T}' x_{2t} x_{2t}' \hat{\theta}_{2T} & t > T.
  \end{cases}
\end{equation*}

The first two conditional moments of $D_t$ for $t > R$ (given
$\mathcal{F}_R$) are straightforward to derive: $\E_R D_{Rt} = -
\hat{\theta}_{2R}' \hat{\theta}_{2R}$ and
\begin{equation*}
  \sigma_R^2 \equiv \E_R(D_{Rt} - \E_R D_{Rt})^2 = 4
  \hat{\theta}_{2R}^{R\prime} \hat{\theta}_{2R}^{R} +
  \E_R(\hat{\theta}_{2R}^{R\prime} (I - x_{2t} x_{2t}')
  \hat{\theta}_{2R}^{R})^2   \geq 4 \hat{\theta}_{2R}^{R\prime}
  \hat{\theta}_{2R}^R \quad a.s.
\end{equation*}
Now, $\hat{\theta}_{2R}' \hat{\theta}_{2R}$ is
a.s. positive and uniformly integrable with mean $4 K_2 / (R - K_2 -
1)$ (since $(X_{2R}'X_{2R})^{-1}$ is Inverse-Wishart$(I_{K_2}, R)$),
making $\sigma_R^2$ uniformly a.s. positive and ensuring that the
distribution of the \oos\ average is not $P^{1/2}$-degenerate.  The
natural \oos\ variance estimator is consistent for $\sigma_R^2$:
\begin{align*}
  \hat{\sigma}_R^2 &= P^{-1} \sum_{t=R+1}^{T-1} (D_{Rt} - \oosA)^2 \\
  &= 4 \hat{\theta}_{2R}^{R\prime} \Big[P^{-1} \sum_{t=R+1}^{T-1}
  \Big(y_{t+1} x_{2t} - P^{-1} \sum_{s=R+1}^{T-1} y_s x_{2s}\Big)
  \Big(y_{t+1} x_{2t} - P^{-1} \sum_{s=R+1}^{T-1} y_s
  x_{2s}\Big)'\Big] \hat{\theta}_{2R}' \\ & \quad - 4
  \Big[P^{-1} \sum_{t=R+1}^{T-1} \Big(y_{t+1} x_{2t} - P^{-1}
  \sum_{s=R+1}^{T-1} y_s x_{2s}\Big)' \hat{\theta}_{2R}
  \hat{\theta}_{2R}' \Big(x_{2t} x_{2t}' - P^{-1}
  \sum_{s=R+1}^{T-1} x_{2s} x_{2s}' \Big) \hat{\theta}_{2R} \Big]
  \\ &\quad + P^{-1} \sum_{t=R+1}^{T-1}
  \Big[\hat{\theta}_{2R}' \Big(x_{2t} x_{2t}' - P^{-1}
  \sum_{s=R+1}^{T-1} x_{2s} x_{2s}' \Big) \hat{\theta}_{2R}'  \Big]^{2}.
\end{align*}
As $P \to \infty$, the first term converges in probability to
$\hat{\theta}_{2R}^{R\prime} \hat{\theta}_{2R}^R$, the second to zero,
and the third to $\E_R(\hat{\theta}_{2R}^{R\prime}(I - x_{2t}x_{2t}')
\hat{\theta}_{2R}^R)^2$.  Since the number of regressors prevents
$\hat{\theta}_{2R}^R$ from converging i.p. to zero (its expected
value), $\hat{\sigma}_R^2$ does not converge to the unconditional
variance.

We can see that the \oos\ average obeys a \clt\ when centered at $\E_R
\bar{D}_R$.  The centered and scaled \oos\ differences satisfy
\begin{equation*}
  \frac{D_{Rt} - \E_R D_{Rt}}{\sigma_R} =
  \frac{2 y_{t} x_{2t}' \hat{\theta}_{2R} +
    \hat{\theta}_{2R}' (I - x_t x_t') \hat{\theta}_{2R}}{\sqrt{4
    \hat{\theta}_{2R}' \hat{\theta}_{2R} +
    \E_R(\hat{\theta}_{2R}'(I - x_{2t} x_{2t}') \hat{\theta}_{2R})^2}}.
\end{equation*}
This sequence is not i.i.d. and we can not replace $\hat{\theta}_{2R}$
with a sequence of constants $\theta_{2R}$ since $\hat{\theta}_{2R}$
does not converge to a deterministic limit.  But $\sigma_R^{-1}
\E_t(\oosA - \E_R\oosA) = 0$, making this sequence an
\mds,\footnote{The sequence is actually exchangeable, but \mds\ is
  enough to establish the \clt.} so it obeys a \clt\ as long as
$\sigma_R^2$ is uniformly a.s. positive, which we have already
established.  Lemma~\ref{res-mixingale} ensures that the \clt\ holds
more generally.

Finally, we can look at the difference $\E_R \bar{D}_R - \E_T
\bar{D}_T$.  We can write this as
\begin{align*}
  \E_R \oosA - \E_T \oosB &= \hat{\theta}_{2R}'
  \hat{\theta}_{2R} - \hat{\theta}_{2T}' \hat{\theta}_{2T} \\ &=
  \eb_T'[\tilde{X}_{2R}(X_{2R}'X_{2R})^{-2}
  \tilde{X}_{2R} - X_{2T}(X_{2T}'X_{2T})^{-2} X_{2T} ] \eb_T
\end{align*}
with $\tilde{X}_{2R}$ the $T \times K_2$ matrix $[X_{2R}'\ 0]'$.
Standard results for the moments of quadratic forms and of the
Inverse-Wishart distribution \citep{Haf:79} imply that
\begin{equation*}
  \E(\E_R \oosA - \E_T \oosB) = \frac{K_2 P}{(T - K_2 - 1)(R - K_2 - 1)}
\end{equation*}
and
\begin{align*}
  \var(\E_R & \oosA - \E_T \oosB) \\ &= \E
  \big[\tr((X_{2R}'X_{2R})^{-1} - (X_{2T}'X_{2T})^{-1})\big]^2 + 2 \tr
  \E\big((X_{2R}'X_{2R})^{-2} - (X_{2T}'X_{2T})^{-2}\big) \\ &\quad - \big[\tr
  \E((X_{2R}'X_{2R})^{-1} - (X_{2T}'X_{2T})^{-1}) \big]^2 \\ &=
  O(P/T)^2.
\end{align*}
So, in this example, $P^{1/2}(\E_R \oosA - \E_T \oosB) \to^p 0$ only
if $P^3/T^2 \to 0$, which is slightly weaker than the general
requirement that $P^2/T \to 0$.  If $\lim P^3/T^2 > 0$ the \oos\ average
still obeys the \mds\ \clt, but it is not centered correctly at $\E_T
\oosB$.

\subsection{Failure of In-Sample Tests}\label{sec:insample}
This subsection establishes that in-sample tests like the $F$-test can
overreject when the benchmark is more accurate than the alternative.
The issue explored here is different than that of \citet{Cal:11c} and
\citet{Ana:12}; in those papers, the authors demonstrate that full
sample Wald and $F$-tests can be invalid under increasing-$k$
asymptotics,\footnote{\citet{Ana:12} shows that the Wald test is
  invalid in general and gives an adjustment that corrects the
  critical values; he also shows that the $F$-test is asymptotically
  valid under certain conditions on the distribution of the
  regressors.  \citet{Cal:11c} shows that the $F$-test is
  asymptotically invalid without Anatolyev's constraint, and provides
  a correction that gives valid tests.  Both papers only consider
  independent observations.} e.g. they can overreject when testing the
null hypotheses that a set of the coefficients is zero.  This paper,
though, looks at the behavior when the coefficients are non-zero but
the estimated restricted model is more accurate.  In this setting,
these tests reject the benchmark too often---more than their nominal
size---and for most coefficient values will reject almost surely in
the limit.  We also look at some common model selection statistics and
find the same conclusion---they choose the larger model with high
probability regardless of which model will forecast better.  To
simplify the presentation, we'll only present results for nested
models.

Theorem~\ref{res:ftest} presents results for in-sample hypothesis
tests.
\begin{thm}\label{res:ftest}
  Suppose Assumptions 1--3 hold, that the models are nested, and that
  $L(e) = e^2$.  Also, let $\Lambda$ be a test statistic with nominal
  size $\alpha$ that has acceptance region contained in the set
  \[\{\hat{\theta}_T \mid (\hat{\theta}_T -
  \theta_0)'V_T(\hat{\theta}_T - \theta_0) < c \}\] for large enough
  $T$ and some $\theta_0$, $c$, and $V_T$, with $V_T$ a possibly
  random matrix with all eigenvalues uniformly bounded in probability
  and rank satisfying $\lim \rank(V_T)/T > 0$.  Then there exist
  distributions for $(x_t, \e_{t+h})$ such that
  \begin{equation}
    \label{eq:11}
    \Pr[\Lambda \text{ rejects} \mid \E_T \oosB \leq 0] \to^p 1
    \quad \text{as} \quad
    \theta'V_T\theta \to^p \infty \quad \text{($T$ is fixed)}
  \end{equation}
  for almost every sequence $\theta$ (a.e. with respect to Lebesgue
  measure) and
  \begin{equation}
    \label{eq:15}
    \plim_{T \to \infty} \Pr[\Lambda \text{ rejects} \mid
    \E_T \oosB \leq 0] \geq \plim_{T \to \infty} \Pr[c + \delta < (\theta - \theta_0)' V_{T}
    (\theta - \theta_0) \mid \E_T \bar{D}_T \leq 0]
  \end{equation}
  for any $\delta > 0$.
\end{thm}
Note that, if $V_T$ is nonstochastic or converges in probability,
there are distributions on $(x_t, \e_{t+h})$ that are
compatible with~\eqref{eq:15} such that
\[\Pr[c + \delta < (\theta - \theta_0)'V_T (\theta - \theta_0) \mid
E_T \bar{D}_T \leq 0] \to^p 1
\]
for any $\theta \neq \theta_0$.  Otherwise, the rejection probability
will still depend on $\theta$ and $V_T$ and will typically be higher
than the nominal size of the test.

Theorem~\ref{res:ftest} covers a broad class of test statistics and
allows the in-sample test to be valid or invalid in finite samples or
asymptotically.  It implies that a large class of in-sample tests will
reject if $\theta$ is nonzero, whether or not the alternative model is
more accurate.  As an example, consider the $F$-test of the null
hypothesis $A \theta =0$ with deterministic regressors and
i.i.d. Normal errors where $\rank A = J$ and $\lim J/T > 0$.  Simple
algebra reveals that the test rejects if $\hat{\theta}_T' V_T
\hat{\theta}_T \geq 1$ with
\begin{equation*}
  V_T = (T/ J \hat{\xi}^{2}) A' (A (T^{-1}X_T'X_T)^{-1} A')^{-1} A;
\end{equation*}
$\xi^2$ is the variance of the model's innovations and $\hat{\xi}^2$ is
the usual estimator based on the squared residuals.  Note that $\rank
V_T = \rank A$ as long as $X_T'X_T$ is full rank.  Under the null $A
\theta = 0$,
\begin{equation*}
  \hat{\theta}_T' V_T
  \hat{\theta}_T =^d   (\xi^2/ J \hat{\xi}^2) \chi_J^2 =^d 1 + J^{-1/2} Z
  + o_p(T^{-1/2}),
\end{equation*}
where $\chi_J^2$ is an independent $\chi^2$ random variable with $J$
degrees of freedom and $Z \sim N(0,1)$.  Consequently, the $F$-test
accepts whenever $\hat{\theta}_T$ is in a neighborhood of the cylinder
defined by $\hat{\theta}_T' V_T \hat{\theta}_T = 1$ and rejects for
$\hat{\theta}_T$ outside the neighborhood.\footnote{If $V_T$ is full
  rank, this equation defines an ellipsoid in $\mathbb{R}^K$;
  otherwise the equation defines an ellipsoid in a subspace of
  $\mathbb{R}^K$.}  The neighborhood shrinks as $T \to \infty$, so the
$F$-test satisfies the conditions of Theorem~\ref{res:ftest} with
$V_T$ as given above and $c = 1 + \delta$ for any $\delta > 0$.
Notice that $c$ does not depend on the nominal size of the test and
that the exact finite sample distribution of the $F$-test is known in
this example, so the test is valid by conventional measures but the
conclusions of Theorem~\ref{res:ftest} still hold.

The intuition behind Equation~\eqref{eq:11} can be understood by
simplifying further to the case $K_2 = 2$ and $K_1 = 0$, deterministic
orthonormal regressors, and $V_T = I$, so
\begin{equation*}
  E_T \oosB = \theta_{1}^2 + \theta_{2}^2 - (\hat{\theta}_{T,1} -
  \theta_{1})^2 - (\hat\theta_{T,2} - \theta_2)^2.
\end{equation*}
Under these conditions, the test rejects if $\hat{\theta}_{T}$ lies
outside the circle centered at the origin with radius $c^{1/2}$ and
$\E_T \oosB \leq 0$ holds for $\hat\theta_{T}$ on or outside a circle
centered at $\theta$ passing through the origin.
Figure~\ref{fig:rreject} depicts these regions.  Since the \ols\
estimator is normal with mean $\theta$, the probability that
$\hat{\theta}_{T}$ falls in the acceptance region given $\E_T \oosB
\leq 0$ vanishes as $\theta'\theta \to \infty$, even though the full
model will give worse forecasts than the restricted model.

\newcommand{\circlefigA}[4]{
  \begin{tikzpicture}
    \fill[lightgray] (-#3,-#3) rectangle (#4,#4);
    % rejection region for F-test
    \filldraw[fill=white,draw=black] (0,0) circle (#1);
    % circle of equal generalization error
    \filldraw[fill=white,draw=black] (1,1) let \p1=(1,1) in circle({veclen(\x1,\y1)});
    \draw (1,1) let \p1=(1,1) in circle({veclen(\x1,\y1)});
    \fill [black] (1,1) circle (2pt) node[right] {$(\theta_1,\theta_2)$};
    \draw (0,0) circle (#1);
    \draw (1,1)--(0,0);
    % axes
    \draw[->] (0,0)--(#2,0) node[right] {$\hat\theta_{T,1}$};
    \draw[->] (0,0)--(0,#2) node[above] {$\hat\theta_{T,2}$};
  \end{tikzpicture}
}
\newcommand{\circlefigB}[4]{
  \begin{tikzpicture}
    \fill[white] (-#3,-#3) rectangle (#4,#4);
    % rejection region for F-test
    \fill[lightgray] (0,0) circle (#1);
    % circle of equal generalization error
    \fill[white] (1,1) let \p1=(1,1) in circle({veclen(\x1,\y1)});
    \draw (0,0) circle (#1);
    \draw (1,1) let \p1=(1,1) in circle({veclen(\x1,\y1)});
    \fill [black] (1,1) circle (2pt) node[right] {$(\theta_1,\theta_2)$};
    \draw (1,1)--(0,0);
    % axes
    \draw[->] (0,0)--(#2,0) node[right] {$\hat\theta_{T,1}$};
    \draw[->] (0,0)--(0,#2) node[above] {$\hat\theta_{T,2}$};
  \end{tikzpicture}
}

\begin{figure}
  \centering
  \begin{tabular}{cc}
  \subfloat[]{\circlefigA{1}{2.5}{1.4}{3.2}\label{fig:circleA}} &
  \subfloat[]{\circlefigB{1}{2.5}{1.4}{3.2}\label{fig:circleB}}  \\
  \subfloat[]{\circlefigA{.3}{2.5}{1.4}{3.2}\label{fig:circleC}}  &
  \subfloat[]{\circlefigB{.3}{2.5}{1.4}{3.2}\label{fig:circleD}}
  \end{tabular}
  \caption{Graphs indicating the rejection region and region of equal
    generalization error for the models discussed in Section
    \ref{sec:insample}.  The smaller model has no estimated parameters
    and the larger has two coefficients.  The shaded regions in
    Figures (a) and (b) show the rejection region and the acceptance
    region respectively of the full-sample test given $\E_T \oosB \leq
    0$ when the true elements of $\theta$ are moderately small.  Figures
    (c) and (d) show the same regions when the true $\theta$ is large.}
\label{fig:rreject}
\end{figure}

Equation~\eqref{eq:15} is more interesting and implies that these
tests will almost surely reject the benchmark whenever we have a true
but overfit and less accurate alternative.  The intuition is similar
to the previous case.  As $K_2$ increases, the volume that
$\hat{\theta}_{T}$ can occupy increases as well.  If $\theta$ is
outside the acceptance region, the probability that $\hat{\theta}_{T}$
is close to the origin and therefore inside the acceptance region of
the test decreases.  In the limit, the probability that
$\hat{\theta}_{T}$ is outside the acceptance region is one.

A similar result for model selection follows as a Corollary to
Theorem~\ref{res:ftest}.  Consistent model selection in this context
is different than in the finite-$K$ case.  Here we want the
probability of selecting a model given that it is more accurate to
converge to one.  Formally, a model selection procedure is useful if
\begin{gather}\label{eq:13}
  \Pr[\text{model $1$ selected} \mid \E_T \oosB \leq 0] \to 1
  \intertext{and} \Pr[\text{model $2$ selected} \mid \E_T \oosB > 0]
  \to 1.\label{eq:14}
\end{gather}
Note that the conditioning event in~\eqref{eq:14} is different than in
the testing results presented earlier, but otherwise they are similar.
Theorem~\ref{res:ftest} implies that~\eqref{eq:13} will typically fail
for overfit models for many common model selection methods, including
adjusted $R^2$, the \aic, and the \bic.  Corollary~\ref{res:ic}
contains the details.

\begin{cor}\label{res:ic}
  Suppose the conditions of Theorem~\ref{res:ftest} hold but that the
  statistic $\Lambda'$ is used for model selection; $\Lambda'$ selects
  the benchmark model for all $\hat{\theta}_T$ contained in a subset
  of the region
  \begin{equation*}
    \{\hat{\theta}_T \mid \hat{\theta}_T'V_T \hat{\theta}_T < c\}
  \end{equation*}
  and selects the alternative for all $\hat{\theta}_T$ outside that
  region.  Then there exist distributions on $x_t,\e_t$ such
  that
  \begin{equation}
    \label{eq:17}
    \plim \Pr[\Lambda' \text{ selects model 1} \mid \E_T \bar{D}_T \leq 0]
    \leq \plim \Pr[\theta_0'V_T \theta_0 < c \mid \E_T \bar{D}_T].
  \end{equation}
\end{cor}
As before, we often have
\begin{equation*}
  \Pr[\theta_0'V_T \theta_0 < c \mid \E_T \bar{D}_T] \to^p 0.
\end{equation*}
Just as we saw with the $F$-test, adjusted $R$-squared, the \aic\ and
the \bic\ will only choose the benchmark model if $\hat{\theta}_T$
lies in the neighborhood of the cylinder defined by $\hat{\theta}_T
V_{T} \hat{\theta}_T = c$, regardless of which model is expected to
forecast better.  These statistics differ in how they determine the
size of the neighborhood, with the neighborhood shrinking slower for
the \bic\ than the \aic.

It is important to remember that the tests considered do not need to
have correct size for the null $\theta = \theta_0$, but are allowed to
in this analysis, so previous research, such as \citet{Cal:11c} and
\citet{Ana:12}, does not predict these results.  However, these tests
are poorly suited for choosing an accurate forecasting model and
perform worse when the larger model is overfit or explains more of the
variation of the dependent variable.  Essentially, for forecasting we
want to know if the coefficient estimates are close to their true
values, but in-sample statistics tell us only whether the estimates
are close to a particular hypothesized value (usually zero).  There
are lots of estimates that are both far from zero and far from their
true values which cause in-sample statistics to incorrectly choose the
alternative model.

\section{Monte Carlo}\label{sec:mc}
\newcommand{\thetanorm}{\ensuremath{\lvert \theta \rvert_2}} This
section presents two simulations that investigate the accuracy of our
theory in small samples.\footnote{Simulations were conducted in R
  \citep{Rde:10} using Sweave \citep{Lei:02} and the Lattice
  \citep{Sar:10}, RSQLite \citep{Jam:10}, tikzDevice
  \citep{ShB:11:0.6.1}, and pgfSweave \citep{BrS:11:1.2.1} packages.}
The first looks at the asymptotic normality of the \dmw\ statistic
centered on either $\E_R \oosA$ or $\E_T \oosB$.  The previous
section's results imply that centering on the first should give an
approximately normal statistic and centering on the second should give
a normal statistic when $P$ is small relative to $T$.  The second
simulation looks at the size and power of several statistics when
conducting inference about the difference in the models'
generalization error.  For all of the simulations, we look at
fixed-window \oos\ tests to speed up computation time.

\subsection{Setup}\label{sec:simulation-design}
The Monte Carlo experiment is intentionally very simple so that we can
isolate the influence of the models' complexity.  In particular, we do
not include some features that are common in forecasting
environments---serial dependence, heteroskedasticity, and complicated
\dgp s.  We simulate data for both studies from the equation
\begin{equation}\label{eq:6}
  y_t = x_t'\theta + \e_t,\quad \e_t \sim N(0,1),
  \quad t=1,\dots,T.
\end{equation}
The first element of $x_t$ is 1 and the remaining $K_2-1$ elements are
independent Standard Normal.  The benchmark model is
\begin{equation}
  \label{eq:1}
  y_{1t} = \sum_{j=1}^{K_1} x_{jt}\theta_j + \e_t
\end{equation}
and the alternative model is the \dgp\ \eqref{eq:6}.  We let
$(K_1,K_2)$ equal either $(2,3)$ or $(T/20,T/10)$ to study our theory
in its intended application as well as for more parsimonious models.
We let $T$ equal 100, 250, 500, or 1000.  We also vary $\theta$, and do
so giving the benchmark and the alternative model comparable weight in
predicting $y_t$.  Specifically, we set
\begin{equation*}
  \theta_j =
\begin{cases} \frac{c}{\sqrt{K_1}} & j = 1,\dots,K_1 \\
\frac{c}{\sqrt{K_2 - K_1}} & j = K_1 + 1,\dots,K_2 \end{cases}
\end{equation*}
with $c$ equal to zero or one.  When $c$ is one, we're more likely to
draw values of $X$ and $Y$ that make the estimated larger model more
accurate than the benchmark, and when $c$ is zero we're unlikely to
draw such values of $X$ and $Y$.  For all of the studies, $L(x) =
x^2$.

The first set of simulations study how well the theory in
Section~\ref{sec:oostheory} works in practice.  For each draw of $X$
and $Y$, we construct a one-sided \oos\ interval of the
form
\begin{equation*}
  [ \oosA - 1.28 \hat{\sigma_R}, \infty) \quad\text{with}\quad
  \hat\sigma_R^2 = \frac1P \sum_{t=R+1}^T (D_t - \oosA)^{2}
\end{equation*}
for $P$ set to every 10th value between 1 and $2T/3$.  We calculate
the percentage of these intervals that contain $\E_R \oosA$ and that
contain $\E_T \oosB$.  Since the data are i.i.d., both of these
quantities are easy to calculate:
\begin{equation*}
  E_R \oosA = \Bigg[\sum_{i=1}^{K_1} (\tilde{\theta}_{iR} - \theta_i)^2 +
  \sum_{i=K_1+1}^{K_2}\theta_i^2\Bigg] - \sum_{i=1}^{K_2} (\hat{\theta}_{iR} -
  \theta_i)^2
\end{equation*}
and
\begin{equation*}
  E_T \oosB = \Bigg[\sum_{i=1}^{K_1} (\tilde{\theta}_{iT} - \theta_i)^2 +
  \sum_{i=K_1+1}^{K_2}\theta_i^2\Bigg] - \sum_{i=1}^{K_2} (\hat{\theta}_{iT} -
  \theta_i)^2
\end{equation*}
where $\tilde{\theta}$ indicates that the coefficient is estimated
using only the regressors in the benchmark model and $\hat{\theta}$
the full model.  We draw 2000 samples for each combination of the
design parameters.

The second simulation looks at the size and power of the test
statistics when conducting inference about the models' generalization
error.  The null hypothesis we test is
\begin{equation}\label{eq:9}
  H_0:\quad E_T \oosB \leq 0,
\end{equation}
We look at four different statistics---the full-sample $F$-test, the
\dmw\ $t$-test, the
\oos\ $t$-test using McCracken's (2007)
critical values,\footnote{These critical values are not published for
  $K_2-K_1>10$, so we do not report them for $K_2 = T/10$.}
and Clark and West's (2006, 2007) Gaussian out-of-sample
statistic.\footnote{Clark and West (2006, 2007) derive their statistic
using the rolling window estimation scheme.  Here we use the same
statistic, but with a fixed window scheme.}
For the $F$-test, we simply test whether the coefficients on the larger
model are nonzero.  For the out-of-sample tests, we conduct a
one-sided test of out-of-sample performance for every 10th value of
$P$ as before.

To estimate each test's size for the hypothesis \eqref{eq:9}, we draw
samples from the \dgp\ and discard those for which \eqref{eq:9} does
not hold, until we have 2000 for each choice of the design parameters.
The estimated size is the fraction of those samples in which the test
statistic rejects.
\subsection{Results}
We discuss results for the confidence intervals first.
Figures~\ref{fig:interval-R} and~\ref{fig:interval-T} show the
coverage probability of these intervals as a function of $P$ for each
combination of $T$, $K_1$ and $K_2$, and $c$.  The gray horizontal
line shows the intervals' nominal coverage probability.  Each panel
displays the coverage for a different choice of interval, centering
term ($\E_R \oosA$ or $\E_T \oosB$) and combination of design
parameters.

Figure~\ref{fig:interval-R} gives the results for $\E_R \oosA$.  The
actual coverage is very close to the nominal coverage except when $P$
is very small.  The poor behavior for small $P$ is unsurprising, as it
simply means that the \clt\ is a poor approximation when the test
sample is small.  The intervals' good coverage holds even for the
parsimonious models ($K_1=1$ and $K_2= 3$) which our theory does not
necessarily apply to.

Figure~\ref{fig:interval-T} gives the results for $\E_T \oosB$.  In
rows 2, 4, 6, and 8---the overfit models---the coverage is near
nominal coverage for moderately small values of $P$.  As $P$ increases
to $2T/3$, the coverage increases to 1.  With the parsimonious model,
the coverage is near nominal coverage for all $P$ for the one-sided
interval with $c=1$, but only for moderately small $P$ when $c=0$.

The behavior for $K_2 = T/10$ is exactly what our theory predicts.
When $P^2/T$ is small, the coverage is near nominal levels.  The
behavior as $P$ increases, combined with the results for $\E_R
\oosA$, indicate that $\E_T \oosB \geq E_R \oosA$ in general.
Since
\[E_{T} \oosB = E_{R} \oosA + (E_{T} \oosB - E_{R} \oosA), \] and the
interval is approximately centered at $\E_R \oosA$, the difference
$\E_{T} \oosB - E_{R} \oosA$ adds a substantial positive quantity when
$P^2/T$ is not near zero, increasing the coverage of the one-sided
interval.

We next present the size simulations,
Figures~\ref{fig:ftest}--\ref{fig:ttest-power}.  For the \oos\ tests,
the conditional rejection probability is plotted for each combination
of $T$, $K_1$, $K_2$, and $c$ as a function of $P$.  The $F$-test does
not depend on $P$, so a single value is presented for each
combination.

We'll look at the $F$-test first.  To make comparisons easier, the size
is displayed as a dot plot in Figure~\ref{fig:ftest}.  Different
panels display a different combination of $K_1$, $K_2$, and $T$, and
each individual plot shows the empirical size for each choice of $c$.
We see immediately that the actual and nominal size are essentially
equal for $c = 0$, which is unsurprising.  For $c = 0$, the $F$-test is
exact; moreover, the larger model will almost always be less accurate
than the smaller one, so conditioning on $\E_T \oosB \leq 0$ is
almost unrestrictive.  When $c$ increases, though, the $F$-test
overrejects badly---rejecting at roughly 50\% when $c = 1$ for the
parsimonious model and from 70\% to 100\% for the overfit model.  As
our discussion in Section~\ref{sec:insample} predicts, the rejection
probability increases with $T$ for the overfit model but does not
depend on $T$ for the small model.

Figure~\ref{fig:ttest-size} presents the size estimates for the \dmw\
$t$-test.  Again, different panels display results for different
combinations of the design parameters.  Each graph plots the rejection
probability against $P/T$.  For $K/T=10$, the rejection probability
falls as $P/T$ increases, from near nominal size when $P/T$ is small
to zero when $P/T$ is near $2/3$.  Moreover, the rejection probability
falls faster when $T$ is large, as our theory predicts.  When $K=3$,
the rejection probability stays closer to nominal size, but falls with
$P/T$ for $c=0$, under-rejecting by about 5pp when $P/T = 2/3$, and
rises with $P/T$ for $c=1$, overrejecting by about 10pp when
$P/T=2/3$.  For small $P$, the rejection probability is near 10\% for
all simulations (the farthest is $K=T/10$, $c=0$, where the rejection
probability is about 5\%; the other simulations are much closer).

We observe the following patterns.  The
\dmw\ test has close to nominal size when $P$ is small for every
combination of design parameters.  In most cases, the rejection
probability decreases as $P/T$ increases---the exception is for $K_2 =
3$ and $c=1$.  For the large-$K$ simulations, the rejection
probability drops to zero for most of the simulations as $P/T$
increases.  The rejection probability increases with $c$, but the
rejection probability still is near nominal probability for small $P$
with $c=1$.

Clark and West's (2006, 2007) statistic, presented in
Figure~\ref{fig:clarkwest}, behaves quite differently.  For $c=0$ the
test is correctly sized for both the overfit and parsimonious studies,
as we saw for the $F$-test.  When $c=1$, the rejection probability
increases rapidly with $P/T$.  For $K_2=3$, the rejection probability
is near 10\% when $P$ is small but about 40\% when $P/T = 2/3$.  For
$K=T/10$, the rejection probability is even higher and increases with
$T$ as well, from a maximum over 50\% when $T=100$ to a maximum of
nearly 100\% when $T=1000$.

Results using \citepos{Mcc:07} critical values are
presented in Figure~\ref{fig:mccracken} and are similar to those using
Clark and West's test.  For $c=0$ the rejection probability is nearly
the test's nominal size.  For $c=1$, the rejection probability
increases with $P/T$, from close to the nominal size when $P/T$ is
small to over 25\% when $P/T = 2/3$.  Note that all of the simulations
use the parsimonious model.  McCracken's statistic overrejects here
by slightly less than Clark and West's, but still by a substantial
amount.

Since the \dmw\ test tends to have low rejection probability, the
test's power is a concern.  We'll present some power results,
simulating from \eqref{eq:1} with $c = 1$ or 2 subject to the
constraint that $\E_T \oosB > 0$.\footnote{Draws of $X$ and $Y$ with
  $\E_T \oosB > 0$ are very rare when $c=0$, so we do not present
  results for that value of $c$.}  Figure~\ref{fig:ttest-power} plots
the power for the \dmw\ test; since the other test statistics greatly
overreject, we do not present their power.  For $c=1$, the power is
never greater than nominal size and decreases to zero as $P/T$
increases for the overfit model.  For $c=2$ the power is better,
increasing with $P/T$ for a stretch and then decreasing as $P/T$ grows
beyond approximately 1/4 for the overfit model.  Larger values of $T$
give a higher peak and greater power overall, but the power still
falls to nearly zero if $P/T$ is too large (approximately 2/3 in our
simulations).  The power with the parsimonious model is typically
quite low but greater than nominal size for $c = 2$.

Both sets of simulations support our theoretical results.  The first
simulation confirms that the \dmw\ \oos\ $t$-test is centered at $\E_R
\oosA$ for all choices of $P$ and $R$ and is centered on $\E_T \oosB$
only when $P$ is small.  The second simulation confirms that the \dmw\
test has correct size for the null hypothesis that $\E_T \oosB \leq 0$
when $P$ is small and that tests designed to test whether the
benchmark is true, like the $F$-test and Clark and West's (2006, 2007)
and McCracken's (2007) \oos\ tests can reject by much more than their
nominal size when testing the null $\E_T \oosB \leq 0$.  Moreover,
these simulations demonstrate that the restriction $P^2/T \to 0$ is
binding in practice, as the \dmw\ test under-rejects and has very low
power when it is not satisfied.
\section{Empirical Exercise}\label{sec:empirics}
This section presents an analysis of equity premium predictability
similar to \citet{GoW:08}.\footnote{All of the calculations in this
  section are done in R \citep{Rde:10}.  We also use code provided by
  \citet{Zeh:02}, \citet{Zei:04}, \citet{Har:10}, and
  \citet{ShB:11:0.6.1}.}  We estimate the expected forecasting
performance of the largest model they consider, a model with 13
regressors, using 81 observations (annual data from 1928 to 2009).  We
construct confidence intervals for the difference in performance
between this model and a prevailing mean benchmark, and do so for
different values of $R$ and $P$ to examine the effect of the training
and test sample choice on this \oos\ statistic.  We find that the
estimate of the larger model's \mse\ decreases relative to the
benchmark as $R$ increases, but it is never smaller than the
benchmark's.  This result, combined with the larger model's in-sample
significance, suggests that there may be a real relationship between
the equity premium and some of these predictors, but that the
relationship can not be estimated accurately enough to be useful for
forecasting and supports \citepos{GoW:08} conclusions.  These results
hold even when imposing \citepos{CaT:08} restriction that the equity
premium forecast be non-negative.

We'll start with a very brief review.  \citet{GoW:08}
study the \oos\ forecasting performance of different variables thought
to predict the equity premium (calculated as the difference between
the return on the S\&P 500 index and the T-bill rate).  Some of these
variables are listed in
Table~\ref{tab:equity}.\footnote{Table~\ref{tab:equity} only lists the
  variables used in \citepos{GoW:08} ``kitchen sink''
  model.  Some of the variables that they use in bivariate models are
  excluded from this model either because the series are too short or
  because the variables are linear combinations of other variables.}
\citet{GoW:08} estimate the \oos\ forecasting
performance of bivariate models of the form
\[
r_{t+1} = \beta_0 + \beta_1 x_t + \e_{t+1}
\]
using \ols\ with a recursive window, where $r_{t+1}$ is the equity
premium and $x_t$ is a generic predictor, as well as some larger
models.  They find that these models forecast no better than the
prevailing mean of the equity premium.
\citet{CaT:08} find that nonlinear extensions of
Goyal and Welch's models are more accurate, either imposing sign
restrictions on the estimated coefficients or imposing that the
forecast be non-negative, but do not test for significance.

Obviously, \citepos{GoW:08} bivariate models do not
match the asymptotic theory of this paper, so we restrict our
attention here to their ``kitchen sink'' model, which includes all of
the variables, except for a few dropped because of data availability or
(perfect) multicollinearity.  Formally, the benchmark and alternative models are
\begin{eqnarray}
  \label{eq:3}
  r_{t+1} &=& \mu + \e_{1,t+1} \\
  \label{eq:2}
r_{t+1} &=& \beta_0 + \sum_{i=1}^K \beta_{i} x_{it} + \e_{2,t+1}
\end{eqnarray}
respectively and are estimated by \ols\ using the fixed-window scheme.
The predictors are listed in Table~\ref{tab:equity}; for a detailed
description of each predictor, please see Goyal and Welch's original
paper.  In this model, there are 13 regressors (including the constant
term) and we estimate the coefficients using annual data from 1928 to
2009.  This makes the ratio $K/T$ equal to about 0.16.  We also
present results for one of \citepos{CaT:08}
restricted forecasting models.  For this model, we estimate
\eqref{eq:3} and \eqref{eq:2}, but impose that $\hat r_{t+1}$ be
non-negative for each forecast.  We then calculate the \oos\ test just
as for the unconstrained model.

Table~\ref{tab:gwinsample} gives the results of the full sample
regression for Equation~\eqref{eq:2}, using Newey-West standard errors
with two lags \citep{NeW:87}.\footnote{We compare the test statistic
  to critical values from the $F$-distribution.}  The $p$-value is
very small (less than 0.01), indicating that the coefficients are
nonzero in population and at least one of these predictors is
correlated with the equity premium.  As we argue earlier, this result
does not imply that the model will forecast well, which we look at
next.

To study the effect of the training sample size on the \dmw\ statistic, we
calculate the one-sided confidence interval for $\E_T \oosB$ given
by \eqref{interval-greater} corresponding to the null and alternative
hypotheses
\[ H_0: \quad E_T \oosB \leq 0 \qquad
H_A: \quad E_T \oosB > 0
\]
using the fixed-window scheme for each value of $R$ between 20 and
$T-10$.  The standard deviation is estimated using a Newey-West
estimator with $\lfloor P^{1/4}\rfloor$ lags.  For small values of
$R$, the \oos\ average is expected to underestimate the performance of
the larger model relative to the smaller, but this may not hold in
this particular dataset.

Figures~\ref{fig:empirics1} plots the \ols\ results and
Figure~\ref{fig:empirics2} imposes \citepos{CaT:08} restriction.  The
solid line in each figure shows the \oos\ average, $\oosA$, and the
shaded region indicates the 95\% one-sided confidence interval implied
by the \dmw\ test.  Negative numbers indicate that the kitchen sink
model has higher out-of-sample loss.  We can see that the same
patterns hold for both models: the performance difference decreases as
$R$ grows, but the kitchen sink model is never more accurate.  We also
see that the performance difference decreases suddenly over the period
$R=29$ to $R=34$ (corresponding to the years 1956--1961).
Figure~\ref{fig:empirics3} plots the accuracy of the individual
forecasts (only for the linear models) and shows that this change is
the result of a sudden improvement in the kitchen sink model.  This
change may indicate instability in the underlying relationship, as
\citet{GoW:08} propose.

In summary, we fail to reject the null that the benchmark prevailing
mean model is more accurate than \citepos{GoW:08} kitchen sink.  This
result is consistent with Goyal and Welch's original analysis.  Unlike
Goyal and Welch, we attribute this result, at least in part, to
parameter uncertainty---the full sample results indicate that the
larger model could predict better than the benchmark with enough
data.\footnote{\citet{BWB:10} make a similar point about exchange rate
models, but see also \citet{Chi:10} and \citet{Gia:10}.}
Obviously, this is an illustrative exercise only and is not meant to
be comprehensive.  In this dataset, there is likely parameter
instability that we have not addressed, and we have not dealt with
variable persistence at all.  However, as we mention earlier, our
requirement that $P$ be small should let our theoretical results
extend to moderate instability, and the fact that we do not rely on
the asymptotic distribution of the coefficient estimates should
mitigate the effect of persistence as well.
\section{Conclusion}\label{sec:conclusion}
This paper gives a theoretical motivation for using \oos\ comparisons:
the \dmw\ \oos\ test allows a forecaster to conduct inference about
the expected future accuracy of his or her models when one or both is
overfit.  We show analytically and through Monte Carlo that standard
full-sample test statistics can not test hypotheses about this
performance.

Our paper also shows that popular test and training sample sizes may
give misleading results if researchers are concerned about overfit.
We show that $P^2/T$ must converge to zero for the \dmw\ test to give
valid inference about the expected forecast accuracy, otherwise the
test measures the accuracy of the estimates constructed using only the
training sample.  In empirical research, $P$ is typically much larger
than this.  Our simulations indicate that using large values of $P$
with the \dmw\ test gives undersized tests with low power, so this
practice may favor simple benchmark models too much.  Existing
corrections, proposed by \citet{ClM:01,ClM:05}, \citet{Mcc:07} and
\citet{ClW:06,ClW:07}, seem to correct too much, though, and reject
too often when the benchmark model is more accurate.

More work remains.  The requirement that $P^2/T$ converge to zero is
limiting, as it implies that in typical macroeconomic datasets, only a
handful of observations should be used for testing.  This requirement
can be relaxed only slightly; $P = O(T^{1/2})$ is required for the
\oos\ test to have nontrivial power in general, but there are loss
functions and \dgp s for which some relaxation is possible.  This
constraint could be mitigated by extending our results to
cross-validation or other resampling strategies, or by constructing
full-sample statistics that allow inference about $\E_T \bar{D}_T$.
It would also be useful to extend our results to other forecasting
models and to explore how stationarity could be relaxed, but such
extensions are less important than improving the available statistics.

\appendix
\section*{Appendix: Mathematical Details}
\setcounter{section}{1}
% Change lemma style to prepend 'A'
\setcounter{lem}{0}
\renewcommand{\thelem}{A\arabic{lem}}

\subsection*{Supporting Results}
The results in this paper rely heavily on a coupling argument for
absolutely regular sequences, Berbee's Lemma \citep{Ber:79}.  Many of
the results of this paper (Lemmas \ref{res-mixingale} and
\ref{res-variance-estimator}) are modifications of existing results
for \ned\ functions of mixing processes by \citet{Jon:97} and
\citet{JoD:00}; this coupling argument is used to explicitly derive
inequalities that arise naturally for \ned\ processes.  Lemma
\ref{lem-basic-coupling} establishes these inequalities, which are
based on a proposition of \citet{MeP:02}.

We present \citeauthor{MeP:02}'s
(\citeyear{MeP:02}) statement of Berbee's Lemma for
the reader's reference.  In the following Lemma, $\beta(X,Y)$ is the
coefficient of absolute regularity:
\[
\beta(X,Y) = \sup_{A \in \sigma(Y)} \E \lvert \Pr(A \mid \sigma(X))
  - \Pr(A) \rvert.
\]
\begin{lem}\label{lem-berbee}\quad

\begin{quotation}\noindent
  Let $X$ and $Y$ be random variables defined on a probability space
  $(\Omega, \mathcal{T}, \Pr)$ with values in a Polish space
  $S$.  Let $\sigma(X)$ be a $\sigma$-algebra generated by $X$ and let
  $U$ be a random variable uniformly distributed on $[0,1]$
  independent of $(X,Y)$.  Then there exists a random variable $Y^{*}$
  measurable with respect to $\sigma(X) \vee \sigma(Y) \vee
  \sigma(U)$, independent of $X$ and distributed as $Y$, and such that
  $\Pr(Y \neq Y^{*}) = \beta(X,Y)$.

  \noindent\citep{MeP:02}
\end{quotation}
\end{lem}

The advantage of this result over coupling arguments that use other
forms of weak dependence is that the difference between the original
variable, $Y$, and the new variable, $Y^{*}$, does not depend on their
dimension.  Similar results for strong mixing sequences depend on the
dimension of $Y$, which makes them unsuitable for our applications.

\begin{lem}\label{lem-extend-mp}
  Suppose that $X$ and $X^*$ are $L_p$-bounded random variables, with
  $p > 2$, that satisfy ${\Pr[X \neq X^*] = c}$.  Then
  \[
    \lVert X - X^* \rVert_2 \leq 2^{1/p} (\lVert X \rVert_p + \lVert
    X^* \rVert_p) c^{(p-2)/2p}
  \]
\end{lem}

The proof is virtually identical to the proof of Proposition 2.3 in
\citet{MeP:02} and is omitted.

\begin{lem}\label{lem-basic-coupling}
  Suppose Assumptions \ref{asmp-1}--\ref{asmp-3} hold.  Then, for any
  T, $s$, and $t$ with $t \geq s$, there exist random variables
  $\tilde D_s,\dots,\tilde D_t$ such that
  \begin{equation}\label{eq:coupling1}
    P[(\tilde D_s,\dots,\tilde D_t) \neq (D_s,\dots,D_t)] \leq \beta_{s-u}
  \end{equation}
  and
  \begin{equation}\label{eq:coupling2}
    \E(\phi(\tilde D_s,\dots, \tilde D_t) \mid \mathcal{F}_u ) =
    \int
    \phi(D_s,\dots,D_t) f(\mathbf{x}, \mathbf{y})\ d\mathbf{x}\ d\mathbf{y}
  \end{equation}
  almost surely for all measurable functions $\phi$ such that the
  expectations are finite, where
  \[ \mathbf{x} = (x_{s}', \dots, x_{t}')', \qquad
  \mathbf{y} = (y_{s+\h},\dots,y_{t+\h})',\] and $f$ is the
  joint density of $(\mathbf{x}, \mathbf{y})$.  Moreover,
 \begin{equation}\label{eq:coupling3}
   \| \tilde D_v - D_v \|_2 \leq 2^{1+1/\rho} B_L
   \beta_{s-u}^{\rhoExp}, \qquad v = s,\dots,t.
 \end{equation}
\end{lem}

\begin{proof}
  The proof follows as a consequence of Lemmas \ref{lem-berbee} and
  \ref{lem-extend-mp}.  Let $l = t-s$.  For any fixed values of $l$
  and $T$, the sequence of vectors
  \[ V_s = (y_{s+\h}, x_{s}', \dots, y_{s+l+\h}, x_{s+l}') \] is
  absolutely regular of size $\rho/(\rho-2)$.  Berbee's Lemma implies
  that there is a random vector $V^*$ that is independent of
  $\mathcal{F}_u$, equal to $V_s$ in distribution, and satisfies
  \[\Pr[V^* \neq V_s] = \beta_{s-u}.\]

  Now define
  \begin{equation*}
    \tilde D_v =
    \begin{cases}
      L(y_{v+\h}^* - x_{1v}^{*\prime} \hat{\theta}_{1R}) - L(y_{v+\h}^* -
      x_{2v}^{*\prime} \hat{\theta}_{2R}) & v \leq T \\
      L(y_{v+\h}^* - x_{1v}^{*\prime} \hat{\theta}_{1T}) - L(y_{v+\h}^* -
      x_{2v}^{*\prime} \hat{\theta}_{2T}) & v > T
    \end{cases}
  \end{equation*}
  with $y_{v+\h}^*$ and $x_{iv}^*$ denoting the elements of $V^*$
  corresponding to $y_{v+\h}$ and $x_{iv}$ in $V_s$.  Equations
  (\ref{eq:coupling1}) and (\ref{eq:coupling2}) are satisfied by
  construction, and (\ref{eq:coupling3}) follows from Lemma
  \ref{lem-extend-mp}.
\end{proof}

\begin{lem}\label{lem:a2}
  Suppose Assumptions~\ref{asmp-1}--\ref{asmp-3} hold.  Let $b_T$ be a
  sequence such that $b_T \to \infty$ and $b_T = o(P)$ and define
  \begin{equation*}
    Z_i = \ZDef.
  \end{equation*}
  Then
  \begin{equation}
    \label{eq:10}
    \sum_{i=1}^{\lfloor P/b_T \rfloor} (\E_R Z_i^2 - \E_{R+(i-1) b_T}
    Z_i^2) \to^p 0.
  \end{equation}
  If Assumptions~\ref{asmp-4} and \ref{asmp-5} also hold and we define
  \begin{gather*}
    Z_{1t} = \varianceTermIIIa, \intertext{and} Z_{2t} =
    \varianceTermIVb,
  \end{gather*}
  then
  \begin{equation}
    \label{eq:12}
    \sum_{t=-P+1}^{2P} (Z_{1t} Z_{2t} - \E_R Z_{1t} Z_{2t}) \to^p 0.
  \end{equation}
\end{lem}
\begin{proof}
\newcommand{\UFiltration}[1]{\ensuremath{\mathcal{F}_{(#1)b_{T}+R-P}}}%
The first result,~\eqref{eq:10}, follows a similar argument to Lemma~5
of \citet{Jon:97} and~\eqref{eq:12} to Lemma~A.4 of \citet{JoD:00}.
Since these arguments are similar and our modification is the same for
both, we'll just present the more complicated version,~\eqref{eq:12}.

Note that $\{Z_{1t}^2 P\gamma/b_T\}$ and $\{Z_{2t}^2 P\gamma/b_T\}$
are uniformly integrable.  As in~\citet[Lemma A.4]{JoD:00}, we can
assume that there is a constant $C$ such that $Z_{1t}$ and $Z_{2t}$
are bounded in absolute value by $C\sqrt{b_T/P\gamma}$; uniform
integrability ensures that the difference between the unbounded random
variables and these truncated versions is negligible for large enough
values of $C$.

Let $r = \lfloor 3P/2b_T \rfloor$ and rewrite the summation as
\begin{align*}
  \vtSum \vtIIIsummand &= \vtSumr \vtSuma \vtIIIsummand \\
  &\quad+ \vtSumr \vtSumb \vtIIIsummand \\
  &\quad+ \sum_{t=r b - P + 1}^{2P} \vtIIIsummand \\
  &\equiv \vtSumr (U_i - \E_R U_i) + \vtSumr (U_i' - \E_R U_i') + o_{L_1}(1).
\end{align*}
The proof then holds if we can show that both $U_i$ and $U_i'$ obey
\lln s.  We'll do so by proving that $\{U_i -\E_R U_i,
\UFiltration{2i-1}\}$ and $\{U_i' - \E_R U_i', \UFiltration{2i}\}$ are
$L_2$-mixingales of size $-1/2$ and using the bound $\E(\vtSumr (U_i -
\E_R U_i))^2 = O(\vtSumr c_i^2)$ where the $c_i$ are the mixingale
magnitude indices \citep{Mcl:75}.

For non-negative $m$, we have
\begin{equation*}
U_i - \E_R U_i \in \UFiltration{2i+2m-1},
\end{equation*}
establishing half of the mixingale result trivially.  Now fix $i$ and $m >
0$ and use Lemma \ref{lem-basic-coupling} to define $\tilde{D}_{ts}$
for each $t =(2i-2)b_T-P+1,\dots,(2i-1)b_T-P$ and $s =
\max(t+R-b_T,R-\h+1),\dots,\min(t+R+b_T,T)$ such that
\begin{equation*}
 \E_R \tilde D_{ts} = \E_{(2i-2m-1)b_T+R-P} \tilde D_{ts} \quad a.s.
\end{equation*}
and
\begin{equation*}
  \lVert \tilde D_{ts} - D_{Rs} \rVert_2 \leq \couplingBound{s - (2i-2m-1)b_T-R+P}.
\end{equation*}
Also define
\begin{equation*}
  \tilde Z_{1t} = 1/\sqrt{P\gamma} \sum_{l=\vttLower}^{\vttUpper}
  (\tilde D_{t,t+l+R} - \E_R\tilde D_{t,t+l+R})\ W(l/\gamma), \\
\end{equation*}
and
\begin{equation*}
\tilde Z_{2t} = 1/\sqrt{P\gamma} \sum_{j=\vttLower}^{\vttUpper}
    (\tilde D_{t,t+l+R} - \E_R\tilde D_{t,t+l+R})\ \kernelB{j/\gamma}.
\end{equation*}

Now, we have the inequalities
\begin{align*}
  \lVert \E( U_i - \E_R U_i & \mid \UFiltration{2i-2m-1}) \rVert_2
  \\ &\leq
  \vtSuma \lVert \E(Z_{1t}Z_{2t} \mid \UFiltration{2i-2m-1}) - \E_R
  Z_{1t}Z_{2t} \rVert_2 \\
  &= \vtSuma \lVert \E(Z_{1t}Z_{2t} \mid \UFiltration{2i-2m-1}) \\
  &\quad - \E(\tilde{Z}_{1t}\tilde{Z}_{2t} \mid \UFiltration{2i-2m-1}) \\
  &\quad + \E(\tilde{Z}_{1t}\tilde{Z}_{2t} \mid \UFiltration{2i-2m-1})
  - \E_R Z_{1t}Z_{2t} \rVert_2 \\
%  \leq \vtSuma (\wall \lVert \wall \E(Z_{1t}Z_{2t} \mid \UFiltration{2i-2m-1}) \\
%  - \E(\tilde{Z}_{1t}\tilde{Z}_{2t} \mid \UFiltration{2i-2m-1}) \rVert_2\return
%  \\ + \lVert \E_R Z_{1t}Z_{2t} - \E_R \tilde{Z}_{1t}\tilde{Z}_{2t}
%  \rVert_2)\return \\
  &\leq 2 \vtSuma \lVert Z_{1t} Z_{2t} - \tilde Z_{1t} \tilde{Z}_{2t}
  \rVert_2 \\
  &\leq 2 \vtSuma (\lVert Z_{1t} - \tilde{Z}_{1t} \rVert_2 \lVert
  Z_{2t} \rVert_{\infty}
  + \lVert Z_{2t} - \tilde{Z}_{2t} \rVert_2 \lVert \tilde{Z}_{1t} \rVert_{\infty}) \\
  &\leq \frac{2 C b_T^{1/2}}{(P\gamma)^{1/2}} \vtSuma (\lVert Z_{1t} - \tilde{Z}_{1t} \rVert_2
  + \lVert Z_{2t} - \tilde{Z}_{2t} \rVert_2).
\end{align*}

And we can finish the proof with the following inequalities:
\begin{align*}
  \frac{2 C b_T^{1/2}}{(P \gamma)^{1/2}} &\vtSuma \lVert Z_{1t} - \tilde{Z}_{1t} \rVert_2 \\& \leq
  \frac{4C b_T^{1/2}}{P\gamma} \vtSuma \sum_{l=\vttLower}^{\vttUpper} \lVert
  D_{t+l+R,R} - \tilde{D}_{t,t+l+R} \rVert_2 W(l/\gamma) \\&
  \leq O\Bigg(\frac{b_T^{1/2}}{P\gamma}\Bigg) \vtSuma
  \sum_{l=\vttLower}^{\vttUpper} \couplingBeta{t+l-(2i-2m-1)b_T+P}\\&
  = O\Bigg(\frac{b_T^{1/2}}{P\gamma}\Bigg) O(b_T^{3/2-u}\, m^{-1/2-u})
\end{align*}
for some positive $u$.  The same argument holds for $Z_{2t}$.
As a result,
\[
E\Big(\vtSumr (U_i - \E_R U_i)\Big)^2 = o\Big(\vtSumr b_T^2/P\gamma\Big) =
 o(b_T/\gamma) \to 0,
\]
as required.
\end{proof}

%\newcommand{\DD}[2][{}]{(D_{#2}^{#1} - \E_R D_{#2}^{#1})}
%\newcommand{\ovg}[1]{\tfrac{1}{P} \sum_{#1=R+1}^{T-h}}

\begin{lem}\label{lem:a6}
  Suppose the conditions of Lemma~\ref{res-variance-estimator} hold.
  Then
  \begin{equation*}
    \hat{\sigma}_R^2 - P^{-1} \sum_{s,t=R+1}^{T-h} (D_{Rs} - \E_R
    D_{Rs}) (D_{Rt} - \E_R D_{Rt}) W((t-s)/\gamma) \to^{L_1} 0.
  \end{equation*}
\end{lem}

\begin{proof}
   It follows from simple algebra that
\begin{multline*}
  \Big| \hat\sigma_R^2 -  P^{-1} \sum_{s,t=R+1}^{T-h} (D_{Rs} - \E_R
    D_{Rs}) (D_{Rt} - \E_R D_{Rt}) W((t-s)/\gamma) \Big| \leq\\
  \varianceDiffA \\ + P^{-1} \oosSum{s,t}{1} \lvert (D_{Rs} -
  \oosA)(\E_R D_{Rt} - \oosA) \rvert \vWeight + o_p(1).
\end{multline*}
We'll prove that these two sums are $o_p(1)$; uniform integrability
then implies convergence in $L_1$.  The arguments are $o_p(1)$ are
almost identical, so we'll only present the first.

Applying the Cauchy-Scwarz inequality twice and simplifying gives the
upper bound
\begin{multline*}
\varianceDiffA \\ \leq O(1) \Big[\varianceDiffAii\Big]^{1/2} \Big[\varianceDiffAi\Big]^{1/2}.
\end{multline*}
Now,
$P^{-1} \oosSum{s}{1}(D_{Rs} - \E_R D_{Rs})^2 = O_p(1)$, and it suffices to prove
that \[\varianceDiffAi = o_p(1).\]  Observe that
\begin{align*}
  \varianceDiffAi & = O_p\Big(P^{-1} \oosSum{s}{1}(\E_{R}D_{Rs} -
  \E_R\oosA)^2\Big) + O_p(\oosA - \E_R\oosA)^{2} \\
  &= O_p\Big(P^{-1} \oosSum{s}{1} (\E_RD_{Rs})^2 - (\E_R\oosA)^2\Big) +
  o_p(1),
\end{align*}
with the second term vanishing by Lemma \ref{res-mixingale} and
\citepos{Dav:93} mixingale \lln.

Now define $\tilde D_s$, $s = R+1,\dots,T-\h$, as in Lemma
\ref{lem-basic-coupling} so that $\E_{s-1} \tilde D_s = \E_R \tilde
D_s$ almost surely.  Note that we also have the equality $\E_R \tilde
D_s = \E_R \tilde D_{R+1}$ almost surely for all $s\geq R+1$, and so
\[
P^{-1} \oosSum{s}{1} (\E_R \tilde D_s)^2 = \Big(P^{-1} \oosSum{s}{1} \E_R
\tilde D_s\Big)^2 \quad \text{a.s..}
\]

Consequently,
\begin{align*}
P^{-1} \oosSum{s}{1} (\E_R D_s)^2 - (\E_R\oosA)^2 & =
P^{-1} \oosSum{s}{1} [(\E_R D_s)^2 - (\E_R\tilde D_s)^2]\\
&\quad+ \Big(P^{-1} \oosSum{s}{1} \E_R \tilde D_s\Big)^2 - (\E_R\oosA)^2 \quad \text{a.s}.\\
&= O_p\Big(P^{-1} \oosSum{s}{1} [(\E_R D_s)^2 - (\E_R \tilde D_s)^2]\Big).
\end{align*}
Finally,
\begin{align*}
\Big\lVert P^{-1} \oosSum{s}{1} [(\E_RD_s)^2 - (\E_R \tilde D_s)^2] \Big\rVert_1
&\leq P^{-1} \oosSum{s}{1} \lVert \E_RD_s - \E_R \tilde D_s \rVert_2 \lVert
\E_RD_s + \E_R \tilde D_s \rVert_2
\\ &\leq (4 B_L/P) \oosSum{s}{1} \lVert \E_RD_s - \E_R \tilde D_s \rVert_2,
\end{align*}
and this last term vanishes as in the proof of Lemma~\ref{lem:a2}.
\end{proof}

\subsection*{Proof of Theorem \ref{res-confidence-intervals}}

It suffices to prove asymptotic normality.  We can rewrite the
centered \oos\ average as
\[
\frac{\bar{D}_{R} - \E_T \bar{D}_T}{\hat\sigma_R / \sqrt{P}} =
\frac{1}{\hat\sigma_R \sqrt{P}}\oosSum{t}{1} (D_{Rt} - \E_R D_{Rt}) +
\frac{\sqrt{P}}{\hat\sigma_R} (\E_R\oosA - \E_T\oosB).
\]
Lemmas \ref{res-mixingale} and
\ref{res-convergence} ensure that the first term is asymptotically
normal and the second term vanishes.\qed

\subsection*{Proof of Theorem \ref{res:oostest}}
By Lemma~\ref{res-convergence}, we know that
\begin{equation*}
  \Pr[\E_R \oosA \leq 0] - \Pr[\E_T \oosB \leq 0] \to 0
\end{equation*}
and
\begin{equation*}
  \Pr[P^{1/2} \oosA / \hat\sigma > z_\alpha \text{ and } \E_R \oosA \leq 0]
  - \Pr[P^{1/2} \oosA / \hat\sigma > z_\alpha \text{ and }
  \E_T \oosB \leq 0] \to 0.
\end{equation*}
For large enough $T$, both $\Pr[\E_R \oosA \leq 0]$ and $\Pr[\E_T
\oosB \leq 0]$ are positive, so
\begin{equation*}
  \Pr[P^{1/2} \oosA / \hat\sigma > z_\alpha
  \mid \E_R \oosA \leq 0] =
  \frac{\Pr[P^{1/2} \oosA / \hat\sigma > z_\alpha \text{ and }
    \E_R \oosA \leq 0]}{\Pr[\E_R \oosA \leq 0]}
\end{equation*}
and
\begin{equation*}
  \Pr[P^{1/2} \oosA / \hat\sigma > z_\alpha
  \mid \E_T \oosB \leq 0] =
  \frac{\Pr[P^{1/2} \oosA / \hat\sigma > z_\alpha \text{ and }
    \E_T \oosB \leq 0]}{\Pr[\E_T \oosB \leq 0]}
\end{equation*}
almost surely and consequently
\begin{equation*}
  \Pr[P^{1/2} \oosA / \hat\sigma > z_\alpha
  \mid \E_T \oosB \leq 0]
  - \Pr[P^{1/2} \oosA / \hat\sigma > z_\alpha
  \mid \E_R \oosA \leq 0] \to^p 0.
\end{equation*}
Finally,
\begin{align*}
  \Pr[P^{1/2} \oosA / \hat\sigma > z_\alpha \mid \E_R \oosA \leq 0]
  &\leq \Pr[P^{1/2} (\oosA - \E_R \oosA) / \hat\sigma > z_\alpha
  \mid \E_R \oosA \leq 0] \\
  &= \E( \Pr[P^{1/2} (\oosA - \E_R \oosA) / \hat\sigma > z_\alpha
  \mid \mathcal{F}_R] \mid \E_R \oosA \leq 0) \\
  &= \E(\alpha \mid \E_R \oosA \leq 0) \\
  &= \alpha
\end{align*}
where the second equality holds by Lemma~\ref{res-mixingale} and
Theorem~\ref{res-variance-estimator}, completing the proof.
\qed

\subsection*{Proof of Lemma \ref{res-convergence}}
\newcommand{\resConvgRHS}[1]{\ensuremath{\E(L(y^* - x_1^{*\prime}\bh{1#1}) - L(y^{*} -
x_2^{*\prime}\bh{2#1}) \mid \bh{#1})}}
\newcommand{\resConvgEstDiff}[1]{\ensuremath{\E(L(y^{*} -
    x_{#1}^{*\prime}\bh{#1R}) \mid \bh{R}) -
\E(L(y^{*} - x_{#1}^{*\prime}\bh{#1T}) \mid \bh{T})}}
\newcommand{\resConvgEstDiffRV}[1]{\ensuremath{L(y^{*} -
    x_{#1}^{*\prime}\bh{#1R}) - L(y^{*} - x_{#1}^{*\prime}\bh{#1T})}}

Equation~\eqref{eq:7} holds if we show
\begin{gather}
\E_R \oosA = \resConvgRHS{R} + o_p(P^{-1/2}),\label{eq:18}\\
\E_T \oosB = \resConvgRHS{T} + o_p(Q^{-1/2}),\label{eq:19}
\intertext{and}
\resConvgEstDiff{i} = O_p(\sqrt{P/T}),\label{eq:20}
\end{gather}
where $\hat\theta_R = (\hat\theta_{1R}, \hat\theta_{2R})$,
$\hat\theta_T = (\hat\theta_{1T}, \hat\theta_{2T})$, and $y^{*}$,
$x_1^{*}$ and $x_2^{*}$ are random variables drawn from the joint
distribution of $(y_{t+\h},x_{1t},x_{2t})$ independently of
$\mathcal{F}_T$.

For~\eqref{eq:18}, define $D_t^*$ for each $t=R+1,R+2,\dots,T-h$ so
that
\begin{equation*}
  \| D_t^* - D_{Rt} \|_2 \leq 2^{(1+\rho)/\rho} B_L \beta_{t-R}^{(\rho-2)/2\rho}
\end{equation*}
and
\begin{equation*}
  \E_R D_t^* = \E(L(y^* - x_1^{*\prime}\hat\theta_{1R}) -
  L(y^* - x_2^{*\prime}\hat\theta_{2R}) \mid \hat\theta_R) \qquad a.s.
\end{equation*}
Lemma~\ref{lem-basic-coupling} ensures that these $D_t^*$ exist.
Now,
\begin{align*}
  \Big\lVert \E_R \oosA - \E\Big(P^{-1} \sum_{t=R+1}^{T-h} D_t^* \mid
  \hat\theta_R\Big) \Big\rVert_2
  &= \Big\lVert \E_R\Big( \oosA - P^{-1} \sum_{t=R+1}^{T-h} D_t^*
  \Big) \Big\rVert_2 \\
  &= P^{-1} \sum_{t=R+1}^{T-h} \lVert D_{Rt} - D_t^* \rVert_2\\
  &= O(P^{-1}) \sum_{t=R+1}^{T-h} \beta_{t-R}^{(\rho-2)/2\rho}
\end{align*}
and this last term is $o(P^{-1/2})$ by assumption.  Essentially the
same argument proves~\eqref{eq:19} as well.

For~\eqref{eq:20}, observe that
\begin{align*}
  \|L(y^{*} -  x_i^{*\prime} \hat{\theta}_{iR}) -
  L(y^{*} - x_i^{*\prime} \hat{\theta}_{iT}) \|_1
  &\leq B_{L} P^{-1/2} \| x_i^{*\prime} (\hat{\theta}_{iR} - \hat{\theta}_{iT}) \|_2 \\
  &\leq B_{L} P^{-1/2}\big(\| x_i^{*\prime} [(X_{iT}'X_{iT})^{-1} -
  (X_{iR}'X_{iR})^{-1}] X_{iT}' \eb_{iT} \|_2 \\
  &\quad+ \| x_i^{*\prime}
  (X_{iR}'X_{iR})^{-1}[X_{iT}'\eb_{iT} - X_{iR}
  \eb_{iR} ] \|_2 \big).
\end{align*}
The first term in the upper bound satisfies
\begin{align*}
  \| x_i^{*\prime} &[(X_{iT}'X_{iT})^{-1} -
  (X_{iR}'X_{iR})^{-1}] X_{iT}' \eb_{iT} \|_2^2 =\\
  &= \E \tr\big( x_i^* x_i^{*\prime} X_{iT}'\eb_{iT} \eb_{iT}X_{iT}
   [(X_{iT}'X_{iT})^{-1} - (X_{iR}'X_{iR})^{-1}]^2 \big) \\
  &\leq \eigen_{K_i}(\E x_i x_i') \E \Big|  \eigen_{K_i}(X_{iT}' \eb_{iT}
  \eb_{iT}' X_{iT}) \sum_{j=1}^{K_i} \eigen_j^2((X_{iR}'X_{iR})^{-1} -
  (X_{iT}'X_{iT})^{-1}) \Big| \\
  &= O(T) \, \Big\| \sum_{j=1}^{t-R} \eigen_j^{-2}(X_{iR}'X_{iR}) \Big\|_{3/2} \\
  &= O(P^2/R)
\end{align*}
The second equality holds because $\eigen_{K_i}(X_{iT}' \eb_{iT}
\eb_{iT} X_{iT}) = O_{L_{3}}(T)$ by assumption and
$(X_{iR}'X_{iR})^{-1} - (X_{iT}'X_{iT})^{-1}$ has rank $t-R$ with
eigenvalues bounded by those of $(X_{iR}'X_{iR})^{-1}$.

A similar argument proves that
\begin{equation*}
  \big\| x_i^{*\prime} (X_{iR}'X_{iR})^{-1}
  [X_{iT}'\eb_{iT} - X_{iR} \eb_{iR} ] \big\|_2 \leq \Delta^3 P^2/R
\end{equation*}
as well, completing the proof.
\qed

\subsection*{Proof of Lemma \ref{res-mixingale}}
\noindent \textit{Part 1.} Fix $R$, $j$ and $l$ and use Lemma
\ref{lem-basic-coupling} to define $\tilde{D}_{R+j}$ so that
\[\E_R\tilde D_{R+j} = \E_{R+j-l} \tilde D_{R+j} \quad a.s. \]
and
\[\lVert D_{R+j,R} - \tilde D_{R+j} \rVert_2 \leq \couplingBound{l}.\]
Then
\begin{align*}
\lVert \E_{R+j-l} D_{R+j,R} - \E_R D_{R+j,R} \rVert_2 & \leq
\lVert \E_{R+j-l} D_{R+j,R} - \E_{R+j-l} \tilde{D}_{R+j} \rVert_2 \\
&\quad + \lVert
\E_{R+j-l} \tilde{D}_{R+j} - \E_R D_{R+j,R} \rVert_2 \\
&\leq 2 \lVert D_{R+j,R} - \tilde D_{R+j} \rVert_2 \\
&\leq 2^{2 + 1/\rho}\ B_L\ \couplingBeta{l}.
\end{align*}
Now $\couplingBeta{l} = O(l^{-1/2 - \delta})$ for some
positive $\delta$, completing the proof.\qed

\vspace{\baselineskip}
\noindent\textit{Part 2.}  A modification of \citepos{Jon:97} \clt s for
mixingale and \ned\ arrays will complete the proof.  Define
\[\label{eq:z1}
  Z_i = \ZDef
\]
where $b_T$ is a sequence that satisfies $b_T\leq P$,
$b_T\to\infty$, and $b_T/P\to 0$.  The same arguments used in
\citepos{Jon:97} Theorem 1 show that
\begin{equation*}
   \SumOuterBlock{i} Z_i = P^{-1/2} \oosSum{s}{1} (D_{Rt} - \E_R D_{Rt}) + o_p(1).
\end{equation*}
and
\begin{equation*}
  \SumOuterBlock{i} Z_i = \SumOuterBlock{i} (Z_i - E_{R + (i-1)b_T}
  Z_i) + o_p(1).
\end{equation*}
Note that $\{Z_i - E_{R + (i-1)b_T},\mathcal{F}_{R + i b_T}\}_i$ is an
\mds\ by construction, so \citepos{HaH:80} Theorem 3.2 and Corollary
3.1 ensure that $\sigma_R^{-1} \SumOuterBlock{i} Z_i \to^d N(0,1) $ as
long as
\begin{equation}\label{eq:cltvar2}
  \sigma_R^2 - \SumOuterBlock{i} \E_R Z_i^2 \to^p 0,
\end{equation}
and
\begin{equation}\label{eq:cltvar1}
  \SumOuterBlock{i} \E_R Z_i^2 - \SumOuterBlock{i} \ZSqCE \to^p 0.\footnote{Note that $\sigma_R^2 \in \mathcal{F}_t$ for
    all $t \geq R$, so Hall and Heyde's condition (3.21) is
    unnecessary---see the remarks after their result.}
\end{equation}
Equation (\ref{eq:cltvar2}) holds as in \citet{Jon:97} (see the proof
of his Theorem 2); (\ref{eq:cltvar1}) is ensured by
Lemma~\ref{lem:a2}. \qed

\subsection*{Proof of Lemma \ref{res-variance-estimator}}
The proof follows \citepos{JoD:00} Theorem 2.1 closely.  We
start by defining similar quantities to theirs, borrowing their
notation when possible to make the similarities apparent.  For
positive $\delta$, define $b_T \equiv \lfloor \gamma/\delta \rfloor$,
$\kernelB{x} \equiv \kernelBDefn{x}$, and
\begin{align*}
  \varianceTermI &\equiv
  \varianceTermIDefn,\\ \varianceTermII &\equiv \vtSum
  \varianceTermIIa\\& \quad \times \varianceTermIIb,\\
  \varianceTermIII &\equiv \vtSum \varianceTermIIIa\\
  &\quad \times \varianceTermIIb,\\
  \varianceTermIV &\equiv \vtSum \varianceTermIIIa \\
  &\quad\times \varianceTermIVb,
\end{align*}
giving us the inequalities
\begin{align*}
  \lVert \hat\sigma_R^2 - \sigma_R^2 \rVert_1 &
  \leq \lVert \hat\sigma_R^2 - \varianceTermI \rVert_1
  + \lVert \varianceTermI - \varianceTermII \rVert_1
  + \lVert \varianceTermII - \varianceTermIII \rVert_1
  + \lVert \varianceTermIII - \varianceTermIV \rVert_1\\ & \quad
  + \lVert \varianceTermIV - \E_R \varianceTermIV \rVert_1
  + \lVert \E_R\varianceTermIII - \E_R\varianceTermIV \rVert_1
  + \lVert \E_R\varianceTermII - \E_R\varianceTermIII \rVert_1\\ & \quad
  + \lVert \E_R\varianceTermI - \E_R\varianceTermII \rVert_1
  + \lVert \E_R \varianceTermI - \sigma_R^2 \rVert_1
  \\ &
  \leq  \lVert \hat\sigma_R^2 - \varianceTermI \rVert_1
  + 2(\lVert \varianceTermI - \varianceTermII \rVert_1
      + \lVert \varianceTermII - \varianceTermIII \rVert_1
      + \lVert \varianceTermIII - \varianceTermIV \rVert_1) \\
  & \quad + \lVert \varianceTermIV - \E_R \varianceTermIV \rVert_1
  + \lVert \E_R \varianceTermI - \sigma_R^2 \rVert_1.
\end{align*}
De Jong and Davidson (2000) prove that
\begin{equation} \label{dd1}
\lim_{\delta\to0} \limsup_{T\to\infty} \lVert \varianceTermI -
\varianceTermII\rVert_1 = 0,
\end{equation}
\begin{equation} \label{dd2}
\lim_{\delta\to0} \limsup_{T\to\infty} \lVert \varianceTermII - \varianceTermIII
\rVert_1 = 0,
\end{equation}
\begin{equation} \label{dd3}
\lim_{\delta\to0} \limsup_{T\to\infty} \lVert \varianceTermIII - \varianceTermIV
\rVert_1 = 0,
\end{equation}
and
\begin{equation} \label{dd4}
\lim \lVert \E_R \varianceTermI - \sigma_R^2 \rVert_1 = 0.
\end{equation}
Their proofs of \eqref{dd1}--\eqref{dd4} use the fact that \ned\
functions of mixing processes are also mixingale processes and do not
use any other properties specific to \ned\ processes, so they hold
here as well.  We do need to modify their proofs that
\begin{equation}
  \label{eq:4} \lVert \hat{\sigma}_R^2 - \varianceTermI \rVert_1 \to 0
\end{equation}
and
\begin{equation}
  \label{eq:5} \lVert \varianceTermIV - \E_R \varianceTermIV \rVert_1
  \to 0 \quad \text{for all positive $\delta$},
\end{equation}
though, since their proofs exploit \ned\ properties.  The proofs
follow from Lemmas~\ref{lem:a2} and~\ref{lem:a6}, since
\begin{equation*}
  \varianceTermIV - \E_R \varianceTermIV = \vtSum (Z_{1t}Z_{2t} - \E_R Z_{1t}Z_{2t});
\end{equation*}
$Z_{1t}$ and $Z_{2t}$ are defined in Lemma~\ref{lem:a2}.
\qed

\subsection*{Proof of Theorem~\ref{res:ftest}}

\textit{Equation~\eqref{eq:11}}: Note that a necessary condition for
$(\hat{\theta}_T - \theta_0)'V_T (\hat{\theta}_T - \theta_0) < c$
as $\theta'V_T\theta \to \infty$ is for all of the mass of
$\hat{\theta}_T$ to concentrate on the line joining $\theta$ and
$\theta_0$.  For any distribution of $(\e_{t+h}, x_t')$ such
that $\hat{\theta}_T$ depends on $\theta$ only through its mean, the
Lebesgue measure of parameter sequences satisfying this condition is
zero. \qed

\vspace{\baselineskip}
\noindent\textit{Equation~\eqref{eq:15}}:  Suppose that
$(\e_{t+h}, x_t) \sim i.i.d. N(0,I)$ and condition on $X_T$.
Then the distribution of $\hat{\theta}_{T}$ concentrates uniformly on
the ellipsoid
\begin{equation*}
  (\hat{\theta}_T - \theta)'(\hat{\theta}_T - \theta) = \theta'\theta.
\end{equation*}
Now, for $\theta$ and $V_T$ such that
\begin{equation*}
  (\theta-\theta_0)' V_T (\theta-\theta_0) > c + \delta,
\end{equation*}
the probability that $\hat{\theta}_T$ is inside the acceptance region
of the test is less than $(1/2)^{K_2}$, which converges to zero as $T
\to \infty$. \qed

\bibliographystyle{abbrvnat}
\bibliography{texextra/AllRefs}

\begin{table}[h]
 \begin{multicols}{2}
  \begin{itemize}
  \item log Dividend Price Ratio
  \item log Earnings Price Ratio
  \item Stock Market Variance
  \item Book to Market Ratio
  \item Net Equity Expansion
  \item Percent Equity Issuing
  \item Treasury Bill
  \item Long Term Yield
  \item Long Term Rate
  \item Default Return Spread
  \item Default Yield Spread
  \item Inflation
  \end{itemize}
 \end{multicols}
 \caption{Variables used in Section \ref{sec:empirics} and by Goyal
   and Welch (2008) to predict the equity premium.  Please see Goyal
   and Welch's original paper
   for a detailed description of each variable.}
 \label{tab:equity}
\end{table}

\begin{table}[b]
 \begin{center}
 \begin{tabular}{lrrrr}\hline\hline
   \multicolumn{1}{l}{waldtest}&\multicolumn{1}{c}{Res.Df}&\multicolumn{1}{c}{Df}&\multicolumn{1}{c}{F}&\multicolumn{1}{c}{Pr(\textgreater
     F)}\tabularnewline
   \hline
   1&69&&&\\
   2&81&$-$12&3.4&0.00066 \\
   \hline
\end{tabular}
\caption{Wald test for the null hypothesis that all of the
  coefficients in Goyal and Welch's (2008) ``kitchen sink'' model are
  zero, except for the intercept, using the Newey-West
  variance-covariance matrix.  The p-value is calculated from the $F$
  distribution.\label{tab:gwinsample}}
\end{center}
\end{table}

\clearpage
\begin{figure}
  \centering {\large Simulated Coverage of One-Sided DMW Interval for
    $\E_R\oosA$} \input{mc/plots/interval-testerror1.tikz}
\caption{Simulated coverage of $\E_R \oosA$ at 90\% confidence using a
  one-sided interval based on the \protect\textsc{dmw}
  \protect\textsc{oos} test, plotted as a function of the fraction of
  observations used in the test sample, $P/T$.  The solid horizontal
  line denotes the intervals' nominal coverage.}
 \label{fig:interval-R}
\end{figure}
\clearpage
\begin{figure}
  \centering {\large Simulated Coverage of One-Sided DMW Interval for
    $\E_T\oosB$} \input{mc/plots/interval-generror1.tikz}
\caption{Simulated coverage of $\E_T \oosB$ at 90\% confidence using a
  one-sided interval based on the \protect\textsc{dmw}
  \protect\textsc{oos} test, plotted as a function of the fraction of
  observations used in the test sample, $P/T$.  The solid horizontal
  line denotes the intervals' nominal coverage.}
\label{fig:interval-T}
\end{figure}
\clearpage

\begin{figure}
  \centering {\large Simulated Rejection Probability of $F$-test Under
    $\E_T \oosB \leq 0$}
  \input{mc/plots/ftest-001.tikz}
  \caption{Simulated rejection probabilities for the $F$-test given
    $\E_T \oosB \leq 0$ with nominal size 10\%.  Values greater than
    10\% indicate that the test rejects the benchmark model too often.
    See Section~\ref{sec:simulation-design} for a discussion of the
    simulation design.}
  \label{fig:ftest}
\end{figure}
\clearpage
\begin{figure}
  \centering {\large Simulated Rejection Probability of DMW Test Under
    $\E_T \oosB \leq 0$)} \input{mc/plots/size-dmwsize.tikz}
  \caption{Simulated rejection probabilities for the \protect\textsc{dmw}
    \protect\textsc{oos} $t$-test given $\E_T \oosB \leq 0$ with nominal
    size 10\%.  Values greater than 10\% indicate that the test
    rejects the benchmark model too often.  The solid horizontal line
    indicates the nominal rejection probability.  See
    Section~\ref{sec:simulation-design} for a discussion of the
    simulation design.}
  \label{fig:ttest-size}
\end{figure}

\begin{figure}
  \centering {\large Simulated Rejection Probability of Clark and West
    (2006, 2007) \\ Test Under $\E_T \oosB \leq 0$}
  \input{mc/plots/size-clarkwest.tikz}
  \caption{Simulated rejection probabilities for Clark and West's
    (2006, 2007) \protect\textsc{oos} test statistic given $\E_T \oosB
    \leq 0$ with nominal size 10\%.  Values greater than 10\% indicate
    that the test rejects the benchmark model too often.  The solid
    horizontal line indicates the nominal rejection probability.  See
    Section~\ref{sec:simulation-design} for a discussion of the
    simulation design.}
   \label{fig:clarkwest}
\end{figure}

\begin{figure}
  \centering {\large Simulated Rejection Probability of McCracken
    (2007) Test \\ Under $\E_T \oosB \leq 0$}
  \input{mc/plots/size-mccracken.tikz}
  \caption{Simulated rejection probabilities for McCracken's (2007)
    \protect \textsc{oos} test statistic given $\E_T \oosB \leq 0$ with
    nominal size 10\%.  Values greater than 10\% indicate that the
    test rejects the benchmark model too often.  The solid horizontal
    line indicates the nominal rejection probability.  See
    Section~\ref{sec:simulation-design} for a discussion of the
    simulation design.}
  \label{fig:mccracken}
\end{figure}

\begin{figure}
  \centering {\large Simulated Rejection Probability of DMW Test Under
    $\E_T \oosB > 0$} \input{mc/plots/size-dmwpower.tikz}
  \caption{Simulated rejection probabilities for the \protect \textsc{dmw}
    \protect\textsc{oos} $t$-test given $\E_T \oosB > 0$ with nominal
    size 10\%.  Values greater than 10\% indicate that the test
    rejects the benchmark model too often.  The solid horizontal line
    indicates the nominal rejection probability.  See
    Section~\ref{sec:simulation-design} for a discussion of the
    simulation design.}
  \label{fig:ttest-power}
\end{figure}

\begin{figure}
\centering
\large{Difference in OOS MSE of Prevailing Mean and\\ Kitchen
    Sink Models of Equity Premium (OLS)}
\input{empirics/plots/oos-mse-1.tikz}
\input{empirics/plots/oos-mse-1b.tikz}
\caption{\protect \textsc{oos} difference in the \protect\textsc{mse}
  of the prevailing mean benchmark and the kitchen sink model as a
  function of the test sample size, $R$.  Both models forecast the
  equity premium using annual data from 1928--2008.  The solid line
  gives the \protect \textsc{oos} average, and the shaded region indicates the
  one-sided 95\% confidence interval implied by the \protect
  \textsc{dmw} test.  The bottom panel is a detailed view of the top
  panel for $R \geq 50$.}
\label{fig:empirics1}
\end{figure}

\begin{figure}
\centering
\large{Difference in OOS MSE of Prevailing Mean and\\ Kitchen
    Sink Models of Equity Premium (CT)}
\input{empirics/plots/oos-mse-2.tikz}
\input{empirics/plots/oos-mse-2b.tikz}
\caption{\protect \textsc{oos} difference in the \protect\textsc{mse}
  of the prevailing mean benchmark and the kitchen sink model as a
  function of the test sample size, $R$.  Both models forecast the
  equity premium using annual data from 1928--2008.  The solid line
  gives the \protect \textsc{oos} average, and the shaded region indicates the
  one-sided 95\% confidence interval implied by the \protect
  \textsc{dmw} test.  The bottom panel is a detailed view of the top
  panel for $R \geq 50$.}
\label{fig:empirics2}
\end{figure}

\begin{figure}
\centering
\large{OOS MSE of Individual Forecasts of Equity Premium}
\input{empirics/plots/oos-ind-ks.tikz}
\input{empirics/plots/oos-ind-pm.tikz}
\caption{\protect{\textsc{oos} \textsc{mse} of the Prevailing Mean (\textsc{pm}) and
    Kitchen Sink (\textsc{ks}) models for equity premium prediction as
    a function of the size of the training sample, $R$.  Please note
    that the vertical scales are different in the two plots.}}
\label{fig:empirics3}
\end{figure}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:

% LocalWords:  McCracken's kilian anatolyev rossi REStud JAppliedEconometrics
% LocalWords:  goyal welch rfs reputational calhoun Berbee's mixingales Rt tS
% LocalWords:  Heteroskedasticity Autocorrelation Tt iT th berbee iS de dx ixz
% LocalWords:  DMW mcleish Rs eq ftestreject lightgray veclen RSQLite zeileis
% LocalWords:  heteroskedasticity hothorn RNews JStatSoftware harrell campbell
% LocalWords:  thompson bivariate Welch's multicollinearity Newey newey tw vw
% LocalWords:  merlevede cltvar dR Scwarz davidson indices lrrrr waldtest Df jt
% LocalWords:  dmw McCracken MSE OLS mse inoue huber elemstatlearning efron sw
% LocalWords:  anova akritas jong normalpart coefpart Jong's rproject rsqlite
% LocalWords:  unrestrictive datasets Heyde's ib oos Mcc MeR StW InK DiM CCS lm
% LocalWords:  ClM CoS ClW GiW GiR GoW Cla HTF Efr BoB AkA AkP DeD Mcl Dej Rde
% LocalWords:  Sar Zeh Zei Har CaT NeW MeP HaH AllRefs Diebold jel Whi RoW brc
% LocalWords:  overrejection overreject fpe JoD iR PeT Giacomini WN Wishart T'V
% LocalWords:  premultiply const overrejects overrejecting BWB Gia Schwarz HHK
% LocalWords:  Haf Anatolyev's nonstochastic T'X Sweave tikzDevice pgfSweave

