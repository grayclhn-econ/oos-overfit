\documentclass[12pt]{article}
\input{setup}

\title{Out-of-Sample Comparisons of Overfit Models}
\author{Gray Calhoun\thanks{email: gcalhoun@iastate.edu. I
    would like to thank Julian Betts, Helle Bunzel, Marjorie Flavin,
    Nir Jaimovich, Lutz Kilian, Ivana Komunjer, Michael McCracken,
    Seth Pruitt, Ross Starr, Jim Stock, Yixiao Sun, Allan Timmermann,
    Hal White, several anonymous referees, participants at the 2010
    Midwest Economics Association Annual Meetings, the 2010
    International Symposium on Forecasting, the 2010 Joint Statistical
    Meetings, the 2011 NBER-NSF Time-Series
    Conference, and in many seminars at UCSD, and espeically
    Graham Elliott for their valuable suggestions, feedback and advice
    in writing this paper.  I would also like to thank Amit Goyal for
    providing computer code and data for his 2008 RFS paper
    with Ivo Welch \citep{GoW:08}.} \\ Iowa State University}

\begin{document}
\maketitle

\begin{abstract}\noindent
  This paper uses dimension asymptotics to study why overfit linear
  regression models should be compared out-of-sample; we let the
  number of predictors used by the larger model increase with the
  number of observations so that their ratio remains uniformly
  positive.  Our analysis gives a theoretical motivation for using
  out-of-sample (OOS) comparisons: the DMW OOS test allows a forecaster to
  conduct inference about the expected future accuracy of his or her
  models when one or both is overfit.  We show analytically and
  through Monte Carlo that standard full-sample test statistics can
  not test hypotheses about this performance.  Our paper also shows
  that popular test and training sample sizes may give misleading
  results if researchers are concerned about overfit.  We show that
  $P^2/T$ must converge to zero for the DMW test to give valid
  inference about the expected forecast accuracy, otherwise the test
  measures the accuracy of the estimates constructed using only the
  training sample.  In empirical research, $P$ is typically much
  larger than this.  Our simulations indicate that using large values
  of $P$ with the DMW test gives undersized tests with low power, so
  this practice may favor simple benchmark models too much.

\noindent JEL Classification: C12, C22, C52, C53

\noindent Keywords: Generalization Error, Forecasting, Model
Selection, $t$-test, Dimension Asymptotics
\end{abstract}
\newpage

\section{Introduction}
\label{sec:introduction}

Consider two sequences of length $P$ of prediction errors, the result
of forecasting the same variable with two different estimated models.
Both models are estimated with $R$ observations, collectively called
the {\em estimation window}, and are used to forecast an additional
$P$ observations, called the {\em test sample}.  There are $T$
observations in all, and $R+P=T$.  This paper introduces a new limit
theory for statistics constructed from these prediction errors
designed to approximate the behavior of the statistics when one of the
models is overfit.  In doing so, we provide a theoretical
justification for forecasters to use OOS instead of in-sample
comparisons: the DMW OOS test%
\footnote{In this paper, we will refer to the basic OOS $t$-test
  studied by \citet{DiM:95} and \citet{Wes:96} as the DMW test.} %
allows a forecaster to conduct
inference about the expected future accuracy of his or her models when
one or both is overfit.  We show analytically and through Monte Carlo simulations
that standard full-sample test statistics can not test hypotheses
about this performance.

Our paper also shows that popular test and training sample sizes may
give misleading results if researchers are concerned about overfit.
We show that $P^2/T$ must converge to zero for the DMW test to give
valid inference about the expected forecast accuracy, otherwise the
test measures the accuracy of the estimates constructed using only the
training sample.  In empirical research, $P$ is typically much larger
than this.  Our simulations indicate that using large values of $P$
with the DMW test gives undersized tests with low power, so this
practice may favor simple benchmark models too much.  Existing
corrections, proposed by \citet{ClM:01,ClM:05}, \citet{Mcc:07} and
\citet{ClW:06,ClW:07}, seem to overcorrect for this problem, though, and reject
too often when the benchmark model is more accurate.

Although OOS comparisons have been popular in Macroeconomics and
Finance since \citepos{MeR:83} seminal study of exchange rate models,
it has been unclear from a theoretical perspective whether or not the
statistics are useful.  Empirical researchers often cite ``overfit''
or ``instability'' as reasons for using OOS comparisons, as in
\citet{StW:03}, but neither term is precisely defined or formalized.
Compounding this problem, the asymptotic distributions of these
statistics are derived under conditions that rule out either
instability or overfit and allow a researcher to use a conventional
in-sample comparison---a variation of the $F$-test, for example.  As
\citet{InK:04} argue, the statistics themselves are designed to test
hypotheses that can be tested by these in-sample statistics.  For
example, \citet{DiM:95} and \citet{Wes:96} derive the limiting
distributions of many popular OOS test statistics under conditions
that would justify these full-sample tests.  Much
of the subsequent research by \citet{Mcc:00, Mcc:07}, \citet{CCS:01},
\citet{ClM:01,ClM:05}, \citet{CoS:02,CoS:04}, \citet{ClW:06,ClW:07},
\citet{Ana:07}, and others relaxes several of Diebold and Mariano's
and West's assumptions, but maintains the stationarity and dependence
conditions that permit in-sample comparisons \citep[see][for a
review of this literature]{Wes:06}.%
\footnote{Like us, \citet{Ana:07} allows the number of regressors to
  increase with $T$.  But in that paper, the number of regressors
  increases slowly enough that the OLS coefficients are consistent and
  asymptotically normal.} %
\citet{GiW:06} and
\citet{GiR:09, GiR:10} are exceptions.  Instead of focusing on
hypotheses that can be tested by in-sample comparisons, \citet{GiW:06}
derive an OOS test for the null hypothesis that the difference
between two models' OOS forecasting performance is unpredictable, a
martingale difference sequence (MDS); \citet{GiR:09} test whether the OOS
forecasting performance of a model suffers from a breakdown relative
to its in-sample performance; and \citet{GiR:10} test whether the
forecasting performance is stable. However, those papers focus on a
particular OOS estimation strategy and do not address why OOS
comparisons might be useful as a general strategy.

Since in-sample and OOS statistics require similar assumptions and
test similar hypotheses, one might expect that they would give similar
results.  They do not.  In-sample analyses tend to support more
complicated theoretical models and OOS analyses support simple
benchmarks, as seen in \citet{MeR:83}, \citet{StW:03}, and
\citet{GoW:08}.  Since these different approaches strongly influence
the outcome of research, it is important to know when each is
appropriate.  The explanations in favor of OOS comparisons claim
that they should be more robust to unmodeled instability
\citep{ClM:05,GiW:06,GiR:09,GiR:10} or to overfit
\citep{Mcc:98,Cla:04}.  Both explanations presume that the in-sample
comparison is invalid and the OOS comparisons are more reliable.  Of
course, as \citet{InK:04,InK:06} point out, both in-sample and OOS
methods could be valid, but the OOS methods could have lower power.

In this paper, we study the ``overfit'' possibility and leave
``instability'' to future research. This paper uses dimension
asymptotics to study the behavior of OOS comparisons when at least
one of the models is overfit---the number of its regressors
increases with the number of observations so that their ratio remains
positive.  Although overfit is sometimes used to describe the
situation where a forecaster chooses from many different models,
i.e. \emph{data-mining} or \emph{data-snooping}, we view these as
separate issues.  Procedures that account for the presence of many
models have been, and are continuing to be, developed \citep[see, for
example,][]{Whi:00,Han:05,RoW:05,HHK:10,ClM:12b}, but it is unclear
whether those procedures should themselves use in-sample or OOS
comparisons.  Understanding the difference between in-sample and OOS
comparisons in the context of a simple comparison between two models
is necessary before resolving any new issues that arise with multiple
comparisons. Moreover, the empirical research that motivates
this paper uses \emph{pseudo} OOS comparisons and not
true OOS comparisons.  Even if a true OOS comparison
could account for some forms of data-snooping better than
\citepos{Whi:00} BRC or its extensions, in-sample and pseudo
OOS comparisons would both be affected by the data-snooping, a
point also made by \citet{InK:04}.

We focus on linear regression models estimated with a fixed window
for simplicity, but our basic conclusions should be true for other
models and estimation strategies as well.  Under this
asymptotic theory, where the number of regressors $K$ increases with $T$
so that $\lim K/T$ is positive and less than one,
the OLS coefficient estimator is no longer
consistent or asymptotically normal \citep{Hub:73} and has positive
variance in the limit.  We show that, even so, the usual OOS average
is asymptotically normal and can consistently estimate the difference
between the models' generalization error, the expected loss in the
future conditional on the on the data available in period
$T$.%
\footnote{See, for example, \citet{HTF:08} for a discussion of
  generalization error.} %
Under these asymptotics, the generalization
error does not converge to the expected performance of the pseudotrue
models, so existing in-sample and OOS comparisons measure different
quantities and should be expected to give different results for
reasons beyond simple size and power comparisons.  Under our limit theory,
the model that is closer to the true DGP in population can forecast
worse.  In such a situation, a standard in-sample comparison would
correctly reject the null hypothesis that the benchmark is true, and
an OOS comparison would correctly fail to reject the null that the
benchmark is more accurate.%
\footnote{In a pair of papers similar to
  ours, \citet{ClM:12,ClM:12b} study in-sample and OOS tests that
  the larger model has nonzero coefficients that are too close to zero
  to expect the model to forecast more accurately.  Like this paper,
  they argue that the larger model can be true but less accurate.
  However, they focus on an aspect of the DGP that makes this
  phenomenon likely, while we focus on the coefficient estimates that
  produce less accurate forecasts.  Moreover, the implications of our
  asymptotic theories are different and their papers do not provide
  reasons to do OOS comparisons.} %
Also note that, in this situation, a model that performs well
in-sample can perform badly out-of-sample even if there are no
structural breaks or other forms of instability.  Researchers
have argued that a breakdown of in-sample forecasting ability
indicates a structural break (see \citealp{BoH:99}, and
\citealp{StW:03}, among others), but we show that this breakdown
can be caused by overfit as well.

It is important to realize that we are not \emph{advocating} linear
regression for highly overparametrized models. If $K$ is very large
relative to $T$, researchers will often want to use some form of
shrinkage forecast, for example the LASSO \citep{Tib:96} or a factor
model \citep{StW:02,BaN:02}. Our main focus in practice is settings
where $K$ is \emph{moderately} large and it is not clear whether
overfit will dominate the results. In such settings that, it is
important to understand the behavior of different evaluation criteria
when $K$ is large, in case overfit is a concern, and to have reliable
methods for estimating the models' performance, which is the goal of
this paper.

Our theoretical results partially justify OOS comparisons when
researchers want to
choose a model for forecasting.  Although there has been little
emphasis on hypothesis testing in this setting, testing is usually
appropriate: there is usually a familiar benchmark model in place, and
the cost of incorrectly abandoning the benchmark for a less accurate
alternative model is higher than the cost of incorrectly failing to
switch to a more accurate alternative.  We show that the DMW test
lets the forecaster control the probability of the first error, just
as with conventional hypothesis testing.

But we also identify new practical limitations for applying the DMW
test to overfit models.
Since the models' coefficients are imprecisely estimated in the limit,
the test sample must be small enough that the model estimated over the
training sample is similar to the one that will be estimated over the
full sample.  In particular, $P/T \to 0$ is required for consistent
estimation of the difference in the two models' performance,
and $P^2/T \to 0$ is required for valid confidence
intervals and inference.  For larger $P$, the OOS comparisons remain
asymptotically normal, but are centered on the forecasting performance
associated with the period-$R$ estimates.  In practice, researchers
typically use large values of $P$, so these studies may be too
pessimistic about their models' future accuracy if they use the DMW
test.  Section~\ref{sec:oostheory} lays out the asymptotic behavior of
the DMW test under this limit theory.

Popular in-sample tests and model selection criteria, like the Wald
test and the AIC, do not help forecasters in this setting. We show
that these statistics do not select the more accurate model when
choosing between overfit models. For many DGPs the Wald test and the
will choose the larger alternative model over a small benchmark model
with probability much greater than its nominal size, regardless of
which model is more accurate, and the AIC behaves similarly. The BIC,
however, chooses the benchmark model with probability approaching one
even when the \emph{alternative} model is more accurate.%
\footnote{Our result holds for a broad class of full-sample
  statistics, but there may be other potential statistics that mimic
  the OOS test and remain valid. Exploring such statistics is left for
  future research.} %
This result holds even though modifications of the $F$-test are valid
under this asymptotic theory, as shown by \citet{BoB:95},
\citet{AkA:00}, \citet{AkP:04}, \citet{Cal:11c}, and \citet{Ana:12},
among others.%
\footnote{Also see \citet{Efr:86,Efr:04} for a discussion of naive
  in-sample loss comparisons.} %
Moreover, under this asymptotic theory, many recent OOS test
statistics, such as those derived by \cite{ClM:01,ClM:05},
\citet{Mcc:07}, and \citet{ClW:06,ClW:07} should have the same
problems as in-sample tests.%
\footnote{Our theoretical results apply directly to \citepos{Mcc:07}
  OOS $t$-test, since it simply proposes more liberal critical values
  for the same test statistic that we study.  Since
  \citet{ClW:06,ClW:07} use a finite length estimation window, our
  asymptotics are incompatible with theirs and prevent us from
  studying their test directly, as well as \citepos{GiW:06} and other
  tests based on \citepos{GiW:06} asymptotics.  But \clws\ test can be
  viewed as a stochastic adjustment to the critical values of the
  usual \oost\ test, so our conclusions should apply informally as
  well.  Specifically, we show that the DMW test rejects with
  probability equal to nominal size when the estimated benchmark model
  is expected to be more accurate, so more liberal critical values
  result in overrejection.} %
These tests are also designed to reject the benchmark when the
alternative model is true, and so they may reject too often when the
benchmark is misspecified but more accurate.  Obviously, since the
distribution of these statistics converges to the normal when $P/T \to
0$ (with the number of regressors fixed), these statistics behave like
the DMW test when $P$ is small, but should overreject the benchmark
when $P$ is large. Section~\ref{sec:insample} presents our theoretical
results for full-sample statistics and Section~\ref{sec:mc} presents
Monte Carlo evidence to support these claims.

Finally, this paper introduces a new method of proof for OOS
statistics.  We use a coupling argument (Berbee's Lemma, 1979) to show
that sequences of OOS loss behave like mixingales when the
underlying series are absolutely regular, even if the forecasts depend
on non-convergent estimators.  Moreover, transformations of these
processes also behave like mixingales when recentered, so 
many arguments used to prove asymptotic
results for Near Epoch Dependent (NED) functions of mixing processes
can be used for these OOS processes with only slight modification.

The rest of the paper proceeds as follows.
Section~\ref{sec:assumptions} introduces our notation and assumptions.
Section~\ref{sec:theory} gives the main theoretical results for the
DMW OOS test, shows that standard in-sample tests reject the
benchmark model too often when it is misspecified but more accurate than the
alternative, and shows that standard model selection methods can run
into similar problems. Section~\ref{sec:mc}
presents a Monte Carlo study supporting our theoretical results.
Section~\ref{sec:empirics} applies the OOS statistic to
\citepos{GoW:08} dataset for equity premium prediction, and
Section~\ref{sec:conclusion} concludes.  Proofs and supporting results
are listed in the Appendix.

\section{Setup and assumptions}
\label{sec:assumptions}

The first part of this section will describe the environment in detail
and set up our models and notation. The second part lists the
assumptions underlying our theoretical results.

\subsection{Notation and forecasting environment}

We assume the following forecasting environment. There are two
competing linear models that give forecasts for the target,
$y_{t+\h}$:
\begin{equation*}
y_{t+\h} = x_{1t}'\theta_1 + \e_{1,t+h}, \quad\text{and}\quad
y_{t+\h} = x_{2t}'\theta_2 + \e_{2,t+h};
\end{equation*}
$t = 1,\dots,T-h$, $\h$ is the forecast horizon and the
variables $y_t$, $x_{1t}$, and $x_{2t}$ are all known in period $t$.
The coefficients $\theta_1$ and $\theta_2$ minimize the population
Mean Square Error, so
\begin{equation*}
  \theta_i = \argmin_\theta \sum_{t=1}^{T-\h} \E (y_{t+\h} - x_{it}'\theta)^2,
\end{equation*}
making $\e_{i,t+\h}$ uncorrelated with $x_{i,t}$; $\e_{i,t+\h}$ can exhibit
serial correlation so both of the linear models may be misspecified.
Let $\mathcal{F}_t = \sigma(y_1, x_1, \dots, y_t, x_t)$ be the information
set available in period $t$,
with $x_t$ the vector of all stochastic elements of $x_{1t}$ and $x_{2t}$ after
removing duplicates, and let $\E_t$ and $\var_t$ denote the
conditional mean and variance given $\mathcal{F}_t$.  The first model
uses $K_1$ regressors, and the second uses $K_2$.  Without loss of
generality, assume that $K_1 \leq K_2$. At least one of the models is
overfit, which we represent asymptotically by letting $K_2$ grow with
$T$ quickly enough that $\lim K_2/T$ is positive; $K_1$ may grow with
$T$ as well. Since the models change with $T$, a
stochastic array underlies all of our asymptotic theory, but we
suppress that notation to simplify the presentation.

In the settings we are interested in, a forecaster observes the data
$(y_t,x_t)$ for periods 1 through $T$ and divides these observations
into an estimation sample of the first $R$ observations and a test
sample of the remaining $P$ observations. The forecaster then compares
the models' performance over the test sample, which entails
constructing two sequences of forecasts with a fixed-window
estimation strategy,
\begin{equation*}
\hat y_{i,t+\h} = x_{it}'\hat{\theta}_{it}, \qquad \text{for } i=1,2;
\ t = R+1,\dots,T-h,
\end{equation*}
where
\begin{equation*}
  \bh{it} = \Big(\sum_{s=1}^{R-\h} x_{is}x_{is}'\Big)^{-1} \sum_{s=1}^{R-\h}
  x_{is} y_{s+\h}, \qquad \text{for } i=1,2;\ t = R+1,\dots, T - \h.%
\footnote{It may not be clear why we are using the index $t$ in $\bh{it}$,
    since $\bh{it} = \bh{iR}$ almost surely for all $t \leq T-h$.
    But $\bh{it}$ will be defined for $t > T-\h$
    soon and will not equal $\bh{iR}$ for those values of $t$.} %
\end{equation*}

The models are then compared by their forecast performance over the test
sample. There are many statistics that have been considered in the
literature, but we focus on perhaps the most natural, the DMW \oost\
test \citep{DiM:95,Wes:96}.%
\footnote{The core insights of our paper apply to other OOS statistics
  as well.} %
This statistic is based on
the difference in the models' loss over the test sample, $\oosA$,
defined as
\begin{equation*}
  \oosA \equiv P^{-1} \oosSum{t}{1} D_t
\end{equation*}
where
\begin{equation*}
  D_t = L(y_{t+\h} - x_{1t}'\bh{1t}) - L(y_{t+\h} - x_{2t}'\bh{2t}),
\end{equation*}
and $L$ is a known loss function. The \oost\ test is defined as
$\sqrt{P} \oosA / \sh$, where $\sh^2$ is an estimator of the
asymptotic variance of $\oosA$. (Possibly a Heteroskedasticity- and
Autocorrelation-Consistent, or HAC, estimator.)

Statistics like $\oost$ have a long history in empirical economics
because they capture an intuitive idea of model fit: that a good model
should be able to forecast well on new data. \cite{MeR:83}
use such a statistic to study exchange rate models and find that none
of the models that existed at the time of their study outperform a
random walk. This finding has been remarkably durable and has spawned
an enormous literature; see \cite{Mar:95}, \cite{KiT:03},
\cite{CCP:05}, \cite{EnW:05}, \cite{Ros:05}, and \cite{BWB:10}, among
many others.

Most theoretical research on these statistics, such as \citet{DiM:95}, \citet{Wes:96},
and \cite{Mcc:07}, has focused on using the \oost\ statistic to test
hypotheses about the pseudotrue values $\theta_1$ and $\theta_2$.  In
particular, that research focuses on testing the null hypothesis that
\begin{equation*}
  \E L(y_{t+\h} - x_{1t}'\theta_{1}) = \E L(y_{t+\h} - x_{2t}'\theta_{2}).
\end{equation*}
But the population quantities in this equation do not determine which
model is more accurate in practice. The models'
accuracy will also depend on the specific estimates of $\theta_1$ and
$\theta_2$ used to produce the forecasts.

When the forecaster will use one of the models to make a number of
predictions (call it $Q$) in the future, the quantity of interest
becomes
\begin{equation*}
  \E_T \oosB = Q^{-1} \sum_{t=T+1}^{T+Q} \E_T D_t,
\end{equation*}
where $D_t$ is defined as before,
\begin{equation*}
  D_t = L(y_{t+\h} - x_{1t}'\bh{1t}) - L(y_{t+\h} - x_{2t}'\bh{2t}),
\end{equation*}
but now uses the full-sample estimates of the models' parameters,
\begin{equation*}
  \bh{it} = \Big(\sum_{s=1}^{T-\h} x_{is}x_{is}'\Big)^{-1} \sum_{s=1}^{T-\h}
  x_{is} y_{s+\h}, \qquad \text{for } i=1,2;\ t = T+1,\dots, T + Q.
\end{equation*}
If $\E_T \oosB$ is positive, the second model is expected to forecast
better than the first over the next $Q$ periods, and if $\E_T \oosB$
is negative then the first model is better. We use a conditional
expectation because the coefficient estimates in $\oosB$ are
stochastic but known in period $T$, and their values will determine
the performance of the two models.

Under conventional fixed-$K$ asymptotic theory, $\E_T \oosB$ would
converge in probability to the difference in the expected loss
associated with the pseudotrue models,%
\footnote{This statement is subject to the usual assumptions: some
  form of stationarity, bounded moments, and weak dependence.} %
\begin{equation}\label{eq:22}
  \E L(y_{t+\h} - x_{1t}'\theta_{1}) - \E L(y_{t+\h} - x_{2t}'\theta_{2}).
\end{equation}
But if $K_2$ increases with $T$ these quantities can have different
limits. For a simple example, assume squared-error loss, let $x_{1,t}
= 1$ for all $t$, and let $(y_{t+\h},x_{2,t})$ be i.i.d. $N(0,
\Sigma)$.  Then the difference between $\E_T \oosB$ and
the in quantity~\eqref{eq:22} is
\begin{align*}
  \E_T \oosB - \big(\E L(y_{t+\h} - x_{1t}'\theta_{1}&) - \E L(y_{t+\h} - x_{2t}'\theta_{2})\big) \\
  &= \big(\E_T (y_{T+\h+1} - \bh{1,T})^2 -
     \E_T (y_{T+\h+1} - x_{T+1}'\bh{2,T})^2\big) \\
     &\quad - \big(\E y_{T+\h+1}^2 - \E (y_{T+\h+1} - x_{T+1}'\theta_2)^2 \big)\\
  &= (\bh{2,t} - \theta_2)' \var(x_{2,t})\, (\bh{2,t} - \theta_2) + o_p(1).
\end{align*}
This last term has expectation equal to $\var(y_T) \frac{K_2}{T - K_2 - 1}$ and
would converge to zero in probability if $K_2$ were fixed, but does not when $\lim
K_2 / T > 0$. In Section~\ref{sec:oostheory} we show that the \oost\
statistic can estimate $\E_T \oosB$ under our increasing $K$
asymptotics and does not estimate the expected loss associated with
the pseudotrue coefficients.

The conditional expectation $\E_T \oosB$ has been studied heavily in
cross-sectional settings with independent observations. In such a
setting, $\E_T \oosB$ is equal to the difference in the models'
\emph{generalization error}, which has been used widely as a measure
of model accuracy in the machine
learning literature \citep[see][for further discussion]{HTF:08}.
Moreover, with i.i.d. observations, the expectation of $\E_T \oosB$
equals \citepos{Aka:69} Final Prediction Error (FPE). Both
generalization error and FPE are defined by a model's
performance on a new, independent, data set, but, for lack of a better
term, we will call $\E_T \oosB$ the ``difference in generalization error''
for the rest of the paper with hopefully no risk of confusion.

Finally, define the following notation.  The $l_v$-norm for vectors in
$\Re^p$ (with $p$ arbitrary) is denoted $\lvert \cdot \rvert_v$, and
the $L_v$-norm for $L_v$-integrable random variables is $\lVert \cdot
\rVert_v$.  The functions $\eigen_i(\cdot)$ take a square-matrix
argument and return its $i$th eigenvalue (with $\eigen_{i}(A) \leq
\eigen_{i+1}(A)$ for any matrix $A$).  All limits are taken as $T \to
\infty$ unless stated otherwise.

\subsection{Assumptions}
\label{sec:asmp}

The next conditions are assumed to hold throughout the paper.  The first
assumption controls the dependence of the underlying random array.
The second lays out the details of our asymptotic approximation.
The third assumption controls the
smoothness of the loss function and bounds the moments of the
difference in the models' performance; the fourth assumption describes
the behavior of the estimation and test windows.  And the last
assumption describes the kernel used to estimate the OOS average's
asymptotic variance.

\begin{asmp}\label{asmp-1}
  The random array $\{y_t,x_t\}$ is stationary and absolutely regular
  with coefficients $\beta_j$ of size $-\rho/(\rho-2)$; $\rho$ is
  greater than two and discussed further in Assumption \ref{asmp-3}.
\end{asmp}

This assumption is a standard condition on the dependence of the
underlying stochastic array. The only novelty is that we use
absolute regularity instead of strong or uniform mixing as our
weak dependence condition; absolute regularity admits a particular
coupling argument, \emph{Berbee's Lemma} \citep[reproduced in this
paper as Lemma A.1 for reference]{Ber:79} that is
unavailable for strong mixing sequences. Absolute regularity
implies uniform mixing but is more restrictive than strong mixing,
so this assumption is not unduly strong.
For a detailed discussion of these weak dependence conditions,
please see \citet{Dav:94} or \citet{Dou:94}.

Our strict stationarity assumption is also somewhat stronger than is
typically used; \citet{Wes:96} and \citet{Mcc:07}, for example,
present results assuming covariance stationarity of the loss
associated with the pseudotrue models. We need to make a stronger
assumption because we will need to prove asymptotic results when the
$\bh{it}$ remain random---so we would need covariance stationarity to
hold for almost all estimates of $\theta_i$ and not just for the
pseudotrue value. The only way to guarantee that condition is to
assume strict stationarity for the underlying stochastic processes.

The next assumption describes our asymptotic experiment.
\begin{asmp}\label{asmp-2}
  The number of regressors for each model, $K_1$ and $K_2$, are less
  than $R$ and $(K_2-K_0)/T$ is uniformly positive;
  $K_0$ is the number of regressors shared by the two models ($(K_1 - K_0)/T$
  may be uniformly positive as well, but is not required to be).

  The variance of $y_{t+\h}$ given $\mathcal{F}_t$ is uniformly
  positive and finite and all of the eigenvalues of the covariance
  matrix of $x_t$ are uniformly positive and finite as well.
  Moreover,
  \begin{equation}
    \eigen_{\max}(X_{iS}'X_{iS}) = O_{L_3}(S),
  \end{equation}
  \begin{equation}
    \eigen_{\max}((X_{iS}'X_{iS})^{-1}) = O_{L_3}(1/S),
  \end{equation}
  \begin{multline}\label{eq:28}
    \eigen_{\max}\Bigg(\E\Big(
    \sum_{s,t=U}^{V-\h} \e_{i,s+\h} \e_{i,t+\h} x_{is}x_{it}'
    \mBig
    x_{i1},\dots,x_{i,U-1};
    \sum_{s=U}^{V-\h} x_{is} x_{is}';
    x_{i,V-\h+1},\dots,x_{i,T-\h}
    \Big)\Bigg) \\
    = O_{L_3}(\max(V-U, K_i)),
  \end{multline}
  and
  \begin{multline}\label{eq:8}
    \tr \E\Big(
    \sum_{s,t=U}^{V-\h} \e_{i,s+\h} \e_{i,t+\h} x_{is}x_{it}'
    \mBig
    x_{i1},\dots,x_{i,U-1};
    \sum_{s=U}^{V-\h} x_{is} x_{is}';
    x_{i,V-\h+1},\dots,x_{i,T-\h}
    \Big)\Bigg) \\
    = O_{L_3}((V-U) \times K_i)
  \end{multline}
  for large enough $T$, where $S = R,\dots,T$, $1 \leq U \leq V - \h
  \leq T - \h$, $i = 1,2$,
  \[ X_{iS} \equiv [x_{i1} \quad \dots \quad x_{i,S-\h}]' \qquad
  \text{and} \qquad \ep{iS} = (\e_{i,1+\h}, \dots, \e_{i,S})'.\]

  Additionally, the Euclidean norms of the pseudotrue coefficients,
  $\theta_1$ and $\theta_2$, satisfy $|\theta_1|_2 = O(1)$ and
  $|\theta_2|_2 = O(1)$.
\end{asmp}

The assumption on $K_1$ and $K_2$ is crucial to the paper; we assume
that the model complexity grows with $T$ fast enough to break
consistency. This assumption is how we derive an asymptotic
concept of ``overfit.''

The assumption that $y_{t+\h}$ and $x_t$ have positive and finite
variance is straightforward. The conditions on the eigenvalues are
technical and control the behavior of the OLS estimator as the number
of regressors gets large---the third and fourth assumptions are
nonstandard but can be easily verified under, for example,
independence. Section~\ref{sec:example} contains such an example. The
restrictions on the pseudotrue coefficients ensure that the regression
model doesn't dominate the variance of $y_{t+\h}$ in the limit.

The next assumption establishes moment conditions for the OOS loss
process and smoothness conditions for the loss function itself. The
moment conditions are standard and apply to $D_t$, and the smoothness
conditions are relatively weak.

\begin{asmp}\label{asmp-3}
  The loss function $L$ is continuous, has finite left and right
  derivatives, and $L(0) = 0$.  There is a constant $B_L$ and a
  function $L'$ that bounds the left and right derivative of $L$ at
  every point such that $\|D_t\|_\rho \leq B_L$; $\|D_t^*\|_\rho \leq
  B_L$ for all $t$, where
  \begin{equation}
    D_t^* = L(y^* - x_1^{*\prime}\hat\theta_{1t})
    - L(y^* - x_2^{*\prime}\hat\theta_{2t})
  \end{equation}
  and $(y^*, x_1^*, x_2^*)$ equals $(y_t, x_{1t}, x_{2t})$ in
  distribution but is independent of $\mathcal{F}_T$ ($\rho$ is defined
  in Assumption~\ref{asmp-1}); and
  \begin{equation}\label{eq:29}
    \| L'(y^* - x_{i}^{*\prime} (\alpha \bh{iR} + (1-\alpha) \bh{iT})) \|_2
    \leq B_L
  \end{equation}
  for any $\alpha \in [0,1]$.
\end{asmp}

The differentiability condition in Assumption~\ref{asmp-3} is weak and
allows the loss function itself to be non-differentiable; for example,
absolute error and many asymmetric loss functions satisfy this
assumption. The assumption makes use of both $(y_{t+\h}, x_{1t},
x_{2t})$ and $(y^*, x_1^*, x_2^*)$ because the period $t$ observations
can be dependent on $\bh{iT}$ in complicated ways. When the underlying
observations are independent these assumptions can simplify
considerably.

The next assumption controls the growth of the test and
training samples.
\begin{asmp} \label{asmp-4} (a) $P, R, Q \to\infty$ as $T \to
  \infty$. (b) $P^2/T \to 0$ and $P/Q \to 0$ as $T \to \infty$.
\end{asmp}

The requirements that $P$ and $R$ grow with $T$ are common. Parts of
the assumption are new, in particular the requirement that $P^2/T \to
0$.  See Lemma~\ref{res-convergence} for a discussion of its
implications.  In practical terms, this assumption requires that the
test sample be large and that the training sample be much larger, by
enough that including or excluding the test sample does not affect the
estimates of $\theta_1$ or $\theta_2$ very much.

A final assumption restricts the class of variance estimators we will
consider.  We use the same class of estimators studied by
\citet{JoD:00} (their class $\mathcal{K}$); see
their paper for further discussion.

\begin{asmp}
  \label{asmp-5} $W$ is a kernel from
$\Re$ to $[-1,1]$ such that $W(0) = 1$, $W(x) = W(-x)$ for all $x$,
\begin{equation}
  \int_{-\infty}^{\infty} \lvert W(x) \rvert dx < \infty, \quad
  \int_{-\infty}^{\infty} \lvert \psi(x) \rvert dx < \infty
\end{equation}
with
\begin{equation}
  \psi(x) = \frac1{\sqrt{2\pi}} \int_{-\infty}^{\infty} W(z) e^{ixz}dz,
\end{equation}
and $W(\cdot)$ is continuous at zero and all but a finite number of
points.
\end{asmp}

These assumptions are broadly similar to those existing in the
literature, with some differences in our assumptions relating the
estimated coefficients to future values of the DGP.
Section~\ref{sec:example} contains an extended example that shows how
these assumptions are satisfied in a simple setting.

\section{Theoretical results}
\label{sec:theory}

This section lays out our theoretical results. The first subsection
presents results for the DMW \oost\ test; we show that it is
asymptotically normal when one or both forecasting models is overfit.
(i.e., under our increasing-$K$ asymptotic approximation.) We also
present a new limitation on these statistics---OOS comparisons heavily
penalize overfit models unless the size of the test sample is a very
small proportion of the total sample size, which will not be practical
in much applied research. The second subsection presents results for
full-sample statistics and shows that widely used test statistics and
model selection criteria are misleading when choosing a model for
forecasting. In contrast to OOS comparisons, these full-sample
criteria choose overfit models too often, even when they are less
accurate than simple benchmark models. These results are somewhat
abstract, so the third subsection works through an example DGP in
detail.

\subsection{Asymptotic normality of the DMW test}
\label{sec:oostheory}

This section has two main conceptual results. The first,
Lemma~\ref{res-mixingale}, shows that the OOS average, $\oosA$, is
asymptotically normal as the size of the test sample grows, even when
the models are overfit. But $\oosA$ is centered at $\E_R \oosA$, the
difference in the generalization error of the models estimated over
the \emph{training sample}, which is not the quantity of interest to
forecasters. Forecasters will generally want to estimate or test
hypotheses about the difference in the generalization error of the
models estimated with the full sample, $\E_T \oosB$. Our second
result, Lemma~\ref{res-convergence}, shows that these quantities are
approximately equal only when the test sample is very small relative
to the total sample size. In particular, we show that $\oosA$ is a
consistent estimator of $\E_T \oosB$ when $P/T \to 0$ and is
asymptotically normal with mean $\E_T \oosB$ when $P^2/T \to 0$. After
establishing these Lemmas, we then show that the \oost\ test is
asymptotically standard normal and can be used to test hypotheses
about $\E_T \oosB$, which requires the two Lemmas as well as a
consistent estimator of the variance of the OOS average.

In the first result, we show that
$\sqrt{P} (\oosA - \E_R \oosA)$ is asymptotically normal as $P \to
\infty$. This application of the CLT is complicated by a hidden source
of dependence---the training sample estimators $\bh{iR}$. Under
conventional asymptotic theory, we would replace each $\bh{iR}$ with
its pseudotrue value $\theta_i$ and apply the CLT to $\sqrt{P}
L(y_{t+\h} - x_{it}'\theta_i)$, with some complications potentially
arising from the replacement (as in \citealp{Wes:96},
\citealp{ClM:01}, or \citealp{Mcc:07}, for example). But we can not
make that replacement here, because our asymptotic approximation
prevents $x_{it}'\bh{iR}$ from converging to $x_{it}'\theta_i$.

Instead, we show that $D_t - \E_R D_t$ is an $L_2$-mixingale that
satisfies the CLT. Mixingales satisfy a weak-dependence condition
similar to MDSes,%
\footnote{An array $Z_{n,t}$ and an increasing sequence of \sfields
  $\Gs_{n,t}$ is an \emph{$L_2$-mixingale of size $-1/2$} if there is
  an array of constants $\{c_{n,t}\}$ and a sequence of constants
  $\zeta_l = O(l^{-1/2 - \delta})$ for some $\delta > 0$ such that
  \begin{equation*}
    \lVert \E(Z_{n,t} \mid \Gs_{n,t-l}) \rVert_2
    \leq c_{n,t} \zeta_l
    \qquad\text{and}\qquad
    \lVert Z_{n,t} - \E(Z_{n,t} \mid \Gs_{n,t+l}) \rVert_2
    \leq c_{n,t} \zeta_{l+1}.
  \end{equation*}
  Mixingales were introduced and developed by
  \citet{Mcl:74,Mcl:75,Mcl:75b,Mcl:77}.} %
but they have some limitations. Transformations of mixingales are
typically not themselves mixingales, which means that CLTs for
mixingale processes require additional assumptions to hold beyond the
mixingale property.%
\footnote{See \citet{Jon:97} for an illustration. We will borrow
  heavily from his results in our proofs.} %
This is in contrast to Near Epoch Dependent (NED) processes, which
retain the NED property after transformations. \citep[See chapter 17
of][for further discussion of these properties.]{Dav:94} But the OOS
loss has more structure than most mixingales and behaves like an NED
process in important respects. (The key result here is
Lemma~\ref{lem:a2} in the Appendix.) Lemma~\ref{res-mixingale} presents the
CLT that we use later, and Section~\ref{sec:example} works through an
i.i.d. example in detail. For i.i.d. observations, the OOS loss is an
MDS and is easier to work with.

\begin{lem}\label{res-mixingale}
  If Assumptions~\ref{asmp-1}--\ref{asmp-3} hold then $\{D_t - \E_R
  D_t, \mathcal F_t\}$ is an $L_2$-mixingale of size $-1/2$.
  Moreover
  \begin{equation}
    \sqrt{P}(\oosA - \E_R \oosA)/\sigma \to^d N(0,1)
  \end{equation}
  as $P \to \infty$ if $\sigma^2$ is uniformly almost surely positive,
  where $\sigma^2 = \var_R(\sqrt{P} \oosA)$ (which is equal to $P
  \E_R(\bar{D}_R - \E_R \bar{D}_R)^2$).
\end{lem}

It may be helpful to compare Lemma~\ref{res-mixingale} to the method
of proof in \citet{GiW:06}. \citet{GiW:06} show that the OOS average
is asymptotically normal when the forecasts are estimated with a fixed
length rolling window. In that case, each $\bh{it}$ depends on only
the most recent $R$ observations and, since $R$ is fixed in their
theory, the forecast errors $y_{t+\h} - x_{it}'\bh{it}$ are themselves
mixing processes. Transformations of their forecast errors are
still obviously mixing processes and obey the CLT.

In our paper, $R$ is not fixed and the forecast errors are not a
convenient weakly-dependent process, since the estimation error in
$\bh{it}$ introduces strong dependence. Consequently,
transformations of the forecast errors are not weakly dependent
either. But this additional dependence has a special form and can be
removed by subtracting the conditional mean; specifically $g(y_{t+\h} -
x_{it}'\bh{it})$ is not weakly dependent but $g(y_{t+\h} -
x_{it}'\bh{it}) - \E_R g(y_{t+\h} - x_{it}'\bh{it})$ is.
Assumptions~\ref{asmp-1}--\ref{asmp-3} allow us to show directly
that $D_t - \E_R D_t$ and (crucially) $D^2_t - \E_R D_t^2$ are both
weakly dependent mixingales, ensuring Lemma~\ref{res-mixingale}.

The next Lemma connects $E_R \oosA$ to $\E_T \oosB$, the difference in
the models' generalization error. Under conventional asymptotics,
these quantities are generally close. But they are not for overfit
models, and the models will tend to forecast better when estimated over the
full sample than over the training sample. Consequently, OOS
comparisons will penalize overfit models too much unless the test
sample is small relative to the total data set.

\begin{lem} \label{res-convergence}
  Under Assumptions \ref{asmp-1}--\ref{asmp-4}a,
  \begin{equation}\label{eq:7}
    \E_T \oosB - \E_R \oosA = O_p(\sqrt{P/T}) + o_p(P^{-1/2})
    + o_p(Q^{-1/2}).
  \end{equation}
\end{lem}
We can view $\E_T \oosB - \E_R \oosA$ as noise introduced by
approximating the performance of the full-sample estimates with that
of the partial-sample estimates. The key term on the RHS
of~\eqref{eq:7} in practice is $O_p(\sqrt{P/T})$---unless $P/T$ is
small, this noise dominates the OOS average, making it an inconsistent
estimator of $\E_T \oosB$. The $o_p(P^{-1/2})$ term is completely
unrestrictive in our applications because we will multiply the OOS
average by at most $\sqrt{P}$ (for the CLT). And the $o_p(Q^{-1/2})$
term has some implications for interpreting these results: we can only
measure the average performance of the forecasting models for an
extended period in the future, long enough that the future
observations are essentially independent of the current
information. For $Q=1$, for example, the dependence between
$y_{T+\h+1}$ and the current information set $\Fs_T$ may be very
strong.

Finally, we can use Lemmas~\ref{res-mixingale}
and~\ref{res-convergence} to show that the DMW test is asymptotically
normal and centered at $\E_T \oosB$, as long as
Assumption~\ref{asmp-4}b holds. This theorem only establishes the
asymptotic distribution, though.

\begin{thm}\label{res-oost}
  Suppose that Assumptions \ref{asmp-1}--\ref{asmp-5} hold (including
  \ref{asmp-4}b), that $\gamma \to \infty$ and $\gamma/P \to 0$ as $T
  \to \infty$, and that $\sigma^2$ is uniformly a.s. positive. Define
  $\sh^2$ to be the usual OOS HAC estimator of the asymptotic variance
  of $\oosA$,
  \begin{equation}
    \sh^2 \equiv P^{-1} \oosSum{s,t}{1} (D_t - \oosA)(D_s - \oosA)
    \vWeight.
  \end{equation}
  Then
  \begin{equation}
    \sqrt{P}(\oosA - \E_T\oosB)/\sh \to^d N(0,1).
  \end{equation}
\end{thm}

The requirement that $\sigma^2$ be uniformly a.s. positive is not
restrictive under our asymptotic theory. Since the models'
coefficients are estimated with uncertainty in the limit, the two
models give different forecasts even if they both nest the DGP.  This
is intuitively similar to \citepos{GiW:06} rolling-window result, but
comes from different asymptotic theory; Giacomini and White keep the
variance of the OOS average positive by letting $P \to \infty$ with
$R$ fixed; in this paper, $R \to \infty$ but the variance remains
positive since $K \to \infty$ (quickly) as well.

When $K_1$ and $K_2$ are fixed, West's (1996) and McCracken's (2007)
asymptotic theories apply and the \oost\ test remains normal as long
as $P/T \to 0$, even if $\sigma^2 \to 0$. Under that asymptotic
approximation, the variance will converge to zero whenever the true
DGP is nested in both of the forecasting models. The same principles
may apply in our setting if $K_1$ and $K_2$ grow slowly, so $K_2/T \to
0$. It is likely that a second order expansion along the lines of
\citet{Mcc:07} would lead to asymptotic normality for that
intermediate case, but we leave that issue to future research.

Finally, Theorem~\ref{res:oostest} establishes that the DMW test can be
use to construct confidence intervals for $\E_T \oosB$ and test
hypotheses about $\E_T \oosB$. In particular, forecasters will often
want to test the null hypothesis that $\E_T \oosB \leq 0$, meaning
that the benchmark model is expected to be more accurate than the
alternative model in the future.

\begin{thm}\label{res:oostest}
  Suppose that the conditions of Theorem
  \ref{res-oost} hold. Then each of the usual Gaussian
  confidence intervals,
  \begin{gather}
    [\oosA - z_{\alpha/2} \, \sh /
    \sqrt{P}, \oosA + z_{\alpha/2} \sh / \sqrt{P}],\label{interval-twosided} \\
    [\oosA - z_{\alpha} \, \sh / \sqrt{P}, +\infty), \label{interval-greater}
    \intertext{and}
    (-\infty, \oosA + z_{\alpha} \, \sh / \sqrt{P}],
  \end{gather}
  contains $\E_T\oosB$ with probability $\alpha$ in the limit, with
  $z_{\alpha}$ the $1-\alpha$ quantile of the standard normal
  distribution. If, in addition, $\lim \Pr[\E_T \oosB \leq 0]$ is
  positive then
  \begin{equation}\label{eq:23}
    \lim \Pr[P^{1/2}\oosA/\sh > z_{\alpha} \mid \E_T
    \oosB \leq 0] \leq \alpha.
  \end{equation}
\end{thm}
\noindent
The results for confidence intervals in Theorem~\ref{res:oostest}
follow immediately from Theorem~\ref{res-oost},
but~\eqref{eq:23} requires additional steps.

\subsection{Failure of in-sample statistics for forecast evaluation}
\label{sec:insample}

In this subsection we look at the behavior of some full-sample
statistics under the same asymptotic theory as before.  We show that
these statistics, which include common tests such as the Wald test as
well as model selection criteria such as the AIC, do not measure the
models' generalization error and consequently do not indicate which
model will be more accurate in the future. Some of these statistics
will tend to choose the larger model regardless of which model will be
more accurate in the future, whereas others tend to choose the smaller
model.  This is a different issue than whether or not full-sample
tests are valid for testing hypotheses about the pseudotrue
coefficients of the models; as \citet{Cal:11c} and \citet{Ana:12}
demonstrate, variations of the Wald test can be valid for those
hypotheses under increasing-$K$ asymptotics.%
\footnote{\citet{Ana:12} shows that the Wald test is invalid in
  general and gives an adjustment that corrects the critical values;
  he also shows that the $F$-test is asymptotically valid under
  certain conditions on the distribution of the regressors.
  \citet{Cal:11c} shows that the $F$-test is asymptotically invalid
  without Anatolyev's constraint, even under homoskedasticity, and
  provides a correction that gives valid tests.  Both papers only
  consider independent observations.} %
To simplify the presentation and the intuition we only derive results
for nested models and for MSE loss, but the conclusions hold much more
generally.%
\footnote{Also, define $M_1 = [I_{K_1} \ 0_{K_1 \times (K_2 - K_1)}]$
  and $M_2 = [0_{(K_2 - K_1) \times K_2} \ I_{K_2 - K_1}]$ as the
  selection matrices that return the first $K_1$ and the last $K_2 -
  K_1$ elements of $\theta_2$, and assume that the regressors are
  ordered so that the first $K_1$ elements of $x_{2,t}$ correspond to
  $x_{1,t}$.} %

The full-sample statistics we study in this paper share a common
property. They choose the alternative model when the distance between
a subset of their coefficient estimates and the origin exceeds a
threshold, and choose the benchmark otherwise. For the Wald test, for
example, this threshold is chosen so that the test has correct size
when those coefficients are zero in population. For small models that
can be consistently estimated, these coefficient estimates are close
to their true values and so this criterion can be a reasonable proxy
for the relative accuracy of the larger model.

But for overfit models, the estimates will typically be far from both
their pseudotrue values and also from zero. In that case, the Wald test and
the AIC will both tend to choose the larger model even when it is less
accurate than the smaller benchmark model. This phenomenon is driven
by the dimensionality of the alternative model, since there are more
potential values of the coefficient estimator that are far from zero
when it has many elements. The threshold for the Wald test and the AIC
is set by construction to be a bounded distance from the origin, and
every other statistic that shares this property has the same behavior
and can reject the benchmark model with high probability, even when
it is more accurate. This behavior is
formalized in Theorem~\ref{res:insample1}.

\begin{thm}\label{res:insample1}
  Suppose Assumptions~\ref{asmp-1}--\ref{asmp-4} hold and let
  $\Lambda$ be a model selection statistic that takes the values zero
  or one; $\Lambda = 0$ indicates that the benchmark model is chosen
  and $\Lambda = 1$ indicates the alternative.  Moreover, assume that
  $L(e) = e^2$ and that there exist deterministic scalars $c$ and
  $\delta$ and a sequence of matrices $V_T$ such that
  \begin{equation}\label{eq:25}
    \limsup_{T \to \infty}
    \Pr[\Lambda \geq \1\{\bh{2T}'M_2' V_T M_2\bh{2T} > c \}]
    \leq \delta
  \end{equation}
  where the eigenvalues of $V_T$ are uniformly bounded, and $\lim
  \rank(V_T)/T > 0$.

  Then there exist DGPs satisfying these assumptions such that
  \begin{equation}\label{eq:11}
    \liminf_{T \to \infty} \E( \Lambda \mid \E_T \oosB \leq 0 )
    \to^p \gamma > 1/2 - \delta.
  \end{equation}
\end{thm}

As we state above,~\eqref{eq:11} means that, with high probability,
the benchmark model is rejected even when it is more accurate. In
general, $\delta$ can be made arbitrarily small by increasing the
region enclosed by $\1\{\bh{2T}'M_2' V_T M_2\bh{2T}\}$ and DGPs can be
chosen to put $\gamma$ arbitrarily close to 1 while still satisfying
the assumptions of the Theorem.  The DGPs and
statistics used in Theorem~\ref{res:insample1} are simple and
common. They include correctly specified linear models with normal and
homoskedastic errors; and the statistics that meet the restrictions on
$\Lambda$ include the $F$-test and AIC.

For example, the $F$-statistic to test the null hypothesis that the
benchmark model is correctly specified is known to have the form
\begin{equation*}
  F = \frac{\bh{2T}' M_2' ( M_2 (X_{2T}'X_{2T})^{-1} M_2' )^{-1} M_2
    \bh{2T}}{s^2 (K_2 - K_1)}
\end{equation*}
where $s^2$ is the usual estimator of the variance of the regression
error. Then let $c$ be any number greater than 1 and define
\begin{equation*}
  V_T = M_2' \big( M_2 \big(\tfrac{1}{s^2 (K_2 - K_1)} X_{2T}'X_{2T}\big)^{-1}
  M_2' \big)^{-1} M_2'.
\end{equation*}
If Assumption~\ref{asmp-2} holds and $s^2$ is consistent then $V_T$
has uniformly bounded eigenvalues and has rank $K_2 - K_1$, so it
satisfies~\eqref{eq:25}.%
\footnote{If it is surprising that these conditions hold for any $c >
  1$, remember that the $F$ random variable converges to 1 in
  probability as both degrees of freedom diverge to $\infty$ since its
  numerator and denominator both obey the LLN.} %
Robust variations of the Wald test can obviously satisfy~\eqref{eq:25}
for similar reasons, and the AIC for nested linear models is
equivalent to using the $F$-test with the critical value %
$(e^{2 (K_2 - K_1) / T} - 1) \cdot (T - K_2) / (K_2 - K_1)$, %
which converges to a finite limit, so the AIC satisfies~\eqref{eq:25}
as well.

For statistics that don't satisfy~\eqref{eq:25}, the behavior can be
quite different. The BIC, for example, can also be written in terms of
the $F$-statistic and is equivalent to using the critical value $(T \,
e^{2 (K_2 - K_1) / T} - 1) \cdot (T - K_2) / (K_2 - K_1)$. This
critical value diverges as $T \to \infty$, ensuring that~\eqref{eq:25}
fails for any $c$. For this statistic, we have the opposite problem as
before: when the alternative model is \emph{more} accurate, the
coefficient estimates of the larger model are contained in a bounded
region of the parameter space. Since the acceptance region of the BIC
grows, it eventually contains \emph{any} bounded region of the
parameter space. For large enough $T$, the BIC will always choose the
\emph{smaller model}, even when the larger model is more accurate.

This behavior is formalized in Theorem~\ref{res:insample2}.
\begin{thm}\label{res:insample2}
  Suppose Assumptions~\ref{asmp-1}--\ref{asmp-4} hold, let $L(e) =
  e^2$, and let $\Lambda$ be a model selection statistic as in
  Theorem~\ref{res:insample1}. Also assume that, for any finite scalar
  $c$,
  \begin{equation}\label{eq:27}
    \Pr[\Lambda \leq \1\{\bh{2T}'M_2' V_T M_2\bh{2T} > c \}] \to 1
    \quad \text{as } T \to \infty,
  \end{equation}
  where $V_T$ is a sequence of deterministic matrices with uniformly
  bounded eigenvalues and $\lim \rank(V_T)/T > 0$. Then there exist
  DGPs satisfying these assumptions such that
  \begin{equation}\label{eq:15}
    \E( \Lambda \mid \E_T \oosB \geq 0 ) \to^p 0.
  \end{equation}
\end{thm}

The condition~\label{eq:27} requires that the acceptance region of
$\Lambda$ eventually contains any finite cylindar centered at the
origin. Again,~\eqref{eq:15} implies that statistics like the BIC
will always choose the smaller model for some DGPs, even when the
larger model will give more accurate forecasts. Both models may be
overfit, in that both $K_1/T$ and $K_2/T$ may both be positive in the
limit; the key is that $(K_2 - K_1)/T$ is also positive in the limit.

It is important to remember that previous research, such as
\citet{Cal:11c} and \citet{Ana:12}, does not predict these
results. When $\Lambda$ represents a test statistic, the test may have
correct size for the null hypothesis that the additional coefficients
on the larger model are zero. The results in this subsection are being
driven by the full-sample statistics' behavior when the smaller model
is misspecified but more accurate.

Any statistic that uses the distance of the models' estimated
coefficients from a set point (the origin being the most common) is
poorly suited for choosing between overfit forecasting models. These
models only forecast well when their coefficient estimates are close
to their pseudotrue values, which can be far from the origin or any
other prespecified
point. Depending on the statistic, it can be biased towards choosing
the larger model or the smaller model. Formal in-sample \emph{tests}
will likely be biased towards the larger model, as we show for the
$F$-test and Wald test in Theorem~\ref{res:insample1}.

\subsection{An extended simple example}
\label{sec:example}

This subsection illustrates the previous theoretical results with a
concrete Let $L(e) = e^2$ and $h = 1$. Suppose that the benchmark is
nested in the alternative and $(x_{2t}, \e_{2,t+1}) \sim i.i.d.\
N(0,I)$, and let $y_{t+1} = x_{2t}'\theta_2 + \e_{2,t+1}$
be the DGP. Also assume that $K_2/T \to c_2$ and $K_1 / T \to c_1 <
c_2$.

The first part of this subsection shows how the assumptions in
Section~\ref{sec:asmp} are satisfied and the second part
demonstrates asymptotic normality of the DMW OOS $t$-test. The third
part explicitly shows that $\E_R \oosA$ converges to $\E_T \oosB$ only
when the test sample is small. And the last part demonstrates that the
$F$-test, AIC, and BIC do not indicate which model will be more
accurate in the future, even in this simple example.

\subsubsection*{Fulfillment of Assumptions 1--5}

We will go through the assumptions one by one. The first Assumption is
a condition on the dependence and moments of the process. Since the
DGP is i.i.d. Normal, these conditions are satisfied trivially.

Assumption~\ref{asmp-2} deals with the design matrix. In this example,
we directly assume that $K_2$ and $K_1$ grow at the correct rates so
the the assumption is satisfied. The variance of $y_{t+1}$ given
$\Fs_t$ is simply the unconditional variance of $\e_{2,t+1}$, which is
1; and the eigenvalues of $\E x_{2t} x_{2t}$ all equal 1 as well; and
so they are all uniformly positive and finite as required.

Assumption~\ref{asmp-2} also requires the eigenvalues of $S^{-1}
X_{iS}'X_{iS}$ to be positive and finite, and for the largest
eigenvalue of $S^{-1} X_{iS}'X_{iS}$ to be bounded in $L_3$.  These
results follow from developments in random matrix theory.
\citet{Gem:80} establishes that the largest eigenvalue of
$X_{iS}'X_{iS}$ is of order $S + K_i$ and \citet{Joh:01} shows that it
converges in distribution to the Tracy-Widom law of order 1
\citep{TrW:96}, which has finite 3rd moments. \citet{Sil:85} and
\citet{BFP:98} prove similar results for the smallest eigenvalue.
Assumption~\ref{asmp-2} also requires the largest eigenvalue of
$(X_{iS}'X_{iS})^{-1}$ be bounded in $L_3$; this should follow from a
similar argument, but we are unaware of any papers that explicitly
derive moments for this eigenvalue.

For the conditional expectation of %
$\sum_{s,t=U}^{V-\h} \e_{i,t+\h} \e_{i,s+\h} x_{i,s} x_{i,t}'$,
independence implies that
\begin{equation*}
  \E\Big(
  \sum_{s,t=U}^{V-\h} \e_{i,s+\h} \e_{i,t+\h} x_{is}x_{it}'
  \mBig
  x_{i1},\dots,x_{i,U-1};
  \sum_{s=U}^{V-\h} x_{is} x_{is}';
  x_{i,V-\h+1},\dots,x_{i,T-\h}
  \Big)
  = \sum_{s=U}^{V-\h} x_{is} x_{is}'.
\end{equation*}
The largest eigenvalue of this last matrix is of order $K_i + V - U$,
as discussed, and it has at most $V-\h - U + 1$ nonzero eigenvalues,
ensuring that both~\eqref{eq:28} and~\eqref{eq:8} hold.

Assumption~\ref{asmp-3} restricts the loss function and the realized
OOS loss. In this example, we have
\begin{equation*}
  D_t^* =^d D_t = (e_{2,t+1} + x_{2t}'\theta_2 - x_{1t}'\bh{1t})^2
  - (e_{2,t+1} + x_{2t}'(\theta_2 - \bh{2t}))^2
\end{equation*}
which has bounded $\rho$th moments by construction. The loss function
is differentiable and $L'(e) = 2 e$, so \eqref{eq:29} holds as well.

Finally, Assumption~\ref{asmp-4} deals with the choice of test and
training sample and holds automatically. And Assumption~\ref{asmp-5}
requires the kernel of the HAC variance estimator to be continuous at
zero; it is straightforward to construct an estimator that satisfies
this condition. However, in this section we will use the estimator
\begin{equation*}
  \sh^2 = \frac{1}{n} \sum_{t=R+1}^{T-1} (D_t - \oosA)^2
\end{equation*}
which does not satisfy Assumption~\ref{asmp-5} but nevertheless is
consistent for the conditional variance of $\oosA$ because the
underlying observations are independent.

\subsubsection*{Asymptotic normality of the OOS average, Lemma 1}

This subsection walks through Lemma 1's CLT. Since the observations in
this example are i.i.d., we do not need to use mixingale theory to
derive the results, but the OOS process is still affected by the
estimation error in $\bh{1t}$ and $\bh{2t}$ and has a high degree of
dependence. This dependence makes the OOS process an MDS, and MDS
asymptotic theory replaces mixingale theory in this example.

By construction, we have
\begin{equation*}
  D_t =
  \begin{cases}
    2 \e_{t+1} (x_{2t}'\bh{2R} - x_{1t}'\bh{1R})
    + (x_{2t}'\theta_2 - x_{1t}'\bh{1R})^2
    - (x_{2t}'\theta_2 - x_{2t}'\bh{2R})^2
    & \text{if } t < T \\
    2 \e_{t+1} (x_{2t}'\bh{2T} - x_{1t}'\bh{1T})
    + (x_{2t}'\theta_2 - x_{1t}'\bh{1T})^2
    - (x_{2t}'\theta_2 - x_{2t}'\bh{2T})^2
    & \text{if } t > T
  \end{cases}
\end{equation*}
and so we can explicitly derive the components of
Lemma~\ref{res-mixingale}. First,
\begin{multline*}
  D_t - \E_R D_t
  = 2 \e_{t+1} (x_{2t}'\bh{2R} - x_{1t}'\bh{1R})
  + \Big\{(x_{2t}'\theta_2 - x_{1t}'\bh{1R})^2
          - \E_R (x_{2t}'\theta_2 - x_{1t}'\bh{1R})^2 \Big\} \\
  - \Big\{(x_{2t}'\theta_2 - x_{2t}'\bh{2R})^2
          - \E_R (x_{2t}'\theta_2 - x_{2t}'\bh{2R})^2 \Big\}
\end{multline*}
for $t < T$. Since the underlying observations are i.i.d., $\E_{t-1}
D_t = \E_R D_t$ a.s.\ and consequently $\{D_t - \E_R D_t, \Fs_{t};
t=R+1,\dots,T-1\}$ is an MDS.

Although transformations of $D_t - \E_R D_t$ will obviously not be
MDSes in general, it should be clear that transformations of $D_t$
will be MDSes after subtracting their conditional mean; i.e.
\begin{equation*}
  \{g(D_t) - \E_R g(D_t); \Fs_t; t = R+1,\dots,T-1\}
\end{equation*}
is an MDS as long as
$g(D_t)$ has finite mean, but $g(D_t - \E_R D_t)$ is not. This MDS
result holds because $x_t$ and $y_{t+1}$ are independent of $\bh{1R}$
and $\bh{2R}$, so
\begin{equation*}\begin{split}
  \E_R g(D_t)
  &= \int g\Big((x_1'\bh{1R})^2 - (x_2'\bh{2R})^2
       + 2 y (x'\bh{2R} - x'\bh{1R})\Big) \, f(y, x) \, dx \, dy \\
  &= \E_{t-1} g(D_t)
\end{split}\end{equation*}
a.s., where $f$ is the density of $(y_{t+1}, x_{2t})$ and $x_1$
denotes the first $K_1$ elements of $x$. This result parallels our
results for mixingales in the general case.

As a result, $\oosA$ and $(1/P) \sum_{t=R+1}^{T-1} D_t^2$ both obey
LLNs: $\oosA \to^p \E_R \oosA$ and
\begin{equation*}
  (1/P) \sum_{t=R+1}^{T-1} D_t^2 \to^p \E_R D_T^2.
\end{equation*}
Moreover, these convergence results imply that $\sh^2 - \var_R
\sqrt{P} \oosA \to^p 0$. And so
\begin{equation*}
  \sqrt{P} \oosA/\sh \to N(0,1)
\end{equation*}
by the MDS CLT.

\subsubsection*{Convergence of $\E_R \oosA$ to $\E_T \oosB$, Lemma 2}

This subsection works through Lemma~\ref{res-convergence}. In this
example, the difference between $\E_R \oosA$ and $\E_T \oosB$ equals
\begin{equation*}
  \begin{split}
    \E_R \oosA - \E_T \oosB &= \Big\{(\bh{1R} - \theta_1)'(\bh{1R} -
    \theta_1) -
    (\bh{2R} - \theta_2)'(\bh{2R} - \theta_2)\Big\} \\
    & \quad - \Big\{(\bh{1T} - \theta_1)'(\bh{1T} - \theta_1) -
    (\bh{2T} - \theta_2)'(\bh{2T} - \theta_2)\Big\} \\
    &\begin{split}
      =\ep{T}'\Big\{
      & \tilde{X}_{1R}(X_{1R}'X_{1R})^{-2} \tilde{X}_{1R}'
      - X_{1T}(X_{1T}'X_{1T})^{-2} X_{1T}' \\
      & - \tilde{X}_{2R}(X_{2R}'X_{2R})^{-2} \tilde{X}_{2R}'
      + X_{2T}(X_{2T}'X_{2T})^{-2} X_{2T}'
      \Big\}\ep{T}.
    \end{split}
  \end{split}
\end{equation*}
with $\tilde{X}_{iR}$ the $T \times K_i$ matrix $[X_{iR}'\ 0]'$. This
last term is a quadratic form and the regressors and errors are
assumed to be normal, so we can find the rate that the difference
converges to zero in probability by calculating its first two moments.

The mean difference is
\begin{equation*}
  \begin{split}
  \E(\E_R \oosA - \E_T \oosB)
      &=O\Big(\max_i \E \tr \big\{
        \tilde{X}_{iR}(X_{iR}'X_{iR})^{-2} \tilde{X}_{iR}
        - X_{iT}(X_{iT}'X_{iT})^{-2} X_{iT} \big\} \Big) \\
      &=O\Big(\max_i \tr \big\{\E (X_{1R}'X_{1R})^{-1}
         - \E (X_{1T}'X_{1T})^{-1} \big\}\Big) \\
      &=O( K_1 P / (R-K_1)(T-K_1)) \\
      &=O(P/T)
    \end{split}
\end{equation*}
The first equality follows from the expectation of a quadratic form,
the second from routine manipulations of the trace operator, and the
third from the moments of the inverse Wishart distribution.

Similarly, the variance of the difference is
\begin{align*}
  \var(\E_R & \oosA - \E_T \oosB) \\ &=
  O\Big(\max_i \E\big[\tr((X_{iR}'X_{iR})^{-1} - (X_{iT}'X_{iT})^{-1})\big]^2
  + 2 \tr \E\big((X_{iR}'X_{iR})^{-2} - (X_{iT}'X_{iT})^{-2}\big) \\
  &\qquad - \big[\tr \E((X_{iR}'X_{iR})^{-1} - (X_{iT}'X_{iT})^{-1}) \big]^2 \Big)\\
  &= O(P/T)^2.
\end{align*}
So, in this example, $P^{1/2}(\E_R \oosA - \E_T \oosB) \to^p 0$
if $P^3/T^2 \to 0$, which is slightly weaker than the general
requirement that $P^2/T \to 0$.  If $\lim P^3/T^2 > 0$ the OOS average
still obeys the MDS CLT, but it is not centered correctly at $\E_T
\oosB$.

\subsubsection*{Behavior of the F-test}

This part illustrates the behavior of full-sample statistics in our
simple example. To simplify the presentation even more, assume that
the full-sample design matrix is orthogonal in-sample, not just in
population, so $X_{2T}'X_{2T} = T \cdot I_{K_2 \times K_2}$, and again
let $M_1$ and $M_2$ be the selection matrices for the first $K_1$ and
the last $K_2-K_1$ elements of $\theta_2$. In this example,
\begin{equation*}
  M_2 \bh{2T} \sim
  N(M_2 \theta_2, (1/T) \, I_{K_2-K_1})
\end{equation*}
and, conditional on $\E_T \oosB = 0$, the density of $M_2 \bh{2T}$
concentrates uniformly on the surface of the sphere centered at $M_2
\theta_2$ and passing through the origin:
\begin{equation*}
  (M_2 \bh{2T} - M_2 \theta_2)'(M_2 \bh{2T} - M_2 \theta_2) =
  \theta_2'M_2' M_2 \theta_2.
\end{equation*}
To see intuitively that it must pass through the origin, note that the
two models will give identical forecasts when $\bh{2T} = 0$ since the
regressors are orthogonal. (Identical forecasts obviously have the
same MSE.)

This region is a cylinder in the original space, $\Re^{K_2}$.  We can
also represent the null $\E_T \oosB \leq 0$ by conditioning on $\E_T
\oosB = -d$ for a fixed positive constant $c$. In that case, the
density of $M_2 \bh{2T}$ concentrates on the sphere
\begin{equation*}
  (M_2 \bh{2T} - M_2 \theta_2)'(M_2 \bh{2T} - M_2 \theta_2) =
  \theta_2'M_2' M_2 \theta_2 + d
\end{equation*}
which has the same center but larger radius.

For the $F$-test we have, as before,
\begin{equation*}
  F = \tfrac{T}{s^2 (K_2 - K_1)} \bh{2T}' M_2' M_2 \bh{2T}.
\end{equation*}
We know that $\sqrt{T}(F - 1)$ is asymptotically normal
\citep{Cal:11c} so the test accepts if
\begin{equation}\label{eq:26}
  \tfrac{T}{s^2 (K_2 - K_1)} \bh{2T}' M_2' M_2 \bh{2T}
  \leq 1 + \delta / \sqrt{T},
\end{equation}
where $\delta$ is chosen to determine the size of the test. In other
words, the test accepts if $M_2 \bh{2T}$ falls in the sphere centered
at the origin with radius $s ((K_2 - K_1) / T)^{1/2} +
O_p(1/\sqrt{T})$.

\input{floats/circles}
\begin{figure}
  \centering
  \begin{tabular}{cc}
  \subfloat[]{\circlefigA{1}{2.5}{1.4}{4.5}\label{fig:circleA}} &
  \subfloat[]{\circlefigB{1}{2.5}{1.4}{4.5}\label{fig:circleB}}
  \end{tabular}
  \caption{Graphs indicating the rejection region and region of equal
    generalization error for the models discussed in Section
    \ref{sec:insample}.  The smaller model has no estimated parameters
    and the larger has two coefficients.  The shaded regions in
    Figures (a) and (b) show the rejection region and the acceptance
    region respectively of the full-sample test given $\E_T \oosB \leq
    0$. Here $e_1 = (1,0)$ and $e_2 = (0,1)$ are the first and second
    selection vectors, so the horizontal axes represent changes in the
    first unique element of $\bh{2T}$ and the vertical axes represent
    changes in the second unique element.}
\label{fig:rreject}
\end{figure}

This is perhaps best illustrated with a picture.
Figure~\ref{fig:rreject} plots these quantities when $K_2 - K_1 =
2$. The circle centered at the origin plots the threshold for the
$F$-test; when $M_2 \bh{iT}$ falls outside this circle, the $F$-test
rejects. The circle centered at $M_2 \theta_2$ plots the set of points
for which $\E_T \oosB = 0$. The shaded region in
Figure~\ref{fig:rreject}~(a) plots the rejection region given $\E_T
\oosB \leq 0$ and the shaded region in Figure~\ref{fig:rreject}~(b)
plots the acceptance region given $\E_T \oosB \leq 0$.

In the picture, the unknown coefficients satisfy
\begin{equation*}
\theta_2'M_2'M_2\theta_2 > \var(\e_t)^{1/2} ((K_2 - K_1) / T)^{1/2}
\end{equation*}
so the center of the conditional distribution of $\bh{2T}$ given $\E_T
\oosB \leq 0$ is outside the acceptance region of the $F$-test. When
this is the case, the conditional probability that $\bh{2T}$ falls in
the rejection region is less than 1/2, since $M_2 \bh{2T}$ is
uniformly distributed on the cylindar
\begin{equation*}
  (M_2 \bh{2T} - M_2 \theta_2)'(M_2 \bh{2T} - M_2 \theta_2) =
  \theta_2'M_2' M_2 \theta_2 + d
\end{equation*}
for some positive $d$.

The AIC behaves essentially the same way. For the BIC, the radius of
the cylander centered at the origin increases with $T$ and eventually
encompasses the cylander centered at $M_2 \theta_2$, so it never
selects the alternative once $T$ is large enough.
\section{Monte Carlo}
\label{sec:mc}

This section presents two simulations that investigate the accuracy of
our theory in small samples.
We do several Monte Carlo exercises. The first looks at wheather our
theoretical results for the OOS average are accuarate: whether or not
the OOS average is approximately normal, and whether it is centered on
$\E_R \oosA$, $\E_T \oosB$, or somewhere else entierely. The second
issue is whether or not the \oost\ test is \emph{useful} for
conducting inference about $\E_T \oosB$. Our theoretical results
suggest that it may not be, because we require $P^2/T \to 0$ for
inference about $\E_T \oosB$ to be valid. A highly related issue is
whether other statistics are useful for conducting inference about
$\E_T \oosB$---again, our theoretical results suggest that they are
not.

We use the same DGP for all of these simulations, and it is described
in the next subsection. Results are presented in the subsection after
that. Simulations were conducted in R \citep{Rde:10} using the
\emph{MASS} \citep{VeR:02}, \emph{Matrix} \citep{BM:13}, and
\emph{rlecuyer} \citep{SR:12} and graphs are produced using
\emph{Lattice} \citep{Sar:10}. In this paper, we present results for
the fixed window, but recursive window results are available in a
separate Appendix and are broadly similary.

\subsection{Setup}
\label{sec:simulation-design}

The Monte Carlo experiment is intentionally very simple so that we can
isolate the influence of the models' complexity.  In particular, we do
not include some features that are common in forecasting
environments---serial dependence, heteroskedasticity, and complicated
DGPs. The DGP we use is given by the equation
\begin{equation}\label{eq:6}
  y_t = x_t'\theta + \e_t,\quad \e_t \sim N(0,1),
  \quad t=1,\dots,T.
\end{equation}
The first element of $x_t$ is 1 and the remaining $K_2-1$ elements are
independent Standard Normal.  The benchmark model is
\begin{equation}
  \label{eq:1}
  y_{1t} = \sum_{j=1}^{K_1} x_{jt}\theta_j + \e_t
\end{equation}
and the alternative model is the DGP \eqref{eq:6}.  We let
$(K_1,K_2)$ equal either $(2,3)$ or $(T/20,T/10)$ to study our theory
in its intended application as well as for more parsimonious models.
We let $T$ equal 100, 250, or 500.  We also vary $\theta$, and do
so giving the benchmark and the alternative model comparable weight in
predicting $y_t$.  Specifically, we set
\begin{equation*}
  \theta_j =
\begin{cases} \frac{c}{\sqrt{K_1}} & j = 1,\dots,K_1 \\
\frac{c}{\sqrt{K_2 - K_1}} & j = K_1 + 1,\dots,K_2 \end{cases}
\end{equation*}
with $c$ equal to zero or one.  When $c$ is one, we're more likely to
draw values of $X$ and $Y$ that make the estimated larger model more
accurate than the benchmark, and when $c$ is zero we're unlikely to
draw such values of $X$ and $Y$.  For all of the studies, $L(e) =
e^2$.

To study the accuracy of our theoretical approximations, we first
estmate the coverage probabilities of OOS confidence intervals for
$\E_R \oosA$ and $\E_T \oosB$. For each draw of $X$ and $Y$, we
construct the one-sided OOS interval defined in
Theorem~\ref{res:oostest}:
\begin{equation*}
  [ \oosA - 1.28 \hat{\sigma}, \infty) \quad\text{with}\quad
  \sh^2 = \frac1P \sum_{t=R+1}^T (D_t - \oosA)^{2}
\end{equation*}
for $P = 10,\dots,2T/3$; we then calculate the percentage of
simulations where these intervals contain $\E_R \oosA$ and the
percentage that contain $\E_T \oosB$.  Since the data are i.i.d., both
of these quantities are easy to calculate (see
Section~\ref{sec:example}). For these calculations, we draw 2000
samples for each combination of the design parameters.

Our second set of results studies whether these different OOS and
in-sample statistics are valid for testing the null hypothesis
\begin{equation*}
  H_0:\quad E_T \oosB \leq 0,
\end{equation*}
namely that the benchmark model is expected to be more accurate in the
future than the alternative. Informally, we are interested in whether
the conditional probability
\begin{equation*}
  \Pr[\text{test rejects} \mid E_T \oosB \leq 0] \leq \alpha
\end{equation*}
where $\alpha$ is the nominal size of the test. If this inequality
does not hold, the test statistic is rejecting the benchmark model too
often.

We look at four different statistics---the full-sample $F$-test, the
DMW $t$-test, the OOS $t$-test using McCracken's (2007) critical
values,%
\footnote{These critical values are not published for $K_2-K_1>10$, so
  we do not report them for $K_2 = T/10$.} %
and Clark and West's (2006, 2007) Gaussian out-of-sample statistic.%
\footnote{Clark and West (2006, 2007) derive their statistic using the
  rolling window estimation scheme. Here we use the same statistic,
  but with a fixed window scheme. A supplemental appendix presents
  results for their statistic, using the recursive window.} %
For the $F$-test, we simply test whether the coefficients on the
larger model are nonzero.  For the out-of-sample tests, we conduct a
one-sided test of out-of-sample performance for every value of $P$ as
before. For each of these simulations, we discard draws from the DGP
that violate the null hypothesis $E_T \oosB \leq 0$ and then use the
remaining samples to calculate the conditional probability that each
test rejects. The simulations end when 2000 draws have been retained
for each choice of the design parameters.

\subsection{Results}

We discuss results for the confidence intervals first.
Figures~\ref{fig:interval-R} and~\ref{fig:interval-T} show the
coverage probability of these intervals as a function of $P$ for each
combination of $T$, $K_1$ and $K_2$, and $c$.  The nominal coverage is
90\%.  The gray horizontal line shows the intervals' nominal coverage
probability.  Each panel displays the coverage for a different choice
of design parameters.

Figure~\ref{fig:interval-R} gives the results for $\E_R \oosA$.  The
actual coverage is very close to the nominal coverage except when $P$
is very small.  The poor behavior for small $P$ is unsurprising, as it
simply means that the CLT is a poor approximation when the test sample
is small. The inteval for $T = 100$, $K = T/10$, and $c = 0$ is the
worst, with actual coverage near 85\% for most values of $P/T$. But
the others are much closer to nominal coverage, even for the
parsimonious models ($K_1=1$ and $K_2= 3$) where one might expect
the theoretical results to break down.

Figure~\ref{fig:interval-T} gives the results for $\E_T \oosB$.  In
columns 2 and 4---the overfit models---the coverage is near nominal
coverage for moderately small values of $P$.  As $P$ increases to
$2T/3$, the coverage increases above nominal size; near 100\% for some
DGPs.  With the parsimonious model, the coverage is near nominal
coverage for all $P$ for the one-sided interval with $c=1$, but only
for moderately small $P$ when $c=0$.

The behavior for $K_2 = T/10$ is exactly what our theory predicts.
When $P^2/T$ is small, the coverage is close to nominal levels.  The
behavior as $P$ increases, combined with the results for $\E_R \oosA$,
indicate that typically $\E_T \oosB \geq E_R \oosA$.  Since
\begin{equation*}
  E_{T} \oosB = E_{R} \oosA + (E_{T} \oosB - E_{R} \oosA),
\end{equation*}
and the interval is approximately centered at $\E_R \oosA$, the
difference $\E_{T} \oosB - E_{R} \oosA$ adds a substantial positive
quantity when $P^2/T$ is not near zero, increasing the coverage of the
one-sided interval.

We present the size simulations next, in
Figures~\ref{fig:ftest}--\ref{fig:ttest-power}.  For the OOS tests
We plot each OOS test's conditional rejection probability,
\begin{equation*}
  \Pr[\text{test rejects} \mid E_T \oosB \leq 0],
\end{equation*}
for each combination of $T$, $K_1$, $K_2$, and $c$ as a function of
$P$.  The $F$-test does not depend on $P$, so we calculate and
tabluate it's conditional rejection probability as a single value for
each combination of design parameters.

Table~\ref{fig:ftest} summarizes the simulation results for the
$F$-test. For $c = 0$, the estimated conditional rejection probability
is almost exactly equal to the test's nominal size (10\%), which is
unsurprising.  For $c = 0$, the $F$-test is exact; moreover, the
larger model will almost always be less accurate than the smaller one,
so conditioning on $\E_T \oosB \leq 0$ is almost unrestrictive.  When
$c$ increases, though, the $F$-test overrejects badly---rejecting at
roughly 50\% when $c = 1$ for the parsimonious model and from 70\% to
100\% for the overfit model. This agrees with our
Section~\ref{sec:insample} results and matches results seen in
empirical practice: the $F$-test rejects the benchmark with very high
probability, even though it is, by construction, more accurate than
the alternative model.

Figure~\ref{fig:ttest-size} presents the size estimates for the DMW
\oost\ test.  Again, different panels display results for different
combinations of the design parameters.  Each graph plots the rejection
probability against $P/T$.  For $K/T=10$, the rejection probability
falls as $P/T$ increases, from near nominal size when $P/T$ is small
to zero when $P/T$ is near $2/3$.  Moreover, the rejection probability
falls faster when $T$ is large, as our theory predicts.  When $K=3$,
the rejection probability stays closer to nominal size, but falls with
$P/T$ for $c=0$, under-rejecting by about 5pp when $P/T = 2/3$, and
rises with $P/T$ for $c=1$, overrejecting by about 10pp when
$P/T=2/3$.  For small $P$, the rejection probability is near 10\% for
all simulations (the farthest is $K=T/10$, $c=0$, where the rejection
probability is about 5\%; the other simulations are much closer).

We observe the following patterns.  The
DMW test has close to nominal size when $P$ is small for every
combination of design parameters.  In most cases, the rejection
probability decreases as $P/T$ increases---the exception is for $K_2 =
3$ and $c=1$.  For the large-$K$ simulations, the rejection
probability drops to zero for most of the simulations as $P/T$
increases.  The rejection probability increases with $c$, but the
rejection probability still is near nominal probability for small $P$
with $c=1$.

Clark and West's (2006, 2007) statistic, presented in
Figure~\ref{fig:clarkwest}, behaves quite differently.  For $c=0$ the
test is correctly sized for both the overfit and parsimonious studies,
as we saw for the $F$-test.  When $c=1$, the rejection probability
increases rapidly with $P/T$.  For $K_2=3$, the rejection probability
is near 10\% when $P$ is small but about 40\% when $P/T = 2/3$.  For
$K=T/10$, the rejection probability is even higher and increases with
$T$ as well, from a maximum over 50\% when $T=100$ to a maximum of
nearly 100\% when $T=1000$.

Results using \citepos{Mcc:07} critical values are presented in
Figure~\ref{fig:mccracken} and are similar to those using Clark and
West's test.  For $c=0$ the rejection probability is nearly the test's
nominal size.  For $c=1$, the rejection probability increases with
$P/T$, from close to the nominal size when $P/T$ is small to over 25\%
when $P/T = 2/3$.  Note that all of the simulations use the
parsimonious model.  McCracken's statistic overrejects here by
slightly less than Clark and West's, but still by a substantial
amount. Note that we are unable to plot results for McCracken's
statistic when $K/T = 10$, which is where the brakdown in Clark and
West's test is most pronounced.

Since the DMW test tends to have low rejection probability, the test's
power is a concern.  Figure~\ref{fig:ttest-power} power results for
the DMW test, simulating from \eqref{eq:1} with $c = 1$ or 2 subject
to the constraint that $\E_T \oosB > 0$.%
\footnote{Draws of $X$ and $Y$ with $\E_T \oosB > 0$ are very rare
  when $c=0$, so we do not present results for that value of $c$.} %
Since the other test statistics greatly overreject, we do not present
their power.  For $c=1$, the power is never greater than nominal size
and decreases to zero as $P/T$ increases for the overfit model.  For
$c=2$ the power is better, increasing with $P/T$ at first stretch and
then decreasing as $P/T$ grows beyond approximately 1/4 for the
overfit model.  Larger values of $T$ give a higher peak and greater
power overall, but the power still falls to nearly zero if $P/T$ is
too large (approximately 2/3 in our simulations).  The power with the
parsimonious model is typically quite low but greater than nominal
size for $c = 2$.

Both sets of simulations support our theoretical results.  The first
simulation confirms that the DMW OOS $t$-test is centered at $\E_R
\oosA$ for all choices of $P$ and $R$ and is centered on $\E_T \oosB$
only when $P$ is small.  The second simulation confirms that the DMW
test has correct size for the null hypothesis that $\E_T \oosB \leq 0$
when $P$ is small and that tests designed to test whether the
benchmark is true, like the $F$-test and Clark and West's (2006, 2007)
and McCracken's (2007) OOS tests can reject by much more than their
nominal size when testing the null $\E_T \oosB \leq 0$.  Moreover,
these simulations demonstrate that the restriction that $P$ be small
is binding in practice, as the DMW test under-rejects and has very low
power when $P$ is too large.

\section{Empirical analysis of equity premium predictability}
\label{sec:empirics}

This section presents a study of equity premium predictability based
on \citet{GoW:08}. Goyal and Welch conduct an out of sample analysis
of 18 different variables thought, based on previous research, to
predict the equity premium (calculated as the difference between the
return on the S\&P 500 index and the T-bill rate).%
\footnote{\citet{GoW:08} builds on previous research by \citet{BoH:99}
  and \cite{GoW:03}.} %
Their analysis has two notable features: Goyal and Welch compare different
predictors across the same time periods and frequency as much as
possible, making the accuracy of these predictors directly comparable;
many of the papers that originally proposed these predictors used
different time periods, methods, or observational frequencies so their
results were not directly comparable. And Goyal and Welch compare
these predictors out-of-sample; many of the original studies found
in-sample evidence of equity premium predictability but did not look
at out-of-sample evidence.

Goyal and Welch consider many different models; most of them are very
simple---regression onto a constant and a single stochastic
predictor---but some are more complicated. They find that essentially
none of the models outperform a simple benchmark, the prevailing mean
of the equity premium, and conclude that these variables do not
predict the equity premium. A large literature has subsequently sought
to explain or rebut these results, including several responses to
Goyal and Welch's original working paper published in the same special
issue of \textit{The Review of Financial Studies} as \citet{GoW:08}.%
\footnote{Those papers are \citet{CaT:08}, \citet{Coc:08},
  \citet{BRW:08}, and \citet{LeN:08}.} %

\citet{GoW:08}, as well as \citet{BoH:99}, \citet{LeN:08}, and many
other authors, propose that instability could explain the OOS failure
of these models. However, we have shown in this paper that overfit can
also explain this pattern, significant in-sample results that do not
hold up out of sample. In this section, we explore the extent to which
overfit is a potential concern in this data set and estimate the
expected forecasting performance of the largest model they consider, a
model with 13 regressors, using 81 observations (annual data from 1928
to 2009). The predictors are listed in Table~\ref{tab:equity}; please
see Goyal and Welch's original paper for detailed information about
these variables.%
\footnote{Table~\ref{tab:equity} only lists the variables used in
  \citepos{GoW:08} ``kitchen sink'' model.  Some of the variables that
  they use are excluded from this model either
  because the series are too short or because the variables are linear
  combinations of other variables.} %

\citet{GoW:08} focus primarily on univariate regression models of the
form
\[
r_{t+1} = \beta_0 + \beta_1 x_{it} + \e_{t+1}
\]
using OLS with a recursive window, where $r_{t+1}$ is the equity
premium and $x_{it}$ is one of the predictors listed in
Table~\ref{tab:equity}.
This focus is consistent with much of the rest of the literature. But
it is not clear that this approach---using a large number of
restricted models---is any more reliable than using a single, large
model. Very few papers in the equity-premium predictability literature
explicitly account for the multiplicity of model comparisons%
\footnote{\citet{RaW:06}, \citet{RaZ:12}, and \citet{Cal:13b} are
  exceptions.} %
and most widely-used statistics for multiple comparisons are
derived under the assumption that there is a finite number of
hypotheses or models (as is the case in \citealp{STW:99c},
\citealp{Whi:00}, \citealp{Han:05}, and \citealp{LeR:05}), so there is
little theoretical evidence that they are any more reliable than a
regression model that encompasses all of the smaller models. Moreover,
since \citet{GoW:08} (along with most of the rest of the literature)
want to interpret these univariate regressions as meaningful
statements about the true relationships between the equity premium and
the regressors, omitted variable bias is a serious potential issue.

For example, Table~\ref{tab:gwinsample} presents two full-sample
coefficient estimates for each of the 12 predictors we consider.%
\footnote{For all of our full sample results, we studentize the
  regressors to make it easier to compare coefficient estimates and we
  express the equity premium in basis points.} %
The first column is the estimator of the coefficient on the
variable in the full regression,
\begin{equation*}
  r_{t+1} = \beta_0 + \sum_{i=1}^{12} \beta_{i} x_{it} + \e_{t+1},
\end{equation*}
and the third column is the $p$-value associated with a two-sided
$t$-test that the coefficient is zero in population. The second column
is the estimator of the coefficient from the univariate regression
\begin{equation*}
  r_{t+1} = \alpha_i + \gamma_{i} x_{it} + \e_{t+1},
\end{equation*}
and the fourth column is the $p$-value associated with its $t$-test.%
\footnote{All of the standard errors were calculated using a
  Newey-West kernel with two lags.} %
Some of the coefficient estimates agree, but some do not. The
coefficient estimate for the \emph{default yield spread}, for example,
is substantially and significantly negative in the full model ($-7.79$
with $p$-value $0.042$), but is near zero in the univariate regression
($0.79$ with $p$-value $0.720$). This pair of results implies that the
the default yield spread contains information about the equity premium
but is also correlated with other poor predictors, and that this
additional correlation adds noise to the univariate regression
and masks the true relationship. Similarly, \emph{net equity expansion}
is significant at 10\% in the univariate regression but not in the
full model ($p$-values of $0.060$ and $0.626$ respectively), which is
likely attributable to omitted variable bias. So there is merit to
studying a large model that includes all of the variables.

Results for the full sample estimates of these models are in
Table~\ref{tab:gwinsample}.%
\footnote{Calculations in this section are done in R \citep{Rde:10}
  using the \emph{lmtest} \citep{ZeH:02} and \emph{sandwich}
  \citep{Zei:04} packages.}%
\footnote{We compare the test statistic to critical values from the
  $F$-distribution, which have been shown to be more reliable than
  Chi-squared critical values when there are many regressors
  \citep{Ana:12,Cal:11c}.} %
The last rows of the table list measures of the full-model fit.  The
$p$-value for the test of full model fit is very small (less than
0.01), indicating that some of the coefficients are nonzero in
population and at least one of these predictors is correlated with the
equity premium.%
\footnote{We have done additional analysis to try to identify which
  predictors were correlated with the equity premium, but none of the
  individual regressors were significant after correcting for
  multiplicity. The Bonferroni correction, for example, suggests that
  an individual $p$-value would need to be less than $0.10/12 \approx
  0.0083$ for its corresponding coefficient to be significant at the
  10\% level, but the smallest $p$-value is $0.035$. One can improve
  on the Bonferroni correction, of course, but a comprehensive
  analysis is beyond the scope of this paper.} %
As we argue throughout the paper, this result does not imply that the
model will forecast well.

To determine whether the full model can forecast well, we use the DMW
\oost\ test to compare it to a sample mean benchmark,
\begin{equation*}
r_{t+1} = \mu + \e_{1,t+1}.
\end{equation*}
Both models are estimated by OLS using the fixed-window scheme. We
also present results for a restricted model proposed by
\citet{CaT:08} that imposes that $\hat r_{t+1}$ be non-negative for
each forecast.
To study the effect of the training sample size on the DMW statistic, we
calculate the one-sided confidence interval for $\E_T \oosB$ given
by \eqref{interval-greater} corresponding to the null and alternative
hypotheses
\[ H_0: \quad E_T \oosB \leq 0 \qquad
H_A: \quad E_T \oosB > 0
\]
using the fixed-window scheme for each value of $R$ between 20 and
$T-10$. The standard deviation is estimated using a Newey-West
estimator with $\lfloor P^{1/4}\rfloor$ lags.  For small values of
$R$, the OOS average is expected to underestimate the performance of
the larger model relative to the smaller, but this may not hold in
this particular dataset. For the OOS results, we express the equity
premium in percentage points.

Figure~\ref{fig:empirics1} plots the OLS results and
Figure~\ref{fig:empirics2} imposes \citepos{CaT:08} restriction. The
solid line in each figure shows the OOS average, $\oosA$, and the
shaded region indicates the 95\% one-sided confidence interval implied
by the DMW test.  Negative numbers indicate that the full model has
higher out-of-sample loss.  We can see that the same patterns hold for
both models: the performance difference decreases as $R$ grows, but
the full model is never more accurate.  We also see that the
performance difference decreases suddenly over the period $R=29$ to
$R=34$ (corresponding to the years 1956--1961).
Figure~\ref{fig:empirics3} plots the accuracy of the individual
forecasts (only for the linear models) and shows that this change is
the result of a sudden improvement in the full model.  This change may
indicate instability in the underlying relationship, as proposed by
\citet{GoW:08}.

In summary, we fail to reject the null that the benchmark prevailing
mean model is more accurate than the full model including all of
\citepos{GoW:08} predictors.  This result is consistent with Goyal and
Welch's original analysis.  Unlike Goyal and Welch, we attribute this
result, at least in part, to parameter uncertainty---the full sample
results indicate that there is a true predictive relationship between
some of these variables and the equity premium and the larger model
could predict better than the benchmark with enough data.%
\footnote{\citet{BWB:10} make a similar point about exchange rate
  models, but see also \citet{Chi:10} and \citet{Gia:10}.} %
These also indicate that combination or shrinkage estimators of the
full model have the potential to significantly improve on the benchmark.%
\footnote{See \citet{RaZ:12} for a recent review of this
  literature.} %

\section{Conclusion}
\label{sec:conclusion}

This paper gives a theoretical motivation for using OOS comparisons:
the DMW OOS test allows a forecaster to conduct inference about
the expected future accuracy of his or her models when one or both is
overfit.  We show analytically and through Monte Carlo that standard
full-sample test statistics can not test hypotheses about this
performance.

Our paper also shows that popular test and training sample sizes may
give misleading results if researchers are concerned about overfit.
We show that $P^2/T$ must converge to zero for the DMW test to give
valid inference about the expected forecast accuracy, otherwise the
test measures the accuracy of the estimates constructed using only the
training sample.  In empirical research, $P$ is typically much larger
than this.  Our simulations indicate that using large values of $P$
with the DMW test gives undersized tests with low power, so this
practice may favor simple benchmark models too much.  Existing
corrections, proposed by \citet{ClM:01,ClM:05}, \citet{Mcc:07} and
\citet{ClW:06,ClW:07}, seem to correct too much, though, and reject
too often when the benchmark model is more accurate.

More work remains.  The requirement that $P^2/T$ converge to zero is
limiting, as it implies that in typical macroeconomic datasets, only a
handful of observations should be used for testing.  This requirement
can be relaxed only slightly; $P = O(T^{1/2})$ is required for the
OOS test to have nontrivial power in general, but there are loss
functions and DGPs for which some relaxation is possible.  This
constraint could be mitigated by extending our results to
cross-validation or other resampling strategies, or by constructing
full-sample statistics that allow inference about $\E_T \bar{D}_T$.
It would also be useful to extend our results to other forecasting
models and to explore how stationarity could be relaxed, but such
extensions are less important than improving the available statistics.

\appendix
\section*{Appendix: mathematical details}
\setcounter{section}{1}
% Change lemma style to prepend 'A'
\setcounter{lem}{0}
\renewcommand{\thelem}{A\arabic{lem}}

\subsection*{Supporting results}
The results in this paper rely heavily on a coupling argument for
absolutely regular sequences, Berbee's Lemma \citep{Ber:79}.  Many of
the results of this paper (Lemma \ref{res-mixingale} and
Theorem~\ref{res-oost}) use modifications of existing results
for NED functions of mixing processes by \citet{Jon:97} and
\citet{JoD:00}; this coupling argument is used to explicitly derive
inequalities that arise naturally for NED processes.  Lemma
\ref{lem-basic-coupling} establishes these inequalities, which are
based on a proposition of \citet{MeP:02}.

We present \citeauthor{MeP:02}'s
(\citeyear{MeP:02}) statement of Berbee's Lemma for
the reader's reference.  In the following Lemma, $\beta(X,Y)$ is the
coefficient of absolute regularity:
\begin{equation}
\beta(X,Y) = \sup_{A \in \sigma(Y)} \E \lvert \Pr(A \mid \sigma(X))
  - \Pr(A) \rvert.
\end{equation}

\begin{lem}\label{lem-berbee}\quad

\begin{quotation}\noindent
  Let $X$ and $Y$ be random variables defined on a probability space
  $(\Omega, \mathcal{T}, \Pr)$ with values in a Polish space
  $S$.  Let $\sigma(X)$ be a $\sigma$-algebra generated by $X$ and let
  $U$ be a random variable uniformly distributed on $[0,1]$
  independent of $(X,Y)$.  Then there exists a random variable $Y^{*}$
  measurable with respect to $\sigma(X) \vee \sigma(Y) \vee
  \sigma(U)$, independent of $X$ and distributed as $Y$, and such that
  $\Pr(Y \neq Y^{*}) = \beta(X,Y)$.

  \noindent\citep{MeP:02}
\end{quotation}
\end{lem}

The advantage of this result over coupling arguments that use other
forms of weak dependence is that the difference between the original
variable, $Y$, and the new variable, $Y^{*}$, does not depend on their
dimension.  Similar results for strong mixing sequences depend on the
dimension of $Y$, which makes them unsuitable for this paper.

\begin{lem}\label{lem-extend-mp}
  Suppose that $X$ and $X^*$ are $L_p$-bounded random variables, with
  $p > 2$, that satisfy ${\Pr[X \neq X^*] = c}$.  Then
  \begin{equation}
    \lVert X - X^* \rVert_2 \leq 2^{1/p} (\lVert X \rVert_p + \lVert
    X^* \rVert_p) c^{(p-2)/2p}
  \end{equation}
\end{lem}

The proof is virtually identical to the proof of Proposition 2.3 in
\citet{MeP:02} and is omitted.

\begin{lem}\label{lem-basic-coupling}
  Suppose Assumptions \ref{asmp-1}--\ref{asmp-3} hold.  Then, for any
  T, $s$, $t$, and $u$ with $s < t \leq u$, there exist random
  variables $D_t^*,\dots,D_u^*$ such that
  \begin{equation}\label{eq:coupling1}
    P[(D_{t}^*,\dots,D_u^*) \neq (D_{t},\dots,D_u)] \leq \beta_{t-s}
  \end{equation}
  and
  \begin{equation}\label{eq:coupling2}
    \E(\phi(D_{t}^*,\dots, D_u^*) \mid \mathcal{F}_s ) =
    \int
    \phi(D_{t},\dots,D_u) f(\mathbf{x}, \mathbf{y})\ d\mathbf{x}\ d\mathbf{y}
  \end{equation}
  almost surely for all measurable functions $\phi$ such that the
  expectations are finite, where
  \[ \mathbf{x} = (x_{t}', \dots, x_{u}')', \qquad
  \mathbf{y} = (y_{t+\h},\dots,y_{u+\h})',\] and $f$ is the
  joint density of $(\mathbf{x}, \mathbf{y})$.  Moreover,
 \begin{equation}\label{eq:coupling3}
   \| D_v^* - D_v \|_2 \leq 2^{1+1/\rho} B_L
   \beta_{t-s}^{\rhoExp}, \qquad v = t,\dots,u.
 \end{equation}
\end{lem}

\begin{proof}
  The proof follows as a consequence of Lemmas \ref{lem-berbee} and
  \ref{lem-extend-mp}.  Let $l = u-t$.  For any fixed values of $l$
  and $T$, the sequence of vectors
  \[ V_{t} = (y_{t+\h}, x_{t}', \dots, y_{t+l+\h}, x_{t+l}') \] is
  absolutely regular of size $\rho/(\rho-2)$.  Berbee's Lemma implies
  that there is a random vector $V^*$ that is independent of
  $\mathcal{F}_s$, equal to $V_{t}$ in distribution, and satisfies
  \[\Pr[V^* \neq V_{t}] = \beta_{t-s}.\]

  Now define
  \begin{equation*}
    D_v^* =
    \begin{cases}
      L(y_{v+\h}^* - x_{1v}^{*\prime} \hat{\theta}_{1R}) - L(y_{v+\h}^* -
      x_{2v}^{*\prime} \hat{\theta}_{2R}) & v \leq T \\
      L(y_{v+\h}^* - x_{1v}^{*\prime} \hat{\theta}_{1T}) - L(y_{v+\h}^* -
      x_{2v}^{*\prime} \hat{\theta}_{2T}) & v > T
    \end{cases}
  \end{equation*}
  with $y_{v+\h}^*$ and $x_{iv}^*$ denoting the elements of $V^*$
  corresponding to $y_{v+\h}$ and $x_{iv}$ in $V_{t}$.  Equations
  (\ref{eq:coupling1}) and (\ref{eq:coupling2}) are satisfied by
  construction, and (\ref{eq:coupling3}) follows from Lemma
  \ref{lem-extend-mp}.
\end{proof}

\begin{lem}\label{lem:a2}
  Suppose Assumptions~\ref{asmp-1}--\ref{asmp-3} hold.  Let $b_T$ be a
  sequence of integers such that $b_T \to \infty$ and $b_T = o(P)$ and define
  \begin{equation}
    Z_i = \ZDef.
  \end{equation}
  Then
  \begin{equation}
    \label{eq:10}
    \sum_{i=1}^{\lfloor P/b_T \rfloor} (\E_R Z_i^2 - \E_{R+(i-1) b_T}
    Z_i^2) \to^p 0 \quad\text{ as } P \to \infty.
  \end{equation}

  If the assumptions of Theorem~\ref{res-oost} also hold, $b_T$ is
  restricted further so that $b_T \equiv \lfloor \gamma/\delta
  \rfloor$ for some positive scalar $\delta$, and we define
  \begin{equation}\label{eq:13}
    \kernelB{x} \equiv \kernelBDefn{x},
  \end{equation}
  then
  \begin{equation}
    \label{eq:12}
    \vtSum (Z_{1t} Z_{2t} - \E_R Z_{1t} Z_{2t}) \to^p 0.
  \end{equation}
  where
  \begin{gather}
    Z_{1t} = \varianceTermIIIa,%
    \intertext{and}%
    Z_{2t} = \varianceTermIVb.
  \end{gather}
\end{lem}
\begin{proof}
\newcommand{\UFiltration}[1]{\ensuremath{\mathcal{F}_{(#1)b_{T}+R-P}}}%
The first result,~\eqref{eq:10}, follows a similar argument to Lemma~5
of \citet{Jon:97} and~\eqref{eq:12} to Lemma~A.4 of \citet{JoD:00}.
Since these arguments are similar and our modification is the same for
both, we'll just present the more complicated version,~\eqref{eq:12}.

Note that $\{Z_{1t}^2 P\gamma_T/b_T\}$ and $\{Z_{2t}^2 P\gamma_T/b_T\}$
are uniformly integrable.  As in~\citet[Lemma A.4]{JoD:00}, we can
assume that there is a constant $C$ such that $Z_{1t}$ and $Z_{2t}$
are bounded in absolute value by $C\sqrt{b_T/P\gamma_T}$; uniform
integrability ensures that the difference between the unbounded random
variables and these truncated versions is negligible for large enough
values of $C$.

Let $r = \lfloor 3P/2b_T \rfloor$ and rewrite the summation as
\begin{align*}
  \vtSum \vtIIIsummand &= \vtSumr \vtSuma \vtIIIsummand \\
  &\quad+ \vtSumr \vtSumb \vtIIIsummand \\
  &\quad+ \sum_{t=r b_T - P + R + 1}^{2P+R} \vtIIIsummand \\
  &\equiv \vtSumr (U_i - \E_R U_i) + \vtSumr (U_i' - \E_R U_i') + o_{L_1}(1).
\end{align*}
The proof then holds if we can show that both $U_i$ and $U_i'$ obey
LLNs.  We'll do so by proving that $\{U_i -\E_R U_i,
\UFiltration{2i-1}\}$ and $\{U_i' - \E_R U_i', \UFiltration{2i}\}$ are
$L_2$-mixingales of size $-1/2$ and using the bound $\E(\vtSumr (U_i -
\E_R U_i))^2 = O(\vtSumr c_i^2)$ where the $c_i$ are the mixingale
magnitude indices \citep{Mcl:75}.

For non-negative $m$, we have
\begin{equation*}
U_i - \E_R U_i \in \UFiltration{2i+2m-1},
\end{equation*}
establishing half of the mixingale result trivially.  Now fix $i$ and $m >
0$ and use Lemma \ref{lem-basic-coupling} to define $D_{ts}^*$
for each $t =(2i-2)b_T-P+R+1,\dots,(2i-1)b_T-P+R$ and $s =
\max(t-b_T,R+1),\dots,\min(t+b_T,T-\h)$ such that
\begin{equation*}
 \E_R D_{ts}^* = \E_{(2i-2m-1)b_T+R-P} D_{ts}^* \quad a.s.
\end{equation*}
and
\begin{equation*}
  \lVert D_{ts}^* - D_s \rVert_2 \leq \couplingBound{s - (2i-2m-1)b_T+P}.
\end{equation*}
Also define
\begin{equation*}
  Z_{1t}^* = (P\gamma_T)^{-1/2} \sum_{l=\vttLower}^{\vttUpper}
  (D_{t,t+l}^* - \E_R D_{t,t+l}^*)\ W(l/\gamma_T), \\
\end{equation*}
and
\begin{equation*}
Z_{2t}^* = (P\gamma_T)^{-1/2} \sum_{j=\vttLower}^{\vttUpper}
    (D_{t,t+l}^* - \E_R D_{t,t+l}^*)\ \kernelB{j/\gamma_T}.
\end{equation*}

Now, we have the inequalities
\begin{align*}
  \lVert \E( U_i - \E_R U_i & \mid \UFiltration{2i-2m-1}) \rVert_2
  \\ &\leq
  \vtSuma \lVert \E(Z_{1t}Z_{2t} \mid \UFiltration{2i-2m-1}) - \E_R
  Z_{1t}Z_{2t} \rVert_2 \\
  &= \vtSuma \lVert \E(Z_{1t}Z_{2t} \mid \UFiltration{2i-2m-1}) \\
  &\quad - \E(Z_{1t}^* Z_{2t}^* \mid \UFiltration{2i-2m-1}) \\
  &\quad + \E(Z_{1t}^* Z_{2t}^* \mid \UFiltration{2i-2m-1})
  - \E_R Z_{1t}Z_{2t} \rVert_2 \\
%  \leq \vtSuma (\wall \lVert \wall \E(Z_{1t}Z_{2t} \mid \UFiltration{2i-2m-1}) \\
%  - \E(Z_{1t}^*Z_{2t}^* \mid \UFiltration{2i-2m-1}) \rVert_2\return
%  \\ + \lVert \E_R Z_{1t}Z_{2t} - \E_R Z_{1t}^*Z_{2t}^*
%  \rVert_2)\return \\
  &\leq 2 \vtSuma \lVert Z_{1t} Z_{2t} - Z_{1t}^* Z_{2t}^*
  \rVert_2 \\
  &\leq 2 \vtSuma (\lVert Z_{1t} - Z_{1t}^* \rVert_2 \lVert
  Z_{2t} \rVert_{\infty}
  + \lVert Z_{2t} - Z_{2t}^* \rVert_2 \lVert Z_{1t}^* \rVert_{\infty}) \\
  &\leq \frac{2 C b_T^{1/2}}{(P\gamma_T)^{1/2}} \vtSuma (\lVert Z_{1t} - Z_{1t}^* \rVert_2
  + \lVert Z_{2t} - Z_{2t}^* \rVert_2).
\end{align*}

And we can finish the proof with the following inequalities:
\begin{align*}
  \frac{2 C b_T^{1/2}}{(P \gamma_T)^{1/2}} &\vtSuma \lVert Z_{1t} - Z_{1t}^* \rVert_2 \\& \leq
  \frac{4C b_T^{1/2}}{P\gamma_T} \vtSuma \sum_{l=\vttLower}^{\vttUpper} \lVert
  D_{t+l} - D_{t,t+l}^* \rVert_2 W(l/\gamma_T) \\&
  \leq O\Bigg(\frac{b_T^{1/2}}{P\gamma_T}\Bigg) \vtSuma
  \sum_{l=\vttLower}^{\vttUpper} \couplingBeta{t+l-(2i-2m-1)b_T+P}\\&
  = O\Bigg(\frac{b_T^{1/2}}{P\gamma_T}\Bigg) O(b_T^{3/2-u}\, m^{-1/2-u})
\end{align*}
for some positive $u$.  The same argument holds for $Z_{2t}$.
As a result,
\[
E\Big(\vtSumr (U_i - \E_R U_i)\Big)^2 = o\Big(\vtSumr b_T^2/P\gamma_T\Big) =
 o(b_T/\gamma_T) \to 0,
\]
as required.
\end{proof}

\begin{lem}\label{lem:a6}
  Suppose the conditions of Theorem~\ref{res-oost} hold.
  Then
  \begin{equation}
    \sh^2 - P^{-1} \sum_{s,t=R+1}^{T-h} (D_s - \E_R
    D_s) (D_t - \E_R D_t) W((t-s)/\gamma) \to^{L_1} 0.
  \end{equation}
\end{lem}

\begin{proof}
   It follows from simple algebra that
\begin{multline*}
  \Big| \sh^2 -  P^{-1} \sum_{s,t=R+1}^{T-h} (D_s - \E_R
    D_s) (D_t - \E_R D_t) W((t-s)/\gamma) \Big| \leq\\
  \varianceDiffA \\ + P^{-1} \oosSum{s,t}{1} \lvert (D_s -
  \oosA)(\E_R D_t - \oosA) \rvert \vWeight + o_p(1).
\end{multline*}
We'll prove that these two sums are $o_p(1)$; uniform integrability
then implies convergence in $L_1$.  The arguments for each are almost
identical, so we'll only present the first.

Applying the Cauchy-Schwarz inequality twice and simplifying gives the
upper bound
\begin{multline*}
\varianceDiffA \\ \leq O(1) \Big[\varianceDiffAii\Big]^{1/2} \Big[\varianceDiffAi\Big]^{1/2}.
\end{multline*}
Now,
$P^{-1} \oosSum{s}{1}(D_s - \E_R D_s)^2 = O_p(1)$, and it suffices to prove
that \[\varianceDiffAi = o_p(1).\]  Observe that
\begin{align*}
  \varianceDiffAi & = O_p\Big(P^{-1} \oosSum{s}{1}(\E_{R}D_s -
  \E_R\oosA)^2\Big) + O_p(\oosA - \E_R\oosA)^{2} \\
  &= O_p\Big(P^{-1} \oosSum{s}{1} (\E_RD_s)^2 - (\E_R\oosA)^2\Big) +
  o_p(1),
\end{align*}
with the second term $o_p(1)$ by Lemma \ref{res-mixingale} and
\citepos{Dav:93} mixingale LLN.

Now define $D_s^*$, $s = R+1,\dots,T-\h$, as in Lemma
\ref{lem-basic-coupling} so that $\E_{s-1} D_s^* = \E_R D_s^*$ almost
surely.  Note that we also have the equality $\E_R D_s^* = \E_R
D_{R+1}^*$ almost surely for all $s\geq R+1$, and so
\[
P^{-1} \oosSum{s}{1} (\E_R D_s^*)^2 = \Big(P^{-1} \oosSum{s}{1} \E_R
D_s^*\Big)^2 \quad \text{a.s.}
\]
Consequently,
\begin{align*}
P^{-1} \oosSum{s}{1}& (\E_R D_s)^2 - (\E_R\oosA)^2 \\
&= P^{-1} \oosSum{s}{1} [(\E_R D_s)^2 - (\E_R D_s^*)^2]
 + \Big(P^{-1} \oosSum{s}{1} \E_R D_s^*\Big)^2 - (\E_R\oosA)^2 \quad \text{a.s}.\\
&= O_p\Big(P^{-1} \oosSum{s}{1} [(\E_R D_s)^2 - (\E_R D_s^*)^2]\Big)
 + O_p\Big(P^{-1} \oosSum{s}{1} \E_R (D_s - D_s^*)\Big).
\end{align*}
Finally,
\begin{align*}
\Big\lVert P^{-1} \oosSum{s}{1} [(\E_RD_s)^2 - (\E_R D_s^*)^2] \Big\rVert_1
&\leq P^{-1} \oosSum{s}{1} \lVert \E_R (D_s - D_s^*) \rVert_2 \lVert
\E_R (D_s + D_s^*) \rVert_2
\\ &\leq (4 B_L/P) \oosSum{s}{1} \lVert \E_R (D_s - D_s^*) \rVert_2,
\end{align*}
and this last term vanishes as in the proof of Lemma~\ref{lem:a2},
completing the proof.
\end{proof}

\subsection*{Proof of Lemma \ref{res-mixingale}}
We will start by proving that $\{D_t - \E_R D_t, \Fs_t\}$ is an
$L_2$-mixingale of size $-1/2$; $D_t$ is $\Fs_t$-measurable and so it
suffices to prove that
\begin{equation}\label{eq:24}
  \| \E_{t-l} D_t  - \E_R D_t \|_2 \leq 2^{2 + 1/\rho} B_L \, \couplingBeta{l}
\end{equation}
for $l = 1,\dots,t-R$, since $\couplingBeta{l} = O(l^{-1/2 - \delta})$
for some $\delta > 0$ by assumption. Fix $R$, $t$ and $l$ and use
Lemma \ref{lem-basic-coupling} to define $D_t^*$ so that
\[
\E_R D_t^* = \E_{t-l} D_t^* \quad a.s.
\]
and
\[
\lVert D_t - D_t^* \rVert_2 \leq \couplingBound{l}.
\]
Then
\begin{align*}
\lVert \E_{t-l} D_t - \E_R D_t \rVert_2 & \leq
\lVert \E_{t-l} D_t - \E_{t-l} D_t^* \rVert_2  + \lVert \E_{t-l} D_t^* - \E_R D_t \rVert_2 \\
&\leq 2 \lVert D_t - D_t^* \rVert_2 \\
&\leq 2^{2 + 1/\rho}\ B_L\ \couplingBeta{l}.\tag*{\qed}
\end{align*}

\noindent Asymptotic normality follows from~\eqref{eq:24} using a
modification of \citepos{Jon:97} CLTs for mixingale and NED arrays.
Define
\begin{equation*}
  Z_i = \ZDef
\end{equation*}
where $b_T$ is a sequence that satisfies $b_T\leq P$,
$b_T\to\infty$, and $b_T/P\to 0$.  The same arguments used in
\citepos{Jon:97} Theorem 1 show that
\begin{equation*}
   \SumOuterBlock{i} Z_i = P^{-1/2} \oosSum{s}{1} (D_t - \E_R D_t) + o_p(1).
\end{equation*}
and
\begin{equation*}
  \SumOuterBlock{i} Z_i = \SumOuterBlock{i} (Z_i - E_{R + (i-1)b_T}
  Z_i) + o_p(1).
\end{equation*}
Note that $\{Z_i - E_{R + (i-1)b_T} Z_i,\mathcal{F}_{R + i b_T}\}_i$ is an
MDS by construction, so \citepos{HaH:80} Theorem 3.2 and Corollary
3.1 ensure that $\sigma^{-1} \SumOuterBlock{i} Z_i \to^d N(0,1) $ as
long as
\begin{equation*}
  \sigma^2 - \SumOuterBlock{i} \E_R Z_i^2 \to^p 0,
\end{equation*}
and
\begin{equation*}
  \SumOuterBlock{i} \E_R Z_i^2 - \SumOuterBlock{i} \ZSqCE \to^p 0.%
\footnote{Note that $\sigma^2 \in \mathcal{F}_t$ for
    all $t \geq R$, so Hall and Heyde's condition (3.21) is
    unnecessary---see the remarks after their result.} %
\end{equation*}
The first equation holds as in \citet{Jon:97} (see the proof
of his Theorem 2); the second is ensured by
Lemma~\ref{lem:a2}. \qed

\subsection*{Proof of Lemma \ref{res-convergence}}
\newcommand{\resConvgRHS}[1]{\ensuremath{\E(L(y^* - x_1^{*\prime}\bh{1#1}) - L(y^{*} -
x_2^{*\prime}\bh{2#1}) \mid \bh{#1})}}
\newcommand{\resConvgEstDiff}[1]{\ensuremath{\E(L(y^{*} -
    x_{#1}^{*\prime}\bh{#1R}) \mid \bh{R}) -
\E(L(y^{*} - x_{#1}^{*\prime}\bh{#1T}) \mid \bh{T})}}
\newcommand{\resConvgEstDiffRV}[1]{\ensuremath{L(y^{*} -
    x_{#1}^{*\prime}\bh{#1R}) - L(y^{*} - x_{#1}^{*\prime}\bh{#1T})}}

Equation~\eqref{eq:7} holds if we show
\begin{gather}
\E_R \oosA = \resConvgRHS{R} + o_p(P^{-1/2}),\label{eq:18}\\
\E_T \oosB = \resConvgRHS{T} + o_p(Q^{-1/2}),\label{eq:19}
\intertext{and}
\resConvgEstDiff{i} = O_p(\sqrt{P/T}),\label{eq:20}
\end{gather}
where $\hat\theta_R = (\hat\theta_{1R}, \hat\theta_{2R})$,
$\hat\theta_T = (\hat\theta_{1T}, \hat\theta_{2T})$, and $y^{*}$,
$x_1^{*}$ and $x_2^{*}$ are random variables drawn from the joint
distribution of $(y_{t+\h},x_{1t},x_{2t})$ independently of
$\mathcal{F}_T$.

\begin{proof}[Proof of~\eqref{eq:18} and~\eqref{eq:19}]
For~\eqref{eq:18}, define $D_t^*$ for each $t=R+1,R+2,\dots,T-h$ so
that
\begin{equation*}
  \| D_t^* - D_t \|_2 \leq 2^{(1+\rho)/\rho} B_L \beta_{t-R}^{(\rho-2)/2\rho}
\end{equation*}
and
\begin{equation*}
  \E_R D_t^* = \E(L(y^* - x_1^{*\prime}\hat\theta_{1R}) -
  L(y^* - x_2^{*\prime}\hat\theta_{2R}) \mid \hat\theta_R) \qquad a.s.
\end{equation*}
Lemma~\ref{lem-basic-coupling} ensures that these $D_t^*$ exist.
Now,
\begin{align*}
  \Big\lVert \E_R \oosA - \E\Big(P^{-1} \sum_{t=R+1}^{T-h} D_t^* \mid
  \hat\theta_R\Big) \Big\rVert_2
  &= \Big\lVert \E_R\Big( \oosA - P^{-1} \sum_{t=R+1}^{T-h} D_t^*
  \Big) \Big\rVert_2 \\
  &\leq P^{-1} \sum_{t=R+1}^{T-h} \lVert D_t - D_t^* \rVert_2\\
  &= O(P^{-1}) \sum_{t=R+1}^{T-h} \beta_{t-R}^{(\rho-2)/2\rho}
\end{align*}
and this last term is $o(P^{-1/2})$ by assumption.
Essentially the same argument proves~\eqref{eq:19} as well.
\end{proof}

\begin{proof}[Proof of~\eqref{eq:20}]
Assumption~\ref{asmp-3} and the definition of the OLS estimator
ensure that
\begin{align*}
  \|L(y^{*} -  x_i^{*\prime} \bh{iR}) - L(y^{*} - x_i^{*\prime} \bh{iT}) \|_1
  &\leq B_{L} \| x_i^{*\prime} (\bh{iR} - \bh{iT}) \|_2 \\
  &\leq B_{L} \, \| x_i^{*\prime} [(X_{iT}'X_{iT})^{-1} -
  (X_{iR}'X_{iR})^{-1}] X_{iR}' \eb_{iR} \|_2 \\
  &\quad+ B_L \, \| x_i^{*\prime}
  (X_{iT}'X_{iT})^{-1}[X_{iT}'\eb_{iT} - X_{iR}
  \eb_{iR} ] \|_2 .
\end{align*}

To simplify notation, define $V = (X_{iT}'X_{iT})^{-1} -
(X_{iR}'X_{iR})^{-1}$ and $W = X_{iR}'\eb_{iR} \eb_{iR}'X_{iR}$. The
first term in the upper bound satisfies
\begin{align*}
  \| x_i^{*\prime} V X_{iR}' \eb_{iR} \|_2^2
  &\leq \eigen_{K_i}(\E x_i^* x_i^{*\prime}) \cdot \E \tr\big( V^2 W \big) \\
  &= O(1) \cdot \E \tr\{ V^2 \E(W \mid
  X_{iR}'X_{iR}, x_{i,R-\h+1} x_{i,R-\h+1}',\dots, x_{i,T-\h} x_{i,T-\h}')\}
\end{align*}
by assumption. Observe that
\begin{multline*}
  \E \tr\{ V^2 \E(W \mid X_{iR}'X_{iR}, x_{i,R-\h+1} x_{i,R-\h+1}',\dots, x_{i,T-\h} x_{i,T-\h}')\}
  \\ \leq
  \Big\rVert \sum_{i=1}^{K_i} \eigen_i^2 \big(V\big) \Big\rVert_{3/2}
  \big\rVert \eigen_{K_i}(\E(W \mid
  X_{iR}'X_{iR}, x_{i,R-\h+1} x_{i,R-\h+1}',\dots, x_{i,T-\h} x_{i,T-\h}')) \big\rVert_3.
\end{multline*}
The second term in this product is $O(R)$ by
Assumption~\ref{asmp-2}. To bound the first term, note that
$(X_{iT}'X_{iT})^{-1} - (X_{iR}'X_{iR})^{-1}$ has rank $P$ and
each of its nonzero eigenvalues is bounded in absolute value by the
eigenvalues of $(X_{iR}'X_{iR})^{-1}$. The eigenvalues of
$(X_{iR}'X_{iR})^{-1}$ are $O_{L_3}(1/R)$ by Assumption~\ref{asmp-2}, so
\begin{equation*}
  \Big\|\sum_{j=1}^{K_i}
  \eigen_j^2(V)\Big\|_{3/2}
  \leq \Big\|\sum_{j=K_i - P+1}^{K_i} \eigen_j^2(V) \Big\|_{3/2}
  = O(P/R^2).
\end{equation*}
Consequently,
\begin{equation*}
  \E \tr\bigg\{[(X_{iT}'X_{iT})^{-1} - (X_{iR}'X_{iR})^{-1}]^2
  X_{iR}'\eb_{iR} \eb_{iR}X_{iR} \bigg\} = O(P/R)
\end{equation*}
and so
\begin{equation*}
  \| x_i^{*\prime} [(X_{iT}'X_{iT})^{-1} -
  (X_{iR}'X_{iR})^{-1}] X_{iR}' \eb_{iR} \|_2 = O(\sqrt{P/R}).
\end{equation*}

A similar argument proves that
\begin{equation*}
  \big\| x_i^{*\prime} (X_{iR}'X_{iR})^{-1}
  [X_{iT}'\eb_{iT} - X_{iR} \eb_{iR} ] \big\|_2 =  O(\sqrt{P/R}),
\end{equation*}
completing the proof.
\end{proof}

\subsection*{Proof of Theorem \ref{res-oost}}
We can rewrite the centered \oost\ statistic as
\[
\sqrt{P}(\oosA - \E_T \oosB)/\sh
= \frac{\sigma}{\sh}\, \Big( \sqrt{P}(\oosA - \E_R \oosA)/\sigma
+ \sqrt{P}(\E_R \oosA - \E_T \oosB)/\sigma \Big)
\]
so Lemma \ref{res-mixingale} and \ref{res-convergence} ensure that
this term is asymptotically standard normal as long as $\sigma/\sh
\to^p 1$. Since $\sigma$ is almost surely positive, this convergence
is equivalent to $\sigma^2 - \sh^2 \to^p 0$.

The proof that $\sigma^2 - \sh^2 \to^p 0$ follows \citepos{JoD:00}
Theorem 2.1 closely.  We start by defining similar quantities to
theirs, borrowing their notation when possible to make the
similarities apparent. Let $b_T \equiv \lfloor \gamma/\delta \rfloor$,
define $\kernelB{x}$ as in Equation \eqref{eq:13}, and define the following
terms as in \citet{JoD:00}:
\begin{align*}
  \varianceTermI &\equiv
  \varianceTermIDefn,\\ \varianceTermII &\equiv \vtSum
  \varianceTermIIa\\& \quad \times \varianceTermIIb,\\
  \varianceTermIII &\equiv \vtSum \varianceTermIIIa\\
  &\quad \times \varianceTermIIb,\\
  \varianceTermIV &\equiv \vtSum \varianceTermIIIa \\
  &\quad\times \varianceTermIVb.
\end{align*}
These definitions give the inequalities
\begin{align*}
  \lVert \sh^2 - \sigma^2 \rVert_1 &
  \leq \lVert \sh^2 - \varianceTermI \rVert_1
  + \lVert \varianceTermI - \varianceTermII \rVert_1
  + \lVert \varianceTermII - \varianceTermIII \rVert_1
  + \lVert \varianceTermIII - \varianceTermIV \rVert_1\\ & \quad
  + \lVert \varianceTermIV - \E_R \varianceTermIV \rVert_1
  + \lVert \E_R\varianceTermIII - \E_R\varianceTermIV \rVert_1
  + \lVert \E_R\varianceTermII - \E_R\varianceTermIII \rVert_1\\ & \quad
  + \lVert \E_R\varianceTermI - \E_R\varianceTermII \rVert_1
  + \lVert \E_R \varianceTermI - \sigma^2 \rVert_1
  \\ &
  \leq  \lVert \sh^2 - \varianceTermI \rVert_1
  + 2(\lVert \varianceTermI - \varianceTermII \rVert_1
      + \lVert \varianceTermII - \varianceTermIII \rVert_1
      + \lVert \varianceTermIII - \varianceTermIV \rVert_1) \\
  & \quad + \lVert \varianceTermIV - \E_R \varianceTermIV \rVert_1
  + \lVert \E_R \varianceTermI - \sigma^2 \rVert_1.
\end{align*}
De Jong and Davidson (2000) prove that
\begin{equation*}
\lim_{\delta\to0} \limsup_{T\to\infty} \lVert \varianceTermI -
\varianceTermII\rVert_1 = 0,
\end{equation*}
\begin{equation*}
\lim_{\delta\to0} \limsup_{T\to\infty} \lVert \varianceTermII - \varianceTermIII
\rVert_1 = 0,
\end{equation*}
\begin{equation*}
\lim_{\delta\to0} \limsup_{T\to\infty} \lVert \varianceTermIII - \varianceTermIV
\rVert_1 = 0,
\end{equation*}
and
\begin{equation*}
\lim \lVert \E_R \varianceTermI - \sigma^2 \rVert_1 = 0.
\end{equation*}
Their proofs of these four convergence results use the fact that NED
functions of mixing processes are also mixingale processes and do not
use any other properties specific to NED processes, so their results
hold here as well.  We do need to modify their proofs that
\begin{equation*}
  \lVert \varianceTermIV - \E_R \varianceTermIV \rVert_1 \to 0
\end{equation*}
and
\begin{equation*}
  \lVert \sh^2 - \varianceTermI \rVert_1 \to 0
\end{equation*}
for all positive $\delta$, though, since those proofs exploit NED
properties. These results are presented as Lemmas~\ref{lem:a2}
and~\ref{lem:a6} respectively.  \qed


\subsection*{Proof of Theorem \ref{res:oostest}}
Coverage of the confidence intervals is immediate from
Theorem~\ref{res-oost}, so we will present a proof of~\eqref{eq:23}.
By Lemma~\ref{res-convergence}, we know that $\E_R \oosA - \E_T \oosB
\to^p 0$, so
\begin{equation*}
  \Pr[\E_R \oosA \leq 0] - \Pr[\E_T \oosB \leq 0] \to 0.
\end{equation*}
Now we can use Lemma~\ref{lem-basic-coupling} to define $D_t^*$ as in
the proofs of Lemma~\ref{res-mixingale} and~\ref{res-convergence} so
that $\E_R D_t^* = \E_{t-l} D_t^*$ a.s. and
\[
\lVert D_t - D_t^* \rVert_2 \leq \couplingBound{l}
\]
and define $\oosA^* = P^{-1} \sum_{t=R+1}^{T-\h} D_t^*$. Then
\begin{equation*}
  \lVert \sqrt{P} \oosA - \sqrt{P} \oosA^* \rVert_2
  \leq P^{-1/2} \sum_{t=R+1}^{T-\h} \lVert \oosA - \oosA^* \rVert_2
  = o(1),
\end{equation*}
and, consequently, we have convergence in probability of the following
vectors:
\begin{equation*}
  (\sqrt{P} \oosA^*/\sigma,\ \E_R \oosA) -
  (\sqrt{P} \oosA/\sh,\ \E_T \oosB) \to^p 0,
\end{equation*}
where we are implicitly using consistency of $\sh^2$ for $\sigma^2$.
More importantly, this implies convergence in distribution of these
vectors, so
\begin{equation*}
  \Pr[\sqrt{P} \oosA^* / \sigma > z_\alpha
  \text{ and } \E_R \oosA \leq 0]
  - \Pr[\sqrt{P} \oosA / \sh > z_\alpha \text{ and }
  \E_T \oosB \leq 0] \to 0.
\end{equation*}

For large enough $T$, both $\Pr[\E_R \oosA \leq 0]$ and $\Pr[\E_T
\oosB \leq 0]$ are positive (the second by assumption, the first by
convergence to the second) so (for these $T$)
\begin{align}\label{eq:31}
  \Pr[\sqrt{P} \oosA^* / \sigma > z_\alpha
  \mid \E_R \oosA \leq 0] &=
  \frac{\Pr[\sqrt{P} \oosA^* / \sigma > z_\alpha \text{ and }
    \E_R \oosA \leq 0]}{\Pr[\E_R \oosA \leq 0]}
\intertext{and}\label{eq:32}
  \Pr[\sqrt{P} \oosA / \sh > z_\alpha
  \mid \E_T \oosB \leq 0] &=
  \frac{\Pr[\sqrt{P} \oosA / \sh > z_\alpha \text{ and }
    \E_T \oosB \leq 0]}{\Pr[\E_T \oosB \leq 0]}
\end{align}
almost surely. Since the terms on the RHS of Equations~\eqref{eq:31}
and~\eqref{eq:32} converge to the same limit in probability, we have
\begin{equation*}
  \Pr[\sqrt{P} \oosA / \sh > z_\alpha  \mid \E_T \oosB \leq 0]
  - \Pr[\sqrt{P} \oosA^* / \sigma > z_\alpha \mid \E_R \oosA \leq 0]
  \to^p 0
\end{equation*}
and it suffices to show that
\begin{equation*}
  \plim \Pr[\sqrt{P} \oosA^* / \sigma > z_\alpha \mid \E_R \oosA \leq 0]
  \leq \alpha.
\end{equation*}

To establish this inequality, we have
\begin{align*}
  \Pr[\sqrt{P} \oosA^* / \sigma > z_\alpha \mid \E_R \oosA \leq 0]
  &\leq \Pr[\sqrt{P} (\oosA^* - \E_R \oosA) / \sigma > z_\alpha
  \mid \E_R \oosA \leq 0] \\
  &\leq \E\big(
  \Pr[\sqrt{P} (\oosA^* - \E_R \oosA) / \sigma > z_\alpha \mid \Fs_R]
  \mid \E_R \oosA \leq 0\big).
\end{align*}
By construction,
\begin{equation*}
  \Pr[\sqrt{P} (\oosA^* - \E_R \oosA) / \sigma > z_\alpha \mid \Fs_R]
  \to \alpha
\end{equation*}
since generated pseudo OOS observations in each $D_t^*$ are
independent of $\Fs_R$, completing the proof.
\qed

\subsection*{Proof of Theorem~\ref{res:insample1}}

Let $(\e_{t+h}, x_{t}) \sim i.i.d.\ N(0,I)$ and let $\theta_1 = 0$.
For any $d \geq 0$, the event $\E_T \oosB = -d$ implies that
\begin{equation*}
  \E_T (y_{T+\h+1} - x_{1T+1}'\bh{1T})^2
  = \E_T (y_{T+\h+1} - x_{2T+1}'\bh{2T})^2 - d \quad a.s.
\end{equation*}
which can be expressed as
\begin{equation}\label{eq:34}
  d + \bh{1T}'\bh{1T} - \bh{2T}'M_1'M_1\bh{2T}
  = (\bh{2T} - \theta_2)' M_2'M_2 (\bh{2T} - \theta_2) \quad a.s.
\end{equation}
Now, $M_2 \bh{2T}$ is normally distributed conditional
on $\bh{1T}$ and $M_1 \bh{2T}$, and is distributed on the surface of
the sphere defined by~\eqref{eq:34} conditional on $\bh{1T}$, $M_1
\bh{2T}$, and the event $\E_T \oosB = -d$.

Since $M_2 \bh{2T}$ is normal, this conditional distribution is
invariant to reflection across the axes defined by the eigenvectors of
its covariance matrix. So when $\theta_2$ lies outside the cylindar
that contains the acceptance region of $\Lambda$, ie.e. $\theta_2'M_2'
V_T M_2 \theta_2 > c$, we have
\begin{align*}
  \Pr[ \bh{2T}'M_2' V_T M_2 &\bh{2T} \leq c \mid \E_T \oosB \leq 0] \\
  &= \E\Big( \Pr[ \bh{2T}'M_2' V_T M_2 \bh{2T} \leq c
  \mid \E_T \oosB \leq 0, \bh{1T}, M_1 \bh{2T}]
  \mid \E_T \oosB \leq 0 \Big) \\
  & < 1/2
\end{align*}
since the inner conditional probability is less than 1/2.

Then
\begin{equation*}
  \E(\Lambda \mid \E_T \oosB \leq 0)
  =   \E(\Lambda \mid \E_T \oosB \leq 0)
\end{equation*}

and magick...
\qed

\subsection*{Proof of Theorem~\ref{res:insample2}}
As in the proof of Theorem~\ref{res:insample1}, let $(\e_{t+h}, x_{t})
\sim i.i.d.\ N(0,I)$ and let $\theta_1 = 0$. The event $\E_T \oosB
\geq 0$ implies that
\begin{equation*}
  \E_T (y_{T+\h+1} - x_{1T+1}'\bh{1T})^2
  \geq \E_T (y_{T+\h+1} - x_{2T+1}'\bh{2T})^2 \quad a.s.
\end{equation*}
which can be expressed as
\begin{equation}\label{eq:34}
  \bh{1T}'\bh{1T} - \bh{2T}'M_1'M_1\bh{2T}
  \geq (\bh{2T} - \theta_2)' M_2'M_2 (\bh{2T} - \theta_2) \quad a.s.
\end{equation}
$M_2 \bh{2T}$ is normally distributed conditional on $\bh{1T}$ and
$M_1 \bh{2T}$, and is a.s. contained in the sphere defined
by~\eqref{eq:34} conditional on $\bh{1T}$, $M_1 \bh{2T}$, and the
event $\E_T \oosB \geq 0$.

Then
\begin{equation*}
  \E( \Lambda \mid \E_T \oosB \geq 0 )
\end{equation*}

\qed

\bibliographystyle{abbrvnat}
\bibliography{references}
\clearpage

\begin{table}[tb]
  \begin{tabularx}{\linewidth}{XX}
    \toprule
    Category & Variable \\
    \midrule
    Stock market variables
    & Dividend to price ratio (log) \\
    & Earnings to price ratio (log) \\
    & Stock market variance \\
    & Book to market ratio \\
    & Net equity expansion \\
    & Percent equity issuing \\\\
    Interest rate variables
    & Treasury Bill rate (3 month) \\
    & Long term yield \\
    & Long term rate \\
    & Default return spread \\
    & Default yield spread \\
    & Inflation rate \\
    \bottomrule
  \end{tabularx}
  \caption{Variables used to predict the equity premium
    (Section~\ref{sec:empirics}).
    Please see Goyal and Welch's original paper \citep{GoW:08}
    for a detailed description of each variable.}
  \label{tab:equity}
\end{table}

\tryinput{floats/empirics-insample.tex}
\tryinput{floats/empirics-insample-tuned.tex}
% These macros create tables. They are defined in
% floats/empirics-insample.tex and possibly overwritten in the next
% file.
\insamplecoefs
\insampletests

\tryinput{floats/mc-ftest.tex}

\begin{figure}\centering
  {\large Coverage of DMW OOS interval for $\E_R\oosA$ in simulations}
  \tryinput{floats/mc-interval-testerror1.tex}
  \caption{Simulated coverage of $\E_R \oosA$ at 90\% confidence using
    a one-sided interval based on the DMW \oost\ test, plotted as a
    function of the fraction of observations used in the test sample,
    $P/T$. The solid horizontal line denotes the intervals' nominal
    coverage.}
 \label{fig:interval-R}
\end{figure}
\clearpage

\begin{figure}\centering
  {\large Coverage of DMW OOS interval for $\E_T\oosB$ in simulations}
  \tryinput{floats/mc-interval-generror1.tex}
  \caption{Simulated coverage of $\E_T \oosB$ at 90\% confidence using
    a one-sided interval based on the DMW \oost\ test, plotted as a
    function of the fraction of observations used in the test sample,
    $P/T$.  The solid horizontal line denotes the intervals' nominal
    coverage.}
  \label{fig:interval-T}
\end{figure}
\clearpage

\begin{figure}\centering
  {\large Size of DMW \oost\ test in simulations}
  \tryinput{floats/mc-dmwsize.tex}
  \caption{Simulated rejection probabilities for the DMW \oost\ test
    under the null hypothesis that the benchmark model will forecast
    better, $\E_T \oosB \leq 0$. The nominal size is 10\% and is
    marked with a solid horizontal line. Values greater than 10\%
    indicate that the test rejects the benchmark model too often. See
    Section~\ref{sec:simulation-design} for a discussion of the
    simulation design.}
  \label{fig:ttest-size}
\end{figure}
\clearpage

\begin{figure}\centering
  {\large Size of Clark-West OOS test in simulations}
  \tryinput{floats/mc-clarkwestsize.tex}
  \caption{Simulated rejection probabilities for Clark and West's
    (2006, 2007) OOS test statistic under the null hypothesis that the
    benchmark will forecast better, $\E_T \oosB \leq 0$. The nominal
    size is 10\% and is marked with a solid horizontal line. Values
    greater than 10\% indicate that the test rejects the benchmark
    model too often. See Section~\ref{sec:simulation-design} for a
    discussion of the simulation design.}
   \label{fig:clarkwest}
\end{figure}
\clearpage

\begin{figure}\centering
  {\large Size of McCracken \oost\ test in simulations}
  \tryinput{floats/mc-mccrackensize.tex}
  \caption{Simulated rejection probabilities for McCracken's (2007)
    \oost\ test under the null hypothesis that the benchmark model is
    more accurate, $\E_T \oosB \leq 0$. Nominal size is 10\% and is
    marked with a solid horizontal line. Values greater than 10\%
    indicate that the test rejects the benchmark model too often. See
    Section~\ref{sec:simulation-design} for a discussion of the
    simulation design.}
  \label{fig:mccracken}
\end{figure}
\clearpage

\begin{figure}\centering
  {\large Power of DMW \oost\ test in simulations}
  \tryinput{floats/mc-dmwpower.tex}
  \caption{Simulated rejection probabilities for the DMW \oost\ test
    under the alternative that the benchmark model is less accurate,
    $\E_T \oosB > 0$. Nominal size is 10\% and is marked with a solid
    horizontal line. Values greater than 10\% indicate that the test
    rejects the benchmark model too often. See
    Section~\ref{sec:simulation-design} for a discussion of the
    simulation design.}
  \label{fig:ttest-power}
\end{figure}
\clearpage

\begin{figure}
\centering
\large{Difference in OOS MSE of Prevailing Mean and\\ Kitchen
    Sink Models of Equity Premium (OLS)}
\tryinput{floats/empirics-oos-mse-1.tex}
\tryinput{floats/empirics-oos-mse-1b.tex}
\caption{OOS difference in the MSE
  of the prevailing mean benchmark and the kitchen sink model as a
  function of the test sample size, $R$.  Both models forecast the
  equity premium using annual data from 1928--2008.  The solid line
  gives the OOS average, and the shaded region indicates the
  one-sided 95\% confidence interval implied by the
  DMW test.  The bottom panel is a detailed view of the top
  panel for $R \geq 50$.}
\label{fig:empirics1}
\end{figure}
\clearpage

\begin{figure}
\centering
\large{Difference in OOS MSE of Prevailing Mean and\\ Kitchen
    Sink Models of Equity Premium (CT)}
\tryinput{floats/empirics-oos-mse-2.tex}
\tryinput{floats/empirics-oos-mse-2b.tex}
\caption{OOS difference in the MSE
  of the prevailing mean benchmark and the kitchen sink model as a
  function of the test sample size, $R$.  Both models forecast the
  equity premium using annual data from 1928--2008.  The solid line
  gives the OOS average, and the shaded region indicates the
  one-sided 95\% confidence interval implied by the 
  DMW test.  The bottom panel is a detailed view of the top
  panel for $R \geq 50$.}
\label{fig:empirics2}
\end{figure}
\clearpage

\begin{figure}
\centering
\large{OOS MSE of Individual Forecasts of Equity Premium}
\tryinput{floats/empirics-oos-ind-ks.tex}
\tryinput{floats/empirics-oos-ind-pm.tex}
\caption{OOS MSE of the Prevailing Mean (PM) and
    Kitchen Sink (KS) models for equity premium prediction as
    a function of the size of the training sample, $R$.  Please note
    that the vertical scales are different in the two plots.}
\label{fig:empirics3}
\end{figure}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:

%  LocalWords:  ClM Mcc ClW MeR StW InK CCS CoS GiW GiR BRC HTF BoH
%  LocalWords:  Tib BaN AIC BoB AkA AkP Efr iR eq FPE th Dou iS iT dx
%  LocalWords:  JoD ixz Mcl RHS MSE homoskedastic Joh Widom TrW Sil
%  LocalWords:  BFP dy LLNs VeR rlecuyer RSQLite Sar tikzDevice TikZ
%  LocalWords:  PGF LaTeX jt test's CaT Coc BRW LeN RaW RaZ LeR ZeH
%  LocalWords:  lmtest Zei dbframe Hmsic Har xtable Dah booktabs FeE
%  LocalWords:  Bonferroni BWB Gia MeP indices Schwarz HaH Jong
