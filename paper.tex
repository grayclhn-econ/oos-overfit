\documentclass[12pt]{article}
\input{setup}

\title{Out-of-Sample Comparisons of Overfit Models}
\author{Gray Calhoun\thanks{email: gcalhoun@iastate.edu. I
    would like to thank Julian Betts, Helle Bunzel, Marjorie Flavin,
    Nir Jaimovich, Lutz Kilian, Ivana Komunjer, Michael McCracken,
    Seth Pruitt, Ross Starr, Jim Stock, Yixiao Sun, Allan Timmermann,
    Hal White, several anonymous referees, participants at the 2010
    Midwest Economics Association Annual Meetings, the 2010
    International Symposium on Forecasting, the 2010 Joint Statistical
    Meetings, the 2011 NBER-NSF Time-Series
    Conference, and in many seminars at UCSD, and espeically
    Graham Elliott for their valuable suggestions, feedback and advice
    in writing this paper.  I would also like to thank Amit Goyal for
    providing computer code and data for his 2008 RFS paper
    with Ivo Welch \citep{GoW:08}.} \\ Iowa State University}

\begin{document}
\maketitle

\begin{abstract}\noindent
  This paper uses dimension asymptotics to study why overfit linear
  regression models should be compared out-of-sample; we let the
  number of predictors used by the larger model increase with the
  number of observations so that their ratio remains uniformly
  positive.  Our analysis gives a theoretical motivation for using
  out-of-sample (OOS) comparisons: the DMW OOS test allows a forecaster to
  conduct inference about the expected future accuracy of his or her
  models when one or both is overfit.  We show analytically and
  through Monte Carlo that standard full-sample test statistics can
  not test hypotheses about this performance.  Our paper also shows
  that popular test and training sample sizes may give misleading
  results if researchers are concerned about overfit.  We show that
  $P^2/T$ must converge to zero for the DMW test to give valid
  inference about the expected forecast accuracy, otherwise the test
  measures the accuracy of the estimates constructed using only the
  training sample.  In empirical research, $P$ is typically much
  larger than this.  Our simulations indicate that using large values
  of $P$ with the DMW test gives undersized tests with low power, so
  this practice may favor simple benchmark models too much.

\noindent JEL Classification: C12, C22, C52, C53

\noindent Keywords: Generalization Error, Forecasting, Model
Selection, $t$-test, Dimension Asymptotics
\end{abstract}
\newpage

\section{Introduction}
\label{sec:introduction}

Consider two sequences of length $P$ of prediction errors, the result
of forecasting the same variable with two different estimated models.
Both models are estimated with $R$ observations, collectively called
the {\em estimation window}, and are used to forecast an additional
$P$ observations, called the {\em test sample}.  There are $T$
observations in all, and $R+P=T$.  This paper introduces a new limit
theory for statistics constructed from these prediction errors
designed to approximate the behavior of the statistics when one of the
models is overfit.  In doing so, we provide a theoretical
justification for forecasters to use OOS instead of in-sample
comparisons: the DMW OOS test%
\footnote{In this paper, we will refer to the basic OOS $t$-test
  studied by \citet{DiM:95} and \citet{Wes:96} as the DMW test.} %
allows a forecaster to conduct
inference about the expected future accuracy of his or her models when
one or both is overfit.  We show analytically and through Monte Carlo
that standard full-sample test statistics can not test hypotheses
about this performance.

Our paper also shows that popular test and training sample sizes may
give misleading results if researchers are concerned about overfit.
We show that $P^2/T$ must converge to zero for the DMW test to give
valid inference about the expected forecast accuracy, otherwise the
test measures the accuracy of the estimates constructed using only the
training sample.  In empirical research, $P$ is typically much larger
than this.  Our simulations indicate that using large values of $P$
with the DMW test gives undersized tests with low power, so this
practice may favor simple benchmark models too much.  Existing
corrections, proposed by \citet{ClM:01,ClM:05}, \citet{Mcc:07} and
\citet{ClW:06,ClW:07}, seem to overcorrect for this problem, though, and reject
too often when the benchmark model is more accurate.


Although OOS comparisons have been popular in Macroeconomics and
Finance since \citepos{MeR:83} seminal study of exchange rate models,
it has been unclear from a theoretical perspective whether or not the
statistics are useful.  Empirical researchers often cite ``overfit''
or ``instability'' as reasons for using OOS comparisons, as in
\citet{StW:03}, but neither term is precisely defined or formalized.
Compounding this problem, the asymptotic distributions of these
statistics are derived under conditions that rule out either
instability or overfit and allow a researcher to use a conventional
in-sample comparison---a variation of the $F$-test, for example.  As
\citet{InK:04} argue, the statistics themselves are designed to test
hypotheses that can be tested by these in-sample statistics.  For
example, \citet{DiM:95} and \citet{Wes:96} derive the limiting
distributions of many popular OOS test statistics under conditions
that would justify these full-sample tests.  Much
of the subsequent research by \citet{Mcc:00, Mcc:07}, \citet{CCS:01},
\citet{ClM:01,ClM:05}, \citet{CoS:02,CoS:04}, \citet{ClW:06,ClW:07},
\citet{Ana:07}, and others relaxes several of Diebold and Mariano's
and West's assumptions, but maintains the stationarity and dependence
conditions that permit in-sample comparisons \citep[see][for a
review of this literature]{Wes:06}.%
\footnote{Like us, \citet{Ana:07} allows the number of regressors to
  increase with $T$.  But in that paper, the number of regressors
  increases slowly enough that the OLS coefficients are consistent and
  asymptotically normal.} %
\citet{GiW:06} and
\citet{GiR:09, GiR:10} are exceptions.  Instead of focusing on
hypotheses that can be tested by in-sample comparisons, \citet{GiW:06}
derive an OOS test for the null hypothesis that the difference
between two models' OOS forecasting performance is unpredictable, a
martingale difference sequence (MDS); \citet{GiR:09} test whether the OOS
forecasting performance of a model suffers from a breakdown relative
to its in-sample performance; and \citet{GiR:10} test whether the
forecasting performance is stable. However, those papers focus on a
particular OOS estimation strategy and do not address why OOS
comparisons might be useful as a general strategy.

Since in-sample and OOS statistics require similar assumptions and
test similar hypotheses, one might expect that they would give similar
results.  They do not.  In-sample analyses tend to support more
complicated theoretical models and OOS analyses support simple
benchmarks, as seen in \citet{MeR:83}, \citet{StW:03}, and
\citet{GoW:08}.  Since these different approaches strongly influence
the outcome of research, it is important to know when each is
appropriate.  The explanations in favor of OOS comparisons claim
that they should be more robust to unmodeled instability
\citep{ClM:05,GiW:06,GiR:09,GiR:10} or to overfit
\citep{Mcc:98,Cla:04}.  Both explanations presume that the in-sample
comparison is invalid and the OOS comparisons are more reliable.  Of
course, as \citet{InK:04,InK:06} point out, both in-sample and OOS
methods could be valid, but the OOS methods could have lower power.

In this paper, we study the ``overfit'' possibility and leave
``instability'' to future research. This paper uses dimension
asymptotics to study the behavior of OOS comparisons when at least
one of the models is overfit---the number of its regressors
increases with the number of observations so that their ratio remains
positive.  Although overfit is sometimes used to describe the
situation where a forecaster chooses from many different models,
i.e. \emph{data-mining} or \emph{data-snooping}, we view these as
separate issues.  Procedures that account for the presence of many
models have been, and are continuing to be, developed \citep[see, for
example,][]{Whi:00,Han:05,RoW:05,HHK:10,ClM:12b}, but it is unclear
whether those procedures should themselves use in-sample or OOS
comparisons.  Understanding the difference between in-sample and OOS
comparisons in the context of a simple comparison between two models
is necessary before resolving any new issues that arise with multiple
comparisons. Moreover, the empirical research that motivates
this paper uses \emph{pseudo} OOS comparisons and not
true OOS comparisons.  Even if a true OOS comparison
could account for some forms of data-snooping better than
\citepos{Whi:00} BRC or its extensions, in-sample and pseudo
OOS comparisons would both be affected by the data-snooping, a
point also made by \citet{InK:04}.

We focus on linear regression models estimated with a fixed window
for simplicity, but our basic conclusions should be true for other
models and estimation strategies as well.  Under this
asymptotic theory, where the number of regressors $k$ increases with $T$
so that $\lim k/T$ is positive and less than one,
the OLS coefficient estimator is no longer
consistent or asymptotically normal \citep{Hub:73} and has positive
variance in the limit.  We show that, even so, the usual OOS average
is asymptotically normal and can consistently estimate the difference
between the models' generalization error, the expected loss in the
future conditional on the on the data available in period
$T$.%
\footnote{See, for example, \citet{HTF:08} for a discussion of
  generalization error.} %
Under these asymptotics, the generalization
error does not converge to the expected performance of the pseudotrue
models, so existing in-sample and OOS comparisons measure different
quantities and should be expected to give different results for
reasons beyond simple size and power comparisons.  Under our limit theory,
the model that is closer to the true DGP in population can forecast
worse.  In such a situation, a standard in-sample comparison would
correctly reject the null hypothesis that the benchmark is true, and
an OOS comparison would correctly fail to reject the null that the
benchmark is more accurate.%
\footnote{In a pair of papers similar to
  ours, \citet{ClM:12,ClM:12b} study in-sample and OOS tests that
  the larger model has nonzero coefficients that are too close to zero
  to expect the model to forecast more accurately.  Like this paper,
  they argue that the larger model can be true but less accurate.
  However, they focus on an aspect of the DGP that makes this
  phenomenon likely, while we focus on the coefficient estimates that
  produce less accurate forecasts.  Moreover, the implications of our
  asymptotic theories are different and their papers do not provide
  reasons to do OOS comparisons.} %
Also note that, in this situation, a model that performs well
in-sample can perform badly out-of-sample even if there are no
structural breaks or other forms of instability.  Researchers
have argued that a breakdown of in-sample forecasting ability
indicates a structural break (see \citealp{BoH:99}, and
\citealp{StW:03}, among others), but this breakdown can be caused by
overfit as well.

These theoretical results partially justify OOS comparisons when
researchers want to
choose a model for forecasting.  Although there has been little
emphasis on hypothesis testing in this setting, testing is usually
appropriate: there is usually a familiar benchmark model in place, and
the cost of incorrectly abandoning the benchmark for a less accurate
alternative model is higher than the cost of incorrectly failing to
switch to a more accurate alternative.  We show that the DMW test
lets the forecaster control the probability of the first error, just
as with conventional hypothesis testing.

But we also identify new practical limitations for applying the DMW
test to overfit models.
Since the models' coefficients are imprecisely estimated in the limit,
the test sample must be small enough that the model estimated over the
training sample is similar to the one that will be estimated over the
full sample.  In particular, $P/T \to 0$ is required for consistent
estimation of the difference in the two models' performance,
and $P^2/T \to 0$ is required for valid confidence
intervals and inference.  For larger $P$, the OOS comparisons remain
asymptotically normal, but are centered on the forecasting performance
associated with the period-$R$ estimates.  In practice, researchers
typically use large values of $P$, so these studies may be too
pessimistic about their models' future accuracy if they use the DMW
test.  Section~\ref{sec:oostheory} lays out the asymptotic behavior of
the DMW test under this limit theory.

Popular in-sample tests and model selection criteria, like the Wald
test and the AIC, do not help forecasters in this setting. We show
that these statistics do not select the more accurate model when
choosing between overfit models. For many DGPs the Wald test and the
AIC will choose a larger alternative model over a small benchmark
model with probability approaching one, regardless of which model is
more accurate, while the BIC chooses the benchmark model with
probability approaching one.%
\footnote{Our result holds for a broad class of
  full-sample statistics, but there may be other potential statistics
  that mimic the OOS test and remain valid. Exploring such
  statistics is left for future research.} %
This result holds even though modifications of the $F$-test are valid
under this asymptotic theory, as shown by \citet{BoB:95},
\citet{AkA:00}, \citet{AkP:04}, \citet{Cal:11c}, and \citet{Ana:12},
among others.%
\footnote{Also see \citet{Efr:86,Efr:04} for a discussion
  of naive in-sample loss comparisons.} %
Moreover, under this asymptotic theory, many recent OOS test
statistics, such as those derived by \cite{ClM:01,ClM:05},
\citet{Mcc:07}, and \citet{ClW:06,ClW:07} should have the same
problems as in-sample tests.%
\footnote{Our theoretical results apply
  directly to \citepos{Mcc:07} OOS $t$-test, since it simply
  proposes more liberal critical values for the same test statistic
  that we study.  Since \citet{ClW:06,ClW:07} use a finite length
  estimation window, our asymptotics are incompatible with theirs and
  prevent us from studying their test directly, as well as
  \citepos{GiW:06} and other tests based on \citepos{GiW:06}
  asymptotics.  But \clws\ test can be viewed as a stochastic
  adjustment to the critical values of the usual OOS $t$ test, so
  our conclusions should apply informally as well.  Specifically, we
  show that the DMW test rejects with probability equal to nominal
  size when the estimated benchmark model is expected to be more
  accurate, so more liberal critical values result in overrejection.} %
These tests are also designed to reject the benchmark when the
alternative model is true, and so they may reject too often when the
benchmark is false but more accurate.  Obviously, since the
distribution of these statistics converges to the normal when $P/T \to
0$ (with the number of regressors fixed), these statistics behave like
the DMW test when $P$ is small, but should overreject the benchmark
when $P$ is large. Section~\ref{sec:insample} presents our theoretical
results for full-sample statistics and Section~\ref{sec:mc} presents
Monte Carlo evidence to support these claims.

Finally, this paper introduces a new method of proof for OOS
statistics.  We use a coupling argument (Berbee's Lemma, 1979) to show
that sequences of OOS loss behave like mixingales when the
underlying series are absolutely regular, even if the forecasts depend
on non-convergent estimators.  Moreover, transformations of these
processes also behave like mixingales when recentered, so 
the arguments used to prove asymptotic
results for Near Epoch Dependent (NED) functions of mixing processes
can be used for these OOS processes with only slight modification.

The rest of the paper proceeds as follows.
Section~\ref{sec:assumptions} introduces our notation and assumptions.
Section~\ref{sec:theory} gives the main theoretical results for the
DMW OOS test, shows that standard in-sample tests reject the
benchmark model too often when it is false but more accurate than the
alternative, and shows that standard model selection methods can run
into similar problems. Section~\ref{sec:mc}
presents a Monte Carlo study supporting our theoretical results.
Section~\ref{sec:empirics} applies the OOS statistic to
\citepos{GoW:08} dataset for equity premium prediction, and
Section~\ref{sec:conclusion} concludes.  Proofs and supporting results
are listed in the Appendix.

\section{Setup and assumptions}
\label{sec:assumptions}

The first part of this section will describe the environment in detail
and set up our models and notation. The second part lists the
assumptions underlying our theoretical results.

\subsection{Notation and forecasting environment}

We assume the following forecasting environment. There are two
competing linear models that give forecasts for the target,
$y_{t+\h}$:
\begin{equation}
y_{t+\h} = x_{1t}'\theta_1 + \e_{1,t+h}, \quad\text{and}\quad
y_{t+\h} = x_{2t}'\theta_2 + \e_{2,t+h};
\end{equation}
$t = 1,\dots,T-h$, $\h$ is the forecast horizon and the
variables $y_t$, $x_{1t}$, and $x_{2t}$ are all known in period $t$.
The coefficients $\theta_1$ and $\theta_2$ minimize the population
Mean Square Error, so
\begin{equation}
  \theta_i = \argmin_\theta \sum_{t=1}^{T-\h} \E (y_{t+\h} - x_{it}'\theta)^2,
\end{equation}
making $\e_{i,t+\h}$ uncorrelated with $x_{i,t}$; $\e_{i,t+\h}$ can exhibit
serial correlation so both of the linear models may be misspecified.
Let $\mathcal{F}_t = \sigma(y_1, x_1, \dots, y_t, x_t)$ be the information
set available in period $t$,
with $x_t$ the vector of all stochastic elements of $x_{1t}$ and $x_{2t}$ after
removing duplicates, and let $\E_t$ and $\var_t$ denote the
conditional mean and variance given $\mathcal{F}_t$.  The first model
uses $K_1$ regressors, and the second uses $K_2$.  Without loss of
generality, assume that $K_1 \leq K_2$. At least one of the models is
overfit, which we represent asymptotically by letting $K_2$ grow with
$T$ quickly enough that $\lim K_2/T$ is positive; $K_1$ may grow with
$T$ as well. Since the models change with $T$, a
stochastic array underlies all of our asymptotic theory, but we
suppress that notation to simplify the presentation.

In the settings we are interested in, a forecaster observes the data
$(y_t,x_t)$ for periods 1 through $T$ and divides the observations
into an estimation sample of the first $R$ observations and a test
sample of the remaining $P$ observations. The forecaster then compares
the models' performance over the test sample, which entails
constructing two sequences of forecasts, (using a fixed-window
estimation strategy)
\begin{equation}
\hat y_{i,t+\h} = x_{it}'\hat{\theta}_{it}, \qquad \text{for } i=1,2;
\ t = R+1,\dots,T-h,
\end{equation}
where
\begin{equation}
  \bh{it} = \Big(\sum_{s=1}^{R-\h} x_{is}x_{is}'\Big)^{-1} \sum_{s=1}^{R-\h}
  x_{is} y_{s+\h}, \qquad \text{for } i=1,2;\ t = R+1,\dots, T - \h.%
\footnote{It may not be clear why we are using the index $t$ in $\bh{it}$,
    since $\bh{it} = \bh{iR}$ almost surely for all $t \leq T-h$.
    But $\bh{it}$ will be defined for $t > T-\h$
    soon and will not equal $\bh{iR}$ for those values of $t$.} %
\end{equation}

The models are then compared by their forecast performance over the test
sample. There are many statistics that have been considered in the
literature, but we focus on perhaps the most natural, the DMW \oost\
test \citep{DiM:95,Wes:96}.%
\footnote{The core insights of our paper apply to other OOS statistics
  as well.} %
This statistic is based on
the difference in the models' loss over the test sample, $\oosA$,
defined as
\begin{equation}
  \oosA \equiv P^{-1} \oosSum{t}{1} D_t
\end{equation}
where
\begin{equation}\label{eq:16}
  D_t = L(y_{t+\h} - x_{1t}'\bh{1t}) - L(y_{t+\h} - x_{2t}'\bh{2t}),
\end{equation}
and $L$ is a known loss function. The \oost\ test is defined as
$\sqrt{P} \oosA / \sh$, where $\sh^2$ is an estimator of the
asymptotic variance of $\oosA$. (Possibly a Heteroskedasticity- and
Autocorrelation-Consistent, or HAC, estimator.)

Statistics like $\oost$ have a long history in empirical economics
because they capture an intuitive idea of modeling: that a good model
should be able to forecast well when given new data and not only when
used to fit the same data that were used to estimate it. \cite{MeR:83}
use such a statistic to study exchange rate models and find that none
of the models that existed at the time of the study outperformed a
random walk. This finding has been remarkably durable and has spawned
an enormous literature; see \cite{Mar:95}, \cite{KiT:03},
\cite{CCP:05}, \cite{EnW:05}, \cite{Ros:05}, and \cite{BWB:10}, among
many others.

Most theoretical research on these statistics, such as \citet{DiM:95}, \citet{Wes:96},
and \cite{Mcc:07}, has focused on using the \oost\ statistic to test
hypotheses about the pseudotrue values $\theta_1$ and $\theta_2$.  In
particular, that research focuses on testing the null hypothesis that
\begin{equation}\label{eq:21}
  \E L(y_{t+\h} - x_{1t}'\theta_{1}) - \E L(y_{t+\h} - x_{2t}'\theta_{2}) = 0.
\end{equation}
But the population quantities in \eqref{eq:21} do not determine which
model is more accurate in practice. The models'
accuracy will also depend on the specific estimates of $\theta_1$ and
$\theta_2$ used to produce the forecasts. In this setting, where the
forecaster will use one of the models to make a number of predictions
in the future---call the number of predictions $Q$---the quantity of
interest becomes
\begin{equation}
  \E_T \oosB = Q^{-1} \sum_{t=T+1}^{T+Q} \E_T D_t,
\end{equation}
where $D_t$ is defined in Equation~\eqref{eq:16} but here uses the
full-sample estimates of the models' parameters,
\begin{equation}
  \bh{it} = \Big(\sum_{s=1}^{T-\h} x_{is}x_{is}'\Big)^{-1} \sum_{s=1}^{T-\h}
  x_{is} y_{s+\h}, \qquad \text{for } i=1,2;\ t = T+1,\dots, T + Q.
\end{equation}
If $\E_T \oosB$ is positive, the second model is expected to forecast
better than the first over the next $Q$ periods, and if $\E_T \oosB$
is negative then the first model is better. We use a conditional
expectation because the coefficient estimates in $\oosB$ are
stochastic but known in period $T$, and their values will determine
the performance of the two models.

Under conventional fixed-$K$ asymptotic theory, $\E_T \oosB$ would
converge in probability to the difference in the expected loss
associated with the pseudotrue models,%
\footnote{This statement is subject to the usual assumptions: some
  form of stationarity, bounded moments, and weak dependence.} %
i.e. the LHS of~\eqref{eq:21}.
\begin{equation}\label{eq:22}
  \E L(y_{t+\h} - x_{1t}'\theta_{1}) - \E L(y_{t+\h} - x_{2t}'\theta_{2}).
\end{equation}
But if $K_2$ increases with $T$ these quantities can have different
limits. For a simple example, assume squared-error loss, let $x_{1,t}
= 1$ for all $t$, and let $(y_{t+\h},x_{2,t})$ be i.i.d. $N(0,
\Sigma)$.  Then the difference between $\E_T \oosB$ and
quantity~\eqref{eq:22} is
\begin{align*}
  \E_T \oosB - & \big(\E L(y_{t+\h} - x_{1t}'\theta_{1}) - \E L(y_{t+\h} - x_{2t}'\theta_{2})\big) \\
  &= \big(\E_T (y_{T+\h+1} - \bh{1,T})^2 - \E_T (y_{T+\h+1} - x_{T+1}'\bh{2,T})^2\big) -
  \big(\E y_{T+\h+1}^2 - \E (y_{T+\h+1} - x_{T+1}'\theta_2)^2 \big)\\
  &= (\bh{2,t} - \theta_2)' \Sigma (\bh{2,t} - \theta_2) + o_p(1).
\end{align*}
This last term has expectation equal to $\var(y_T) \frac{K_2}{T - K_2 - 1}$ and
would converge to zero in probability if $K_2$ were fixed, but does not when $\lim
K_2 / T > 0$. In Section~\ref{sec:oostheory} we show that the \oost\
statistic can estimate $\E_T \oosB$ under our increasing $K$
asymptotics and does not estimate the expected loss associated with
the pseudotrue coefficients.

The conditional expectation $\E_T \oosB$ has been studied heavily in
cross-sectional settings with independent observations. In such a
setting, $\E_T \oosB$ is equal to the difference in the models'
\emph{generalization error}, which has been used widely as a measure
of model accuracy in the machine
learning literature \citep[see][for further discussion]{HTF:08}.
Moreover, with i.i.d. observations, the expectation of $\E_T \oosB$
equals \citepos{Aka:69} Final Prediction Error (FPE). Both
generalization error and FPE are defined by a model's
performance on a new, independent, data set, but, for lack of a better
term, we will call $\E_T \oosB$ the difference in generalization error
for the rest of the paper with hopefully no risk of confusion.

Finally, define the following notation.  The $l_v$-norm for vectors in
$\Re^p$ (with $p$ arbitrary) is denoted $\lvert \cdot \rvert_v$, and
the $L_v$-norm for $L_v$-integrable random variables is $\lVert \cdot
\rVert_v$.  The functions $\eigen_i(\cdot)$ take a square-matrix
argument and return its $i$th eigenvalue (with $\eigen_{i}(A) \leq
\eigen_{i+1}(A)$ for any matrix $A$).  All limits are taken as $T \to
\infty$ unless stated otherwise.

\subsection{Assumptions}
\label{sec:asmp}

The next conditions are assumed to hold throughout the paper.  The first
assumption controls the dependence of the underlying random array.
The second lays out the details of our asymptotic approximation.
The third assumption controls the
smoothness of the loss function and bounds the moments of the
difference in the models' performance; the fourth assumption describes
the behavior of the estimation and test windows.  And the last
assumption describes the kernel used to estimate the OOS average's
asymptotic variance.

\begin{asmp}\label{asmp-1}
  The random array $\{y_t,x_t\}$ is stationary and absolutely regular
  with coefficients $\beta_j$ of size $-\rho/(\rho-2)$; $\rho$ is
  greater than two and discussed further in Assumption \ref{asmp-3}.
\end{asmp}

This assumption is a standard condition on the dependence of the
underlying stochastic array. The only novelty is that we use
absolute regularity instead of strong or uniform mixing as our
weak dependence condition; absolute regularity admits a particular
coupling argument, \emph{Berbee's Lemma} \citep[reproduced in this
paper as Lemma A.1 for reference]{Ber:79} that is
unavailable for strong mixing sequences. Absolute regularity
implies uniform mixing, so this assumption is not unduly strong.
For a detailed discussion of these weak dependence conditions,
please see \citet{Dav:94} or \citet{Dou:94}.

Our strict stationarity assumption is also somewhat stronger than is
typically used; \citet{Wes:96} and \citet{Mcc:07}, for example,
present results assuming covariance stationarity of the loss
associated with the pseudotrue models. We need to make a stronger
assumption because we will need to prove asymptotic results when the
$\bh{it}$ remain random---so we would need covariance stationarity to
hold for almost all estimates of $\theta_i$ and not just for the
pseudotrue value. The only way to guarantee that condition is to
assume strict stationarity for the underlying stochastic processes.

The next assumption describes our asymptotic experiment.
\begin{asmp}\label{asmp-2}
  The number of regressors for each model, $K_1$ and $K_2$, are less
  than $R$ and $(K_2-K_0)/T$ is uniformly positive;
  $K_0$ is the number of regressors shared by the two models ($(K_1 - K_0)/T$
  may be uniformly positive as well, but is not required to be).

  The variance of $y_{t+\h}$ given $\mathcal{F}_t$ is uniformly
  positive and finite and all of the eigenvalues of the covariance
  matrix of $x_t$ are uniformly positive and finite as well.
  Moreover,
  \begin{align*}
    &\eigen_{\max}(X_{iS}'X_{iS}) = O_{L_3}(S), \\
    &\eigen_{\max}((X_{iS}'X_{iS})^{-1}) = O_{L_3}(1/S), \\
    \intertext{and}%
    &\eigen_{\max}\Bigg(\E\Big( \sum_{s,t=U}^{S-h} \e_{i,s+\h} \e_{i,t+\h} x_{is}x_{it}'
    \,\Big|\, X_{iS}'X_{iS} \Big)\Bigg) = O_{L_3}(S-U),
  \end{align*}
  for large enough $T$, where $S = R,\dots,T$, $U=1,\dots,S-\h$,
  $i = 1,2$,
  \[ X_{iS} \equiv [x_{i1} \quad \dots \quad x_{i,S-\h}]' \qquad
  \text{and} \qquad \ep{iS} = (\e_{i,1+\h}, \dots, \e_{i,S})'.\]

  Additionally, the Euclidean norms of the pseudotrue coefficients,
  $\theta_1$ and $\theta_2$, satisfy $|\theta_1|_2 = O(1)$ and
  $|\theta_2|_2 = O(1)$.
\end{asmp}

The assumption that $y_{t+\h}$ and $x_t$ have positive and finite
variance is straightforward. The conditions on the eigenvalues are
technical and control the behavior of the OLS estimator as the number
of regressors gets large---the third assumption is nonstandard but can
be easily verified under independence, for example. The restrictions
on the pseudotrue coefficients ensure that the regression model
doesn't dominate the variance of $y_{t+\h}$ in the limit.

The next assumption establishes moment conditions for the OOS loss
process and smoothness conditions for the loss function itself. The
moment conditions are standard and apply to $D_t$, and the smoothness
conditions are relatively weak.

\begin{asmp}\label{asmp-3}
  The loss function $L$ is continuous, has finite left and right
  derivatives, and $L(0) = 0$.  There is a constant $B_L$ and a
  function $L'$ that bounds the left and right derivative of $L$ at
  every point such that $\|D_t\|_\rho \leq B_L$; $\|D_t^*\|_\rho \leq
  B_L$ for all $t$, where
  \begin{equation*}
    D_t^* = L(y^* - x_1^{*\prime}\hat\theta_{1t})
    - L(y^* - x_2^{*\prime}\hat\theta_{2t})
  \end{equation*}
  and $(y^*, x_1^*, x_2^*)$ equals $(y_t, x_{1t}, x_{2t})$ in
  distribution but is independent of $\mathcal{F}_T$ ($\rho$ is defined
  in Assumption~\ref{asmp-1}); and
  \begin{equation*}
    \| L'(y^* - x_{i}^{*\prime} (\alpha \bh{iR} + (1-\alpha) \bh{iT})) \|_2
    \leq B_L
  \end{equation*}
  for any $\alpha \in [0,1]$.
\end{asmp}

The differentiability condition in Assumption~\ref{asmp-3} is weak and
allows the loss function itself to be non-differentiable; for example,
absolute error and many asymmetric loss functions satisfy this
assumption. The Assumption makes use of both $(y_{t+\h}, x_{1t},
x_{2t})$ and $(y^*, x_1^*, x_2^*)$ because the period $t$ observations
can be dependent on $\bh{iT}$ in complicated ways. When the underlying
observations are independent these assumptions can simplify
considerably.

The next assumption controls the growth of the test and
training samples.
\begin{asmp} \label{asmp-4} (a) $P, R, Q \to\infty$ as $T \to
  \infty$. (b) $P^2/T \to 0$ as $T \to \infty$.
\end{asmp}

The requirements that $P$ and $R$ grow with $T$ are common. Parts of
the assumption are new, in particular the requirement that $P^2/T \to
0$.  See Lemma~\ref{res-convergence} for a discussion of its
implications.  In practical terms, this assumption requires that the
test sample be large and that the training sample be much larger, by
enough that including or excluding the test sample does not affect the
estimates of $\theta_1$ or $\theta_2$ very much.

A final assumption restricts the class of variance estimators we will
consider.  We use the same class of estimators studied by
\citet{JoD:00} (their class $\mathcal{K}$); see
their paper for further discussion.

\begin{asmp}
  \label{asmp-5} $W$ is a kernel from
$\Re$ to $[-1,1]$ such that $W(0) = 1$, $W(x) = W(-x)$ for all $x$,
\begin{equation*}
  \int_{-\infty}^{\infty} \lvert W(x) \rvert dx < \infty, \quad
  \int_{-\infty}^{\infty} \lvert \psi(x) \rvert dx < \infty
\end{equation*}
with
\begin{equation*}
  \psi(x) = \frac1{\sqrt{2\pi}} \int_{-\infty}^{\infty} W(z) e^{ixz}dz,
\end{equation*}
and $W(\cdot)$ is continuous at zero and all but a finite number of
points.
\end{asmp}

Assumptions~\ref{asmp-1}--\ref{asmp-5} are broadly similar to
\citepos{Wes:96} assumptions, except with regard to model complexity
and the size of the test sample.  West assumes that $K_1$ and $K_2$
are both finite, while we let them grow rapidly with $T$
(Assumption~\ref{asmp-1}), and West lets $P/R$ converge to any value
in $[0,\infty]$, while we require $P^2/R \to 0$
(Assumption~\ref{asmp-4}).%
\footnote{\citet{Wes:96} does not have a
  second OOS period, so there is no analogue in his paper to our
  restriction on $Q$.} %
Obviously, these differences are
related---allowing for complex models requires tighter constraints on
the size of the test sample.  West allows more general estimation
methods, while we restrict attention to OLS; the advantage of using
OLS is that we can derive the inequality,
\begin{equation*}
  \| x_{it}'\bh{i,T+1} - x_{it}'\bh{iR} \|_2 \leq O(P/T)^{1/2}.
\end{equation*}
Our main results will hold for other models and estimation windows as
long as a similar inequality exists, but proving such inequalities is
left for future research.

Our other assumptions are similar.  The loss function must be smooth
almost everywhere, which is less restrictive than \citet{Wes:96}
requires but is not new; \citet{Mcc:00} extends West's results to
cover a similar case. We assume full stationarity and absolute
regularity of the underlying random array, while West assumes
covariance stationarity and strong mixing of the transformed forecast
errors; and the details our moment conditions are different, but seem
that they would hold in similar environment to West's.  Finally, since
West's is a more standard environment, he assumes the existence of a
consistent estimator of the variance, while we construct it
explicitly.  Only West's assumptions governing model complexity and
the size of the test sample are substantially different than ours.

\section{Theoretical results}
\label{sec:theory}

This section lays out our theoretical results. The first subsection
presents results for the DMW $t$-test; we show that it is
asymptotically normal when one or both forecasting models is overfit.
(i.e., under our increasing-$K$ asymptotic approximation.) We also
present a new limitation on these statistics---OOS comparisons heavily
penalize overfit models unless the size of the test sample is a very
small proportion of the total sample size, which will not be practical
in much applied research. The second subsection presents results for
full-sample statistics and shows that widely used test statistics and
model selection criteria are misleading when choosing a model for
forecasting. In contrast to OOS comparisons, these full-sample
criteria choose overfit models too often, even when they are less
accurate than simple benchmark models. These results are somewhat
abstract, so the third subsection works through an example DGP in
detail.

\subsection{Asymptotic normality of the DMW test}
\label{sec:oostheory}

This section has two main conceptual results. The first,
Lemma~\ref{res-mixingale}, shows that the OOS average, $\oosA$, is
asymptotically normal as the size of the test sample grows, even when
the models are overfit. But $\oosA$ is centered at $\E_R \oosA$, the
difference in the generalization error of the models estimated over
the \emph{training sample}, which is not the quantity of interest to
forecasters. Forecasters will generally want to estimate or test
hypotheses about the difference in the generalization error of the
models estimated with the full sample, $\E_T \oosB$. Our second
result, Lemma~\ref{res-convergence}, shows that these quantities are
approximately equal only when the test sample is very small relative
to the total sample size. In particular, we show that $\oosA$ is a
consistent estimator of $\E_T \oosB$ when $P/T \to 0$ and is
asymptotically normal with mean $\E_T \oosB$ when $P^2/T \to 0$. After
establishing these Lemmas, we then show that the \oost\ test is
asymptotically standard normal and can be used to test hypotheses
about $\E_T \oosB$, which requires the two Lemmas as well as a
consistent estimator of the variance of the OOS average.

In the first result, we show that
$\sqrt{P} (\oosA - \E_R \oosA)$ is asymptotically normal as $P \to
\infty$. This application of the CLT is complicated by a hidden source
of dependence---the training sample estimators $\bh{iR}$. Under
conventional asymptotic theory, we would replace each $\bh{iR}$ with
its pseudotrue value $\theta_i$ and apply the CLT to $\sqrt{P}
L(y_{t+\h} - x_{it}'\theta_i)$, with some complications potentially
arising from the replacement (as in \citealp{Wes:96},
\citealp{ClM:01}, or \citealp{Mcc:07}, for example). But we can not
make that replacement here, because our asymptotic approximation
prevents $x_{it}'\bh{iR}$ from converging to $x_{it}'\theta_i$.

Instead, we show that $D_t - \E_R D_t$ is an $L_2$-mixingale that
satisfies the CLT. Mixingales satisfy a weak-dependence condition
similar to MDSes,%
\footnote{An array $Z_{n,t}$ and an increasing sequence of \sfields
  $\Gs_{n,t}$ is an \emph{$L_2$-mixingale of size $-1/2$} if there is
  an array of constants $\{c_{n,t}\}$ and a sequence of constants
  $\zeta_l = O(l^{-1/2 - \delta})$ for some $\delta > 0$ such that
  \begin{equation*}
    \lVert \E(Z_{n,t} \mid \Gs_{n,t-l}) \rVert_2
    \leq c_{n,t} \zeta_l
    \qquad\text{and}\qquad
    \lVert Z_{n,t} - \E(Z_{n,t} \mid \Gs_{n,t+l}) \rVert_2
    \leq c_{n,t} \zeta_{l+1}.
  \end{equation*}
  Mixingales were introduced and developed by
  \citet{Mcl:74,Mcl:75,Mcl:75b,Mcl:77}.} %
but they have some limitations. Transformations of mixingales are
typically not themselves mixingales, which means that CLTs for
mixingale processes require additional assumptions to hold beyond the
mixingale property.%
\footnote{See \citet{Jon:97} for an illustration. We will borrow
  heavily from his results in our proofs.} %
This is in contrast to Near Epoch Dependent (NED) processes, which
retain the NED property after transformations. \citep[See chapter 17
of][for further discussion of these properties.]{Dav:94} But the OOS
loss has more structure than most mixingales and behaves like an NED
process in important respects.

\begin{lem}\label{res-mixingale}
  If Assumptions~\ref{asmp-1}--\ref{asmp-3} hold then $\{D_t - \E_R
  D_t, \mathcal F_t\}$ is an $L_2$-mixingale of size $-1/2$.
  Moreover
  \begin{equation}\label{normalpart}
    \sqrt{P}(\oosA - \E_R \oosA)/\sigma \to^d N(0,1)
  \end{equation}
  as $P \to \infty$ if $\sigma^2$ is uniformly almost surely positive,
  where $\sigma^2 = \var_R(\sqrt{P} \oosA)$ (which is equal to $P
  \E_R(\bar{D}_R - \E_R \bar{D}_R)^2$).
\end{lem}

It may be helpful to compare Lemma~\ref{res-mixingale} to the method
of proof in \citet{GiW:06}. \citet{GiW:06} show that the OOS average
is asymptotically normal when the forecasts are estimated with a fixed
length rolling window. In that case, each $\bh{it}$ depends on only
the most recent $R$ observations and, since $R$ is fixed in their
theory, the forecast errors $y_{t+\h} - x_{it}'\bh{it}$ are themselves
mixing processes. Transformations of their forecast errors are
still obviously mixing processes and obey the CLT.

In our paper, $R$ is not fixed and the forecast errors are not a
convenient weakly-dependent process, since the estimation error in
$\bh{it}$ introduces strong dependence. Consequently,
transformations of the forecast errors are not weakly dependent
either. But this additional dependence has a special form and can be
removed by subtracting the conditional mean; this means that $g(y_{t+\h} -
x_{it}'\bh{it})$ is not weakly dependent but $g(y_{t+\h} -
x_{it}'\bh{it}) - \E_R g(y_{t+\h} - x_{it}'\bh{it})$ is, as long as
the new process satisfies standard moment
restrictions. Assumptions~\ref{asmp-1}--\ref{asmp-3} ensure that these
restrictions are satisfied for $D_t - \E_R D_t$.

The form of the dependence introduced by $\bh{it}$ also allows the CLT
to hold without additional restrictions---the proof rests on showing
that a process like $D_t^2 - \E_R D_t^2$ is also a mixingale, using a
similar argument to the one used by \citet{Jon:97} for NED
processes. This property needs to be established on its own because it
does not follow from the behavior of $D_t - \E_R D_t$.
Lemma~\ref{lem:a2} presents the details of this part of the proof.

The next Lemma connects $E_R \oosA$ to $\E_T \oosB$, the difference in
the models' generalization error. Under conventional asymptotics,
these quantities are generally close. But they are not for overfit
models, and the models will tend to forecast better when estimated over the
full sample than over the training sample. Consequently, OOS
comparisons will penalize overfit models too much unless the test
sample is small relative to the total data set.

\begin{lem} \label{res-convergence}
  Under Assumptions \ref{asmp-1}--\ref{asmp-4}a,
  \begin{equation}\label{eq:7}
    \E_T \oosB - \E_R \oosA = O_p(\sqrt{P/T}) + o_p(P^{-1/2} + Q^{-1/2}).
  \end{equation}
  Moreover, there exist processes and loss functions that satisfy
  Assumptions \ref{asmp-1}--\ref{asmp-4}a but $\E_T \oosB - \E_R \oosA
  \neq O_p(1)$ unless $P/T \to 0$.
\end{lem}

We can view $\E_T \oosB - \E_R \oosA$ as noise introduced by
approximating the performance of the full-sample estimates with that
of the partial-sample estimates. The second part of
Lemma~\ref{res-convergence} shows that the $O_p(\sqrt{P/T})$ term
is generally binding; using $\oosA$ to estimate $\E_T \oosB$ requires
$P/T \to 0$ for consistency and using $\sqrt{P} \oosA$ to test
hypotheses about $\E_T \oosB$ requires $P^2/T \to 0$.

Finally, we can use Lemmas~\ref{res-mixingale}
and~\ref{res-convergence} to show that the DMW test is asymptotically
normal and centered at $\E_T \oosB$, as long as
Assumption~\ref{asmp-4}b holds. This theorem justifies using the DMW
$t$-test with a short test sample, which may not be feasible in some
applications.

\begin{thm}\label{res-oost}
  Suppose that Assumptions \ref{asmp-1}--\ref{asmp-5} hold (including
  \ref{asmp-4}b), that $\gamma \to \infty$ and $\gamma/P \to 0$ as $T
  \to \infty$, and that $\sigma^2$ is uniformly a.s. positive. Define
  $\sh^2$ to be the usual OOS HAC estimator of the asymptotic variance
  of $\oosA$,
  \begin{equation}
    \sh^2 \equiv P^{-1} \oosSum{s,t}{1} (D_t - \oosA)(D_s - \oosA)
    \vWeight.
  \end{equation}
  Then
  \begin{equation}
    \sqrt{P}(\oosA - \E_T\oosB)/\sh \to^d N(0,1).
  \end{equation}
\end{thm}

The requirement that $\sigma^2$ be uniformly a.s. positive is not
restrictive under our asymptotic theory. Since the models'
coefficients are estimated with uncertainty in the limit, the two
models give different forecasts even if they both nest the DGP.  This
is intuitively similar to \citepos{GiW:06} rolling-window result, but
comes from different asymptotic theory; Giacomini and White keep the
variance of the OOS average positive by letting $P \to \infty$ with
$R$ fixed; in this paper, $R \to \infty$ but the variance remains
positive since $K \to \infty$ (quickly) as well.

Finally, we establish that the DMW test can be use to construct
confidence intervals for $\E_T \oosB$ or test hypotheses about $\E_T
\oosB$. In particular, forecasters will often want to test the null
hypothesis that $\E_T \oosB \leq 0$, meaning that the benchmark model
is expected to be more accurate than the alternative model in the
future.

\begin{thm}\label{res:oostest}
  Suppose that the conditions of Theorem
  \ref{res-oost} hold. Then each of the usual Gaussian
  confidence intervals,
  \begin{gather}
    [\oosA - z_{\alpha/2} \, \sh /
    \sqrt{P}, \oosA + z_{\alpha/2} \sh / \sqrt{P}],\label{interval-twosided} \\
    [\oosA - z_{\alpha} \, \sh / \sqrt{P}, +\infty), \label{interval-greater}
    \intertext{and}
    (-\infty, \oosA + z_{\alpha} \, \sh / \sqrt{P}],
  \end{gather}
  contains $\E_T\oosB$ with probability $\alpha$ in the limit, with
  $z_{\alpha}$ the $1-\alpha$ quantile of the standard normal
  distribution. If $\lim \Pr[\E_T \oosB \leq 0] > 0$ in addition then
  \begin{equation}\label{eq:23}
    \lim \Pr[P^{1/2}\oosA/\sh > z_{\alpha} \mid \E_T
    \oosB \leq 0] \leq \alpha.
  \end{equation}
\end{thm}
The results for confidence intervals in Theorem~\ref{res:oostest}
follow immediately from Theorem~\ref{res-oost},
but~\eqref{eq:23} requires additional steps.

\subsection{Failure of in-sample statistics for forecast evaluation}
\label{sec:insample}

In this subsection we look at the behavior of some full-sample
statistics under the same asymptotic theory as before.  We show that
these statistics, which include common tests such as the Wald test as
well as model selection criteria such as the AIC, do not measure the
models' generalization error and consequently do not indicate which
model will be more accurate in the future. Some of these statistics
will tend to choose the larger model regardless of which model will be
more accurate in the future, whereas others tend to choose the smaller
model.  This is a different issue than whether or not full-sample
tests are valid for testing hypotheses about the pseudotrue
coefficients of the models; as \citet{Cal:11c} and \citet{Ana:12}
demonstrate, variations of the Wald test can be valid for those
hypotheses under increasing-$K$ asymptotics.%
\footnote{\citet{Ana:12} shows that the Wald test is invalid in
  general and gives an adjustment that corrects the critical values;
  he also shows that the $F$-test is asymptotically valid under
  certain conditions on the distribution of the regressors.
  \citet{Cal:11c} shows that the $F$-test is asymptotically invalid
  without Anatolyev's constraint, even under homoskedasticity, and
  provides a correction that gives valid tests.  Both papers only
  consider independent observations.} %
To simplify the presentation we only derive results for nested models
but the conclusions hold more generally.

The full-sample statistics we study in this paper share a common
property. They choose the alternative model when the distance between
a subset of their coefficient estimates and the origin exceeds a
threshold, and choose the benchmark otherwise. For the Wald test, for
example, this threshold is chosen so that the test has correct size
when those coefficients are zero in population. For small models that
can be consistently estimated, these coefficient estimates are close
to their true values and so this criterion can be a reasonable proxy
for the relative accuracy of the larger model.

But for overfit models, the estimates will typically be far from both
their pseudotrue values and from zero. In that case, the Wald test and
the AIC will both tend to choose the larger model even when it is less
accurate than the smaller benchmark model. This phenomenon is driven
by the dimensionality of the alternative model, since there are more
potential values of the coefficient estimator that are far from zero
when it has many elements. The threshold for the Wald test and the AIC
is set by construction to be a bounded distance from the origin.
Every other statistic that shares this property has the same behavior,
since the probability that the coefficient estimators of a
less-accurate but true alternative model fall inside a bounded
threshold are arbitrarily small in the limit. This behavior is
formalized by Theorem~\ref{res:insample1}.
\begin{thm}\label{res:insample1}
  Suppose Assumptions~\ref{asmp-1}--\ref{asmp-4} hold and let
  $\Lambda$ be a model selection statistic that takes the values zero
  or one; $\Lambda = 0$ indicates that the benchmark model is chosen
  and $\Lambda = 1$ indicates the alternative.  Moreover, assume that
  $L(e) = e^2$ and that there exist a deterministic scalar $c$ and a
  sequence of possibly random matrices $V_T$ such that
  \begin{equation}\label{eq:25}
    \Pr[\Lambda > \1\{\bh{2T}' V_T \bh{2T} > c \}] \to 0
    \quad \text{as } T \to \infty
  \end{equation}
  where the eigenvalues of $V_T$ are uniformly bounded in probability
  and $\plim \rank(V_T)/T > 0$.

  Then there exist DGPs satisfing these assumptions such that
  \begin{equation}\label{eq:11}
    \E( \Lambda \mid \E_T \oosB \leq 0 ) \to^p 1.
  \end{equation}
\end{thm}
As we state above,~\eqref{eq:11} means that, with probability
approaching 1, the benchmark model is rejected even when it is more
accurate. The DGPs and statistics used in Theorem~\ref{res:insample1}
are simple and common. The DGPs include correctly specified linear
models with normal and homoskedastic errors. The statistics that meet
the restrictions on $\Lambda$ include the $F$-test and AIC.

For example, the $F$-statistic to test the null that all of the
coefficients unique to the larger model is known to have the form
\begin{equation}\label{eq:17}
  F = \frac{\bh{2T}' M_2' ( M_2 (X_{2T}'X_{2T})^{-1} M_2' )^{-1} M_2
    \bh{2T}}{s^2 (K_2 - K_1)}
\end{equation}
where $s^2$ is the usual estimator of the variance of the regression
error and $M = (0_{K_2 - K_1, K_1}\ \ I_{K_2 - K_1})$. Then let $c$ be any number
greater than 1 and define
\begin{equation*}
  V_T = M_2' \big( M_2 \big(\tfrac{1}{s^2 (K_2 - K_1)} X_{2T}'X_{2T}\big)^{-1}
  M_2' \big)^{-1} M_2'.
\end{equation*}
If Assumption~\ref{asmp-2} holds and $s^2$ is consistent then $V_T$
has uniformly bounded eigenvalues and has rank $K_2 - K_1$, so it
satisfies~\eqref{eq:25}.%
\footnote{If it is surprising that these conditions hold for any $c >
  1$, remember that the $F$ random variable converges to 1 in
  probability as both degrees of freedom diverge to $\infty$ since its
  numerator and denominator both obey the LLN.} %
Robust variations of the Wald test can obviously satisfy~\eqref{eq:25}
for similar reasons, and the AIC for nested linear models is
equivalent to using the $F$-test with the critical value %
$(e^{2 (K_2 - K_1) / T} - 1) \cdot (T - K_2) / (K_2 - K_1)$, %
which converges to a finite limit, so the AIC satisfies~\eqref{eq:25}
as well.

For statistics that don't satisfy~\eqref{eq:25}, the behavior can be
quite different. The BIC, for example, can also be written in terms of
the $F$-statistic and is equivalent to using the critical value $(T \,
e^{2 (K_2 - K_1) / T} - 1) \cdot (T - K_2) / (K_2 - K_1)$. This
critical value diverges as $T \to \infty$, ensuring that~\eqref{eq:25}
fails for any $c$. For this statistic, we have the opposite problem as
before: when the alternative model is \emph{more} accurate, the
coefficient estimates of the larger model are contained in a bounded
region of the parameter space. Since the acceptance region of the BIC
grows, it eventually contains \emph{any} bounded region of the
parameter space. For large enough $T$, the BIC will always choose the
\emph{smaller model}, even when the larger model is more accurate.

Theorem~\ref{res:insample2} describes this behavior in detail.

\begin{thm}\label{res:insample2}
  Suppose Assumptions~\ref{asmp-1}--\ref{asmp-4} hold, let $L(e) =
  e^2$, and let $\Lambda$ be a model selection statistic as in
  Theorem~\ref{res:insample1}. Also assume that, for any finite scalar
  $c$,
  \begin{equation}\label{eq:27}
    \Pr[\Lambda > \1\{\bh{2T}' V_T \bh{2T} > c \}] \to 1
    \quad \text{as } T \to \infty,
  \end{equation}
  where $V_T$ is a sequence of possibly random matrices with with
  eigenvalues uniformly bounded in probability and $\plim \rank(V_T)/T
  > 0$. Then there exist DGPs satisfing these assumptions such that
  \begin{equation}\label{eq:15}
    \E( \Lambda \mid \E_T \oosB \geq 0 ) \to^p 0.
  \end{equation}
\end{thm}

Again,~\eqref{eq:15} implies that statistics like the BIC will always
choose the smaller model, even when the larger model will give more
accurate forecasts. Both models may be overfit, in that both $K_1/T$
and $K_2/T$ may both be positive in the limit; the key is that $(K_2 -
K_1)/T$ is also positive in the limit.

It is important to remember that, when $\Lambda$ represents a test
statistic, the test may have correct size for the null hypothesis that
the additional coefficients on the larger model are zero. So previous
research, such as \citet{Cal:11c} and \citet{Ana:12}, does not predict
these results. Any statistic that uses the distance of the models'
estimated coefficients from a set point (the origin being the most
common) is poorly suited for choosing between overfit models. These
models only forecast well when their coefficient estimates are close
to their pseudotrue values, which can be far from any prespecified
point. Depending on the statistic, it can be biased towards choosing
the larger model or the smaller model. Formal in-sample tests will
likely be biased towards the larger model, as we show for the $F$-test
in Theorem~\ref{res:insample1}.

\subsection{An extended but simple example}
\label{sec:example}

This subsection illustrates the previous theoretical results with a
concrete, extremely simple, example. Let $L(e) = e^2$ and $h =
1$. Suppose that $\e_{t+1} \sim i.i.d.\ N(0,1)$, that the benchmark
model is nested in the alternative, and let
\begin{equation}
  y_{t+1} = x_{2t}'\theta_2 + \e_{t+1}.
\end{equation}
be the DGP. We also assume for simplicity that the full-sample design
matrix $X_{2T}$ satisfies $X_{2T}'X_{2T} = T \times I$ and that
$x_{T+1} \sim N(0, I)$, which violates our assumptions on the
regressors but simplifies this treatment considerably.

The first part shows how the assumptions in Section~\ref{sec:asmp} are
satisfied in this example and the second demonstrates asymptotic
normality of the DMW OOS $t$-test. The third part explicitly shows
that $\E_R \oosA$ converges to $\E_T \oosB$ only when the test sample
is small. And the last part demonstrates that the $F$-test, AIC, and
BIC do not indicate which model will be more accurate in the future,
even in this simple example.

\subsubsection*{Fulfillment of the assumptions}

\subsubsection*{Asymptotic normality of the OOS average}

Under these conditions, we have
\begin{equation}
  D_t =
  \begin{cases}
    2 \e_{t+1} (x_{2t}'\bh{2R} - x_{1t}'\bh{1R})
    + (x_{2t}'\theta_2 - x_{1t}'\bh{1R})^2
    - (x_{2t}'\theta_2 - x_{2t}'\bh{2R})^2
    & \text{if } t < T \\
    2 \e_{t+1} (x_{2t}'\bh{2T} - x_{1t}'\bh{1T})
    + (x_{2t}'\theta_2 - x_{1t}'\bh{1T})^2
    - (x_{2t}'\theta_2 - x_{2t}'\bh{2T})^2
    & \text{if } t > T
  \end{cases}
\end{equation}
and so we can explicitly derive the components of
Lemma~\ref{res-mixingale}:
\begin{multline}\label{eq:17}
  D_t - \E_R D_t
  = 2 \e_{t+1} (x_{2t}'\bh{2R} - x_{1t}'\bh{1R})
  + \Big\{(x_{2t}'\theta_2 - x_{1t}'\bh{1R})^2
          - \E_R (x_{2t}'\theta_2 - x_{1t}'\bh{1R})^2 \Big\} \\
  - \Big\{(x_{2t}'\theta_2 - x_{2t}'\bh{2R})^2
          - \E_R (x_{2t}'\theta_2 - x_{2t}'\bh{2R})^2 \Big\}
\end{multline}
for $t < T$. Since the underlying observations are i.i.d., $\E_{t-1}
D_t = \E_R D_t$ a.s.\ and $\{D_t - \E_R D_t, \Fs_{t};
t=R+1,\dots,T-1\}$ is an MDS. Even in this simple example, the
estimation error in $\bh{1R}$ and $\bh{2R}$ introduces strong
persistance into the OOS forecast errors, but this persistence is
predetermined so we can continue to apply basic asymptotic theory.

Although transformations of $D_t - \E_R D_t$ will obviously not be
MDSes in general, it should be clear that transformations of $D_t$
will be MDSes after subtracting their conditional mean; i.e.
\begin{equation}
  \{g(D_t) - \E_R g(D_t); \Fs_t; t = R+1,\dots,T-1\}
\end{equation}
is an MDS as long as
$g(D_t)$ has finite mean, but $g(D_t - \E_R D_t)$ is not. This MDS
result holds because $x_t$ and $y_{t+1}$ are independent of $\bh{1R}$
and $\bh{2R}$, so
\begin{equation}\begin{split}
  \E_R g(D_t)
  &= \int g\Big((x_1'\bh{1R})^2 - (x_1'\bh{2R})^2
       + 2 y (x'\bh{2R} - x'\bh{1R})\Big) \, f(y, x) \, dx \, dy \\
  &= \E_{t-1} g(D_t)
\end{split}\end{equation}
a.s., where $f$ is the density of $(y_{t+1}, x_{2t})$ and $x_1$
denotes the first $K_1$ elements of $x$.

These MDS results imply that $\oosA$ and $(1/P) \sum_{t=R+1}^{T-1}
D_t^2$ both obey LLNs: $\oosA \to^p \E_R \oosA$ and
\begin{equation}
  (1/P) \sum_{t=R+1}^{T-1} D_t^2 \to^p \E_R D_T^2.
\end{equation}
Moreover, these convergence results imply that $\sh^2 - \var_R
\sqrt{P} \oosA \to^p 0$, where $\sh^2$ is the OOS variance estimator
\begin{equation}
  \sh^2 = (1/P) \sum_{t=R+1}^{T-1} (D_t - \oosA)^2.
\end{equation}
(In the general dependent case, we would need to use a HAC estimator
for consistency.)

Finally, we can apply the MDS CLT to $\oosA$ as long as $\Pr[\lim
\var_R \sqrt{P} \oosA > 0] = 1$. \citep[Theorem 3.2.]{HaH:80} In our
main results, we assume that this condition holds, but it is
straightforward to verify here. Equation~\eqref{eq:17} implies that
\begin{equation}
  \begin{split}
    \var_R \sqrt{P} \oosA &= \E_R(D_t - \E_R D_t)^2 \\
    &\geq 4 \E_R (x_{2t}' \bh{2R} - x_{1t}' \bh{1R})^2
    \quad a.s.
  \end{split}
\end{equation}
and...

\subsubsection*{Convergence of $\E_R \oosA$ to $\E_T \oosB$}

We can also use this example to study Lemma~\ref{res-convergence}.
Under this section's assumptions, $\E_R \oosA$ and $\E_T \oosB$ can be
worked with directly, and their difference is equal to
\begin{equation}
  \begin{split}
    \E_R \oosA - \E_T \oosB &= \Big\{(\bh{1R} - \theta_1)'(\bh{1R} -
    \theta_1) -
    (\bh{2R} - \theta_2)'(\bh{2R} - \theta_2)\Big\} \\
    & \quad - \Big\{(\bh{1T} - \theta_1)'(\bh{1T} - \theta_1) -
    (\bh{2T} - \theta_2)'(\bh{2T} - \theta_2)\Big\} \\
    &\begin{split}
      =\ep{T}'\Big\{
      & \tilde{X}_{1R}(X_{1R}'X_{1R})^{-2} \tilde{X}_{1R}'
      - X_{1T}(X_{1T}'X_{1T})^{-2} X_{1T}' \\
      & - \tilde{X}_{2R}(X_{2R}'X_{2R})^{-2} \tilde{X}_{2R}'
      + X_{2T}(X_{2T}'X_{2T})^{-2} X_{2T}'
      \Big\}\ep{T}.
    \end{split}
  \end{split}
\end{equation}
with $\tilde{X}_{iR}$ the $T \times K_i$ matrix $[X_{iR}'\ 0]'$.

This last term is a quadratic form and the regressors are assumed to
be normal, so its first two moments are easy to calculate. The mean
difference is
\begin{equation}
  \begin{split}
  \E(\E_R \oosA - \E_T \oosB)
      &=\E \tr \Big\{
        \tilde{X}_{1R}(X_{1R}'X_{1R})^{-2} \tilde{X}_{1R}
      - X_{1T}(X_{1T}'X_{1T})^{-2} X_{1T} \\
      & \qquad\qquad- \tilde{X}_{2R}(X_{2R}'X_{2R})^{-2} \tilde{X}_{2R}
      + X_{2T}(X_{2T}'X_{2T})^{-2} X_{2T} \Big\} \\
      &= \tr \Big\{\E (X_{1R}'X_{1R})^{-1}
         - \E (X_{1T}'X_{1T})^{-1} \Big\}
         - \tr \Big\{\E (X_{2R}'X_{2R})^{-1}
         - \E (X_{2T}'X_{2T})^{-1} \Big\}, \\
      &= \tfrac{K_1 P}{(R-K_1-1)(T-K_1-1)} - \tfrac{K_2 P}{(R-K_2-1)(T-K_2-1)}.
    \end{split}
\end{equation}
The variance of the difference is
and

and
\begin{align*}
  \var(\E_R & \oosA - \E_T \oosB) \\ &= \E
  \big[\tr((X_{2R}'X_{2R})^{-1} - (X_{2T}'X_{2T})^{-1})\big]^2 + 2 \tr
  \E\big((X_{2R}'X_{2R})^{-2} - (X_{2T}'X_{2T})^{-2}\big) \\ &\quad - \big[\tr
  \E((X_{2R}'X_{2R})^{-1} - (X_{2T}'X_{2T})^{-1}) \big]^2 \\ &=
  O(P/T)^2.
\end{align*}
So, in this example, $P^{1/2}(\E_R \oosA - \E_T \oosB) \to^p 0$
if $P^3/T^2 \to 0$, which is slightly weaker than the general
requirement that $P^2/T \to 0$.  If $\lim P^3/T^2 > 0$ the OOS average
still obeys the MDS CLT, but it is not centered correctly at $\E_T
\oosB$.

\subsubsection*{Behavior of the F-test}

[no, just assume that the regressors are deterministic]

This example illustrates the problems with in-sample forecast
evaulation especially well, because we have finite sample
distributions of the test statistics of interest, the $F$-test, AIC,
and BIC.

[text]

\begin{equation*}
  M_2 \bh{2T} \sim
  N(M_2 \theta_2, \sigma^2/T)
\end{equation*}
where $M_2 = (0\ I)$. Given $\E_T \oosB = 0$ as well as the previous
information set, and the density of $M_2 \bh{2T}$ concentrates
uniformly on the surface of the sphere centered at $M_2 \theta_2$ and
passing through the origin:
\begin{equation*}
  (M_2 \bh{2T} - M_2 \theta_2)'(M_2 \bh{2T} - M_2 \theta_2) =
  \theta_2'M_2' M_2 \theta_2.
\end{equation*}
We can also represent the null $\E_T \oosB \leq 0$ by conditioning on
$\E_T \oosB = -c$ for a fixed positive constant $c$. In that case, the
density of $M_2 \bh{2T}$ concentrates on the sphere
\begin{equation*}
  (M_2 \bh{2T} - M_2 \theta_2)'(M_2 \bh{2T} - M_2 \theta_2) =
  \theta_2'M_2' M_2 \theta_2 + c
\end{equation*}
which has the same center but larger radius. What makes both
distributions relatively easy to work with is that the estimators are
uniformly distributed on the surface of each sphere.
For other (stochastic) design matrices, the distribution of $M_2
\bh{2T}$ is not uniform on the sphere and its location may be
different, but the same general argument will hold.

For the $F$-test we have, just as in Equation~\eqref{eq:17}
\begin{equation}
  F = \tfrac{T}{s^2 (K_2 - K_1)} \bh{2T}' M_2' M_2 \bh{2T}.
\end{equation}
We know that $\sqrt{T}(F - 1)$ is asymptotically normal
\citep{Cal:11c} so the test accepts if
\begin{equation}\label{eq:26}
  \tfrac{T}{s^2 (K_2 - K_1)} \bh{2T}' M_2' M_2 \bh{2T}
  \leq 1 + \delta / \sqrt{T},
\end{equation}
where $\delta$ is chosen to determine the size of the test. In other
words, the test accepts if $M_2 \bh{2T}$ falls in the sphere centered
at the origin with radius $s ((K_2 - K_1) / T)^{1/2} +
O_p(1/\sqrt{T})$.

So if the unknown coeficients satisfy
\begin{equation*}
\theta_2'M_2'M_2\theta_2 > \var(\e_t)^{1/2} ((K_2 - K_1) / T)^{1/2}
\end{equation*}
for large enough $T$, then the probability of rejecting the null
hypothesis when it is at least as accurate as the altenative is
equivalent to the probability that $\bh{2T}$ violates~\eqref{eq:26}
given that it is uniformly distributed on the sphere
\begin{equation*}
  (M_2 \bh{2T} - M_2 \theta_2)'(M_2 \bh{2T} - M_2 \theta_2) =
  \theta_2'M_2' M_2 \theta_2 + c
\end{equation*}
for some positive $c$. Each individual element of $M_2 \bh{2T}$
independently has probability less than $1/2$ of falling in the
acceptance region, so the probability that at least one doesn't (all
that is needed for rejection) is bounded (for large enough $T$)
\begin{equation*}
  1 - (1/2)^{K_2 - K_1}
\end{equation*}
which obviously converges to 1 as $K_2 - K_1 \to \infty$. The same
bound also holds under the alternative $\E_T \oosB > 0$, so the
$F$-test rejects with probability approaching 1 for these values of
$\theta_2$, regardless of which model is more accurate.

\section{Monte Carlo}
\label{sec:mc}

This section presents two simulations that investigate the accuracy of
our theory in small samples.%
\footnote{Simulations were conducted in R \citep{Rde:10} using the
  MASS \citep{VeR:02}, Matrix \citep{BM:13}, rlecuyer \citep{SR:12},
  and RSQLite \citep{Jam:10} packages. Graphs were produced in R using
  Lattice \citep{Sar:10} and tikzDevice \citep{ShB:11:0.6.1} as well
  as the TikZ and PGF LaTeX packages \citep{Tan:10}} %
The first looks at the asymptotic normality of the DMW statistic
centered on either $\E_R \oosA$ or $\E_T \oosB$.  The previous
section's results imply that centering on the first should give an
approximately normal statistic and centering on the second should give
a normal statistic when $P$ is small relative to $T$.  The second
simulation looks at the size and power of several statistics when
conducting inference about the difference in the models'
generalization error.  For all of the simulations, we look at
fixed-window OOS tests to speed up computation time.

\subsection{Setup}
\label{sec:simulation-design}

The Monte Carlo experiment is intentionally very simple so that we can
isolate the influence of the models' complexity.  In particular, we do
not include some features that are common in forecasting
environments---serial dependence, heteroskedasticity, and complicated
DGPs.  We simulate data for both studies from the equation
\begin{equation}\label{eq:6}
  y_t = x_t'\theta + \e_t,\quad \e_t \sim N(0,1),
  \quad t=1,\dots,T.
\end{equation}
The first element of $x_t$ is 1 and the remaining $K_2-1$ elements are
independent Standard Normal.  The benchmark model is
\begin{equation}
  \label{eq:1}
  y_{1t} = \sum_{j=1}^{K_1} x_{jt}\theta_j + \e_t
\end{equation}
and the alternative model is the DGP \eqref{eq:6}.  We let
$(K_1,K_2)$ equal either $(2,3)$ or $(T/20,T/10)$ to study our theory
in its intended application as well as for more parsimonious models.
We let $T$ equal 100, 250, 500, or 1000.  We also vary $\theta$, and do
so giving the benchmark and the alternative model comparable weight in
predicting $y_t$.  Specifically, we set
\begin{equation*}
  \theta_j =
\begin{cases} \frac{c}{\sqrt{K_1}} & j = 1,\dots,K_1 \\
\frac{c}{\sqrt{K_2 - K_1}} & j = K_1 + 1,\dots,K_2 \end{cases}
\end{equation*}
with $c$ equal to zero or one.  When $c$ is one, we're more likely to
draw values of $X$ and $Y$ that make the estimated larger model more
accurate than the benchmark, and when $c$ is zero we're unlikely to
draw such values of $X$ and $Y$.  For all of the studies, $L(x) =
x^2$.

The first set of simulations study how well the theory in
Section~\ref{sec:oostheory} works in practice.  For each draw of $X$
and $Y$, we construct a one-sided OOS interval of the
form
\begin{equation*}
  [ \oosA - 1.28 \hat{\sigma}, \infty) \quad\text{with}\quad
  \sh^2 = \frac1P \sum_{t=R+1}^T (D_t - \oosA)^{2}
\end{equation*}
for $P$ set to every 10th value between 1 and $2T/3$.  We calculate
the percentage of these intervals that contain $\E_R \oosA$ and that
contain $\E_T \oosB$.  Since the data are i.i.d., both of these
quantities are easy to calculate:
\begin{equation*}
  E_R \oosA = \Bigg[\sum_{i=1}^{K_1} (\tilde{\theta}_{iR} - \theta_i)^2 +
  \sum_{i=K_1+1}^{K_2}\theta_i^2\Bigg] - \sum_{i=1}^{K_2} (\hat{\theta}_{iR} -
  \theta_i)^2
\end{equation*}
and
\begin{equation*}
  E_T \oosB = \Bigg[\sum_{i=1}^{K_1} (\tilde{\theta}_{iT} - \theta_i)^2 +
  \sum_{i=K_1+1}^{K_2}\theta_i^2\Bigg] - \sum_{i=1}^{K_2} (\hat{\theta}_{iT} -
  \theta_i)^2
\end{equation*}
where $\tilde{\theta}$ indicates that the coefficient is estimated
using only the regressors in the benchmark model and $\hat{\theta}$
the full model.  We draw 2000 samples for each combination of the
design parameters.

The second simulation looks at the size and power of the test
statistics when conducting inference about the models' generalization
error.  The null hypothesis we test is
\begin{equation}\label{eq:9}
  H_0:\quad E_T \oosB \leq 0,
\end{equation}
We look at four different statistics---the full-sample $F$-test, the
DMW $t$-test, the
OOS $t$-test using McCracken's (2007)
critical values,%
\footnote{These critical values are not published for
  $K_2-K_1>10$, so we do not report them for $K_2 = T/10$.} %
and Clark and West's (2006, 2007) Gaussian out-of-sample
statistic.%
\footnote{Clark and West (2006, 2007) derive their statistic
using the rolling window estimation scheme.  Here we use the same
statistic, but with a fixed window scheme.} %
For the $F$-test, we simply test whether the coefficients on the larger
model are nonzero.  For the out-of-sample tests, we conduct a
one-sided test of out-of-sample performance for every 10th value of
$P$ as before.

To estimate each test's size for the hypothesis \eqref{eq:9}, we draw
samples from the DGP and discard those for which \eqref{eq:9} does
not hold, until we have 2000 for each choice of the design parameters.
The estimated size is the fraction of those samples in which the test
statistic rejects.

\subsection{Results}

We discuss results for the confidence intervals first.
Figures~\ref{fig:interval-R} and~\ref{fig:interval-T} show the
coverage probability of these intervals as a function of $P$ for each
combination of $T$, $K_1$ and $K_2$, and $c$.  The gray horizontal
line shows the intervals' nominal coverage probability.  Each panel
displays the coverage for a different choice of interval, centering
term ($\E_R \oosA$ or $\E_T \oosB$) and combination of design
parameters.

Figure~\ref{fig:interval-R} gives the results for $\E_R \oosA$.  The
actual coverage is very close to the nominal coverage except when $P$
is very small.  The poor behavior for small $P$ is unsurprising, as it
simply means that the CLT is a poor approximation when the test
sample is small.  The intervals' good coverage holds even for the
parsimonious models ($K_1=1$ and $K_2= 3$) which our theory does not
necessarily apply to.

Figure~\ref{fig:interval-T} gives the results for $\E_T \oosB$.  In
rows 2, 4, 6, and 8---the overfit models---the coverage is near
nominal coverage for moderately small values of $P$.  As $P$ increases
to $2T/3$, the coverage increases to 1.  With the parsimonious model,
the coverage is near nominal coverage for all $P$ for the one-sided
interval with $c=1$, but only for moderately small $P$ when $c=0$.

The behavior for $K_2 = T/10$ is exactly what our theory predicts.
When $P^2/T$ is small, the coverage is near nominal levels.  The
behavior as $P$ increases, combined with the results for $\E_R
\oosA$, indicate that $\E_T \oosB \geq E_R \oosA$ in general.
Since
\[E_{T} \oosB = E_{R} \oosA + (E_{T} \oosB - E_{R} \oosA), \] and the
interval is approximately centered at $\E_R \oosA$, the difference
$\E_{T} \oosB - E_{R} \oosA$ adds a substantial positive quantity when
$P^2/T$ is not near zero, increasing the coverage of the one-sided
interval.

We next present the size simulations,
Figures~\ref{fig:ftest}--\ref{fig:ttest-power}.  For the OOS tests,
the conditional rejection probability is plotted for each combination
of $T$, $K_1$, $K_2$, and $c$ as a function of $P$.  The $F$-test does
not depend on $P$, so a single value is presented for each
combination.

We'll look at the $F$-test first.  To make comparisons easier, the size
is displayed as a dot plot in Figure~\ref{fig:ftest}.  Different
panels display a different combination of $K_1$, $K_2$, and $T$, and
each individual plot shows the empirical size for each choice of $c$.
We see immediately that the actual and nominal size are essentially
equal for $c = 0$, which is unsurprising.  For $c = 0$, the $F$-test is
exact; moreover, the larger model will almost always be less accurate
than the smaller one, so conditioning on $\E_T \oosB \leq 0$ is
almost unrestrictive.  When $c$ increases, though, the $F$-test
overrejects badly---rejecting at roughly 50\% when $c = 1$ for the
parsimonious model and from 70\% to 100\% for the overfit model.  As
our discussion in Section~\ref{sec:insample} predicts, the rejection
probability increases with $T$ for the overfit model but does not
depend on $T$ for the small model.

Figure~\ref{fig:ttest-size} presents the size estimates for the DMW
$t$-test.  Again, different panels display results for different
combinations of the design parameters.  Each graph plots the rejection
probability against $P/T$.  For $K/T=10$, the rejection probability
falls as $P/T$ increases, from near nominal size when $P/T$ is small
to zero when $P/T$ is near $2/3$.  Moreover, the rejection probability
falls faster when $T$ is large, as our theory predicts.  When $K=3$,
the rejection probability stays closer to nominal size, but falls with
$P/T$ for $c=0$, under-rejecting by about 5pp when $P/T = 2/3$, and
rises with $P/T$ for $c=1$, overrejecting by about 10pp when
$P/T=2/3$.  For small $P$, the rejection probability is near 10\% for
all simulations (the farthest is $K=T/10$, $c=0$, where the rejection
probability is about 5\%; the other simulations are much closer).

We observe the following patterns.  The
DMW test has close to nominal size when $P$ is small for every
combination of design parameters.  In most cases, the rejection
probability decreases as $P/T$ increases---the exception is for $K_2 =
3$ and $c=1$.  For the large-$K$ simulations, the rejection
probability drops to zero for most of the simulations as $P/T$
increases.  The rejection probability increases with $c$, but the
rejection probability still is near nominal probability for small $P$
with $c=1$.

Clark and West's (2006, 2007) statistic, presented in
Figure~\ref{fig:clarkwest}, behaves quite differently.  For $c=0$ the
test is correctly sized for both the overfit and parsimonious studies,
as we saw for the $F$-test.  When $c=1$, the rejection probability
increases rapidly with $P/T$.  For $K_2=3$, the rejection probability
is near 10\% when $P$ is small but about 40\% when $P/T = 2/3$.  For
$K=T/10$, the rejection probability is even higher and increases with
$T$ as well, from a maximum over 50\% when $T=100$ to a maximum of
nearly 100\% when $T=1000$.

Results using \citepos{Mcc:07} critical values are
presented in Figure~\ref{fig:mccracken} and are similar to those using
Clark and West's test.  For $c=0$ the rejection probability is nearly
the test's nominal size.  For $c=1$, the rejection probability
increases with $P/T$, from close to the nominal size when $P/T$ is
small to over 25\% when $P/T = 2/3$.  Note that all of the simulations
use the parsimonious model.  McCracken's statistic overrejects here
by slightly less than Clark and West's, but still by a substantial
amount.

Since the DMW test tends to have low rejection probability, the
test's power is a concern.  We'll present some power results,
simulating from \eqref{eq:1} with $c = 1$ or 2 subject to the
constraint that $\E_T \oosB > 0$.%
\footnote{Draws of $X$ and $Y$ with
  $\E_T \oosB > 0$ are very rare when $c=0$, so we do not present
  results for that value of $c$.} %
Figure~\ref{fig:ttest-power} plots
the power for the DMW test; since the other test statistics greatly
overreject, we do not present their power.  For $c=1$, the power is
never greater than nominal size and decreases to zero as $P/T$
increases for the overfit model.  For $c=2$ the power is better,
increasing with $P/T$ for a stretch and then decreasing as $P/T$ grows
beyond approximately 1/4 for the overfit model.  Larger values of $T$
give a higher peak and greater power overall, but the power still
falls to nearly zero if $P/T$ is too large (approximately 2/3 in our
simulations).  The power with the parsimonious model is typically
quite low but greater than nominal size for $c = 2$.

Both sets of simulations support our theoretical results.  The first
simulation confirms that the DMW OOS $t$-test is centered at $\E_R
\oosA$ for all choices of $P$ and $R$ and is centered on $\E_T \oosB$
only when $P$ is small.  The second simulation confirms that the DMW
test has correct size for the null hypothesis that $\E_T \oosB \leq 0$
when $P$ is small and that tests designed to test whether the
benchmark is true, like the $F$-test and Clark and West's (2006, 2007)
and McCracken's (2007) OOS tests can reject by much more than their
nominal size when testing the null $\E_T \oosB \leq 0$.  Moreover,
these simulations demonstrate that the restriction $P^2/T \to 0$ is
binding in practice, as the DMW test under-rejects and has very low
power when it is not satisfied.

\section{Empirical analysis of equity premium predictability}
\label{sec:empirics}

This section presents an analysis of equity premium predictability
similar to \citet{GoW:08}.%
\footnote{Calculations in this section are done in R \citep{Rde:10}
  using the lmtest \citep{ZeH:02} and sandwich \citep{Zei:04}
  packages. Tables and grapsh were prepared with the Hmsic
  \citep{Har:10} and tikzDevice \citep{ShB:11:0.6.1} R packages as
  well as the TikZ and PGF LaTeX packages \citep{Tan:10}} %
We estimate the expected forecasting
performance of the largest model they consider, a model with 13
regressors, using 81 observations (annual data from 1928 to 2009).  We
construct confidence intervals for the difference in performance
between this model and a prevailing mean benchmark, and do so for
different values of $R$ and $P$ to examine the effect of the training
and test sample choice on this OOS statistic.  We find that the
estimate of the larger model's Mean Squared Error (MSE) decreases relative to the
benchmark as $R$ increases, but it is never smaller than the
benchmark's.  This result, combined with the larger model's in-sample
significance, suggests that there may be a real relationship between
the equity premium and some of these predictors, but that the
relationship can not be estimated accurately enough to be useful for
forecasting and supports \citepos{GoW:08} conclusions.  These results
hold even when imposing \citepos{CaT:08} restriction that the equity
premium forecast be non-negative.

We'll start with a very brief review.  \citet{GoW:08}
study the OOS forecasting performance of different variables thought
to predict the equity premium (calculated as the difference between
the return on the S\&P 500 index and the T-bill rate).  Some of these
variables are listed in
Table~\ref{tab:equity}.%
\footnote{Table~\ref{tab:equity} only lists the
  variables used in \citepos{GoW:08} ``kitchen sink''
  model.  Some of the variables that they use in bivariate models are
  excluded from this model either because the series are too short or
  because the variables are linear combinations of other variables.} %
\citet{GoW:08} estimate the OOS forecasting
performance of bivariate models of the form
\[
r_{t+1} = \beta_0 + \beta_1 x_t + \e_{t+1}
\]
using OLS with a recursive window, where $r_{t+1}$ is the equity
premium and $x_t$ is a generic predictor, as well as some larger
models.  They find that these models forecast no better than the
prevailing mean of the equity premium.
\citet{CaT:08} find that nonlinear extensions of
Goyal and Welch's models are more accurate, either imposing sign
restrictions on the estimated coefficients or imposing that the
forecast be non-negative, but do not test for significance.

Obviously, \citepos{GoW:08} bivariate models do not
match the asymptotic theory of this paper, so we restrict our
attention here to their ``kitchen sink'' model, which includes all of
the variables, except for a few dropped because of data availability or
(perfect) multicollinearity.  Formally, the benchmark and alternative models are
\begin{eqnarray}
  \label{eq:3}
  r_{t+1} &=& \mu + \e_{1,t+1} \\
  \label{eq:2}
r_{t+1} &=& \beta_0 + \sum_{i=1}^K \beta_{i} x_{it} + \e_{2,t+1}
\end{eqnarray}
respectively and are estimated by OLS using the fixed-window scheme.
The predictors are listed in Table~\ref{tab:equity}; for a detailed
description of each predictor, please see Goyal and Welch's original
paper.  In this model, there are 13 regressors (including the constant
term) and we estimate the coefficients using annual data from 1928 to
2009.  This makes the ratio $K/T$ equal to about 0.16.  We also
present results for one of \citepos{CaT:08}
restricted forecasting models.  For this model, we estimate
\eqref{eq:3} and \eqref{eq:2}, but impose that $\hat r_{t+1}$ be
non-negative for each forecast.  We then calculate the OOS test just
as for the unconstrained model.

Table~\ref{tab:gwinsample} gives the results of the full sample
regression for Equation~\eqref{eq:2}, using Newey-West standard errors
with two lags \citep{NeW:87}.%
\footnote{We compare the test statistic
  to critical values from the $F$-distribution.} %
The $p$-value is
very small (less than 0.01), indicating that the coefficients are
nonzero in population and at least one of these predictors is
correlated with the equity premium.  As we argue earlier, this result
does not imply that the model will forecast well, which we look at
next.

To study the effect of the training sample size on the DMW statistic, we
calculate the one-sided confidence interval for $\E_T \oosB$ given
by \eqref{interval-greater} corresponding to the null and alternative
hypotheses
\[ H_0: \quad E_T \oosB \leq 0 \qquad
H_A: \quad E_T \oosB > 0
\]
using the fixed-window scheme for each value of $R$ between 20 and
$T-10$.  The standard deviation is estimated using a Newey-West
estimator with $\lfloor P^{1/4}\rfloor$ lags.  For small values of
$R$, the OOS average is expected to underestimate the performance of
the larger model relative to the smaller, but this may not hold in
this particular dataset.

Figures~\ref{fig:empirics1} plots the OLS results and
Figure~\ref{fig:empirics2} imposes \citepos{CaT:08} restriction.  The
solid line in each figure shows the OOS average, $\oosA$, and the
shaded region indicates the 95\% one-sided confidence interval implied
by the DMW test.  Negative numbers indicate that the kitchen sink
model has higher out-of-sample loss.  We can see that the same
patterns hold for both models: the performance difference decreases as
$R$ grows, but the kitchen sink model is never more accurate.  We also
see that the performance difference decreases suddenly over the period
$R=29$ to $R=34$ (corresponding to the years 1956--1961).
Figure~\ref{fig:empirics3} plots the accuracy of the individual
forecasts (only for the linear models) and shows that this change is
the result of a sudden improvement in the kitchen sink model.  This
change may indicate instability in the underlying relationship, as
\citet{GoW:08} propose.

In summary, we fail to reject the null that the benchmark prevailing
mean model is more accurate than \citepos{GoW:08} kitchen sink.  This
result is consistent with Goyal and Welch's original analysis.  Unlike
Goyal and Welch, we attribute this result, at least in part, to
parameter uncertainty---the full sample results indicate that the
larger model could predict better than the benchmark with enough
data.%
\footnote{\citet{BWB:10} make a similar point about exchange rate
models, but see also \citet{Chi:10} and \citet{Gia:10}.} %
Obviously, this is an illustrative exercise only and is not meant to
be comprehensive.  In this dataset, there is likely parameter
instability that we have not addressed, and we have not dealt with
variable persistence at all.  However, as we mention earlier, our
requirement that $P$ be small should let our theoretical results
extend to moderate instability, and the fact that we do not rely on
the asymptotic distribution of the coefficient estimates should
mitigate the effect of persistence as well.

\section{Conclusion}
\label{sec:conclusion}

This paper gives a theoretical motivation for using OOS comparisons:
the DMW OOS test allows a forecaster to conduct inference about
the expected future accuracy of his or her models when one or both is
overfit.  We show analytically and through Monte Carlo that standard
full-sample test statistics can not test hypotheses about this
performance.

Our paper also shows that popular test and training sample sizes may
give misleading results if researchers are concerned about overfit.
We show that $P^2/T$ must converge to zero for the DMW test to give
valid inference about the expected forecast accuracy, otherwise the
test measures the accuracy of the estimates constructed using only the
training sample.  In empirical research, $P$ is typically much larger
than this.  Our simulations indicate that using large values of $P$
with the DMW test gives undersized tests with low power, so this
practice may favor simple benchmark models too much.  Existing
corrections, proposed by \citet{ClM:01,ClM:05}, \citet{Mcc:07} and
\citet{ClW:06,ClW:07}, seem to correct too much, though, and reject
too often when the benchmark model is more accurate.

More work remains.  The requirement that $P^2/T$ converge to zero is
limiting, as it implies that in typical macroeconomic datasets, only a
handful of observations should be used for testing.  This requirement
can be relaxed only slightly; $P = O(T^{1/2})$ is required for the
OOS test to have nontrivial power in general, but there are loss
functions and DGPs for which some relaxation is possible.  This
constraint could be mitigated by extending our results to
cross-validation or other resampling strategies, or by constructing
full-sample statistics that allow inference about $\E_T \bar{D}_T$.
It would also be useful to extend our results to other forecasting
models and to explore how stationarity could be relaxed, but such
extensions are less important than improving the available statistics.

\appendix
\section*{Appendix: mathematical details}
\setcounter{section}{1}
% Change lemma style to prepend 'A'
\setcounter{lem}{0}
\renewcommand{\thelem}{A\arabic{lem}}

\subsection*{Supporting results}
The results in this paper rely heavily on a coupling argument for
absolutely regular sequences, Berbee's Lemma \citep{Ber:79}.  Many of
the results of this paper (Lemma \ref{res-mixingale} and
Theorem~\ref{res-oost}) use modifications of existing results
for NED functions of mixing processes by \citet{Jon:97} and
\citet{JoD:00}; this coupling argument is used to explicitly derive
inequalities that arise naturally for NED processes.  Lemma
\ref{lem-basic-coupling} establishes these inequalities, which are
based on a proposition of \citet{MeP:02}.

We present \citeauthor{MeP:02}'s
(\citeyear{MeP:02}) statement of Berbee's Lemma for
the reader's reference.  In the following Lemma, $\beta(X,Y)$ is the
coefficient of absolute regularity:
\[
\beta(X,Y) = \sup_{A \in \sigma(Y)} \E \lvert \Pr(A \mid \sigma(X))
  - \Pr(A) \rvert.
\]
\begin{lem}\label{lem-berbee}\quad

\begin{quotation}\noindent
  Let $X$ and $Y$ be random variables defined on a probability space
  $(\Omega, \mathcal{T}, \Pr)$ with values in a Polish space
  $S$.  Let $\sigma(X)$ be a $\sigma$-algebra generated by $X$ and let
  $U$ be a random variable uniformly distributed on $[0,1]$
  independent of $(X,Y)$.  Then there exists a random variable $Y^{*}$
  measurable with respect to $\sigma(X) \vee \sigma(Y) \vee
  \sigma(U)$, independent of $X$ and distributed as $Y$, and such that
  $\Pr(Y \neq Y^{*}) = \beta(X,Y)$.

  \noindent\citep{MeP:02}
\end{quotation}
\end{lem}

The advantage of this result over coupling arguments that use other
forms of weak dependence is that the difference between the original
variable, $Y$, and the new variable, $Y^{*}$, does not depend on their
dimension.  Similar results for strong mixing sequences depend on the
dimension of $Y$, which makes them unsuitable for this paper.

\begin{lem}\label{lem-extend-mp}
  Suppose that $X$ and $X^*$ are $L_p$-bounded random variables, with
  $p > 2$, that satisfy ${\Pr[X \neq X^*] = c}$.  Then
  \[
    \lVert X - X^* \rVert_2 \leq 2^{1/p} (\lVert X \rVert_p + \lVert
    X^* \rVert_p) c^{(p-2)/2p}
  \]
\end{lem}

The proof is virtually identical to the proof of Proposition 2.3 in
\citet{MeP:02} and is omitted.

\begin{lem}\label{lem-basic-coupling}
  Suppose Assumptions \ref{asmp-1}--\ref{asmp-3} hold.  Then, for any
  T, $s$, $t$, and $u$ with $s < t \leq u$, there exist random
  variables $D_t^*,\dots,D_u^*$ such that
  \begin{equation}\label{eq:coupling1}
    P[(D_{t}^*,\dots,D_u^*) \neq (D_{t},\dots,D_u)] \leq \beta_{t-s}
  \end{equation}
  and
  \begin{equation}\label{eq:coupling2}
    \E(\phi(D_{t}^*,\dots, D_u^*) \mid \mathcal{F}_s ) =
    \int
    \phi(D_{t},\dots,D_u) f(\mathbf{x}, \mathbf{y})\ d\mathbf{x}\ d\mathbf{y}
  \end{equation}
  almost surely for all measurable functions $\phi$ such that the
  expectations are finite, where
  \[ \mathbf{x} = (x_{t}', \dots, x_{u}')', \qquad
  \mathbf{y} = (y_{t+\h},\dots,y_{u+\h})',\] and $f$ is the
  joint density of $(\mathbf{x}, \mathbf{y})$.  Moreover,
 \begin{equation}\label{eq:coupling3}
   \| D_v^* - D_v \|_2 \leq 2^{1+1/\rho} B_L
   \beta_{t-s}^{\rhoExp}, \qquad v = t,\dots,u.
 \end{equation}
\end{lem}

\begin{proof}
  The proof follows as a consequence of Lemmas \ref{lem-berbee} and
  \ref{lem-extend-mp}.  Let $l = u-t$.  For any fixed values of $l$
  and $T$, the sequence of vectors
  \[ V_{t} = (y_{t+\h}, x_{t}', \dots, y_{t+l+\h}, x_{t+l}') \] is
  absolutely regular of size $\rho/(\rho-2)$.  Berbee's Lemma implies
  that there is a random vector $V^*$ that is independent of
  $\mathcal{F}_s$, equal to $V_{t}$ in distribution, and satisfies
  \[\Pr[V^* \neq V_{t}] = \beta_{t-s}.\]

  Now define
  \begin{equation*}
    D_v^* =
    \begin{cases}
      L(y_{v+\h}^* - x_{1v}^{*\prime} \hat{\theta}_{1R}) - L(y_{v+\h}^* -
      x_{2v}^{*\prime} \hat{\theta}_{2R}) & v \leq T \\
      L(y_{v+\h}^* - x_{1v}^{*\prime} \hat{\theta}_{1T}) - L(y_{v+\h}^* -
      x_{2v}^{*\prime} \hat{\theta}_{2T}) & v > T
    \end{cases}
  \end{equation*}
  with $y_{v+\h}^*$ and $x_{iv}^*$ denoting the elements of $V^*$
  corresponding to $y_{v+\h}$ and $x_{iv}$ in $V_{t}$.  Equations
  (\ref{eq:coupling1}) and (\ref{eq:coupling2}) are satisfied by
  construction, and (\ref{eq:coupling3}) follows from Lemma
  \ref{lem-extend-mp}.
\end{proof}

\begin{lem}\label{lem:a2}
  Suppose Assumptions~\ref{asmp-1}--\ref{asmp-3} hold.  Let $b_T$ be a
  sequence of integers such that $b_T \to \infty$ and $b_T = o(P)$ and define
  \begin{equation*}
    Z_i = \ZDef.
  \end{equation*}
  Then
  \begin{equation}
    \label{eq:10}
    \sum_{i=1}^{\lfloor P/b_T \rfloor} (\E_R Z_i^2 - \E_{R+(i-1) b_T}
    Z_i^2) \to^p 0 \quad\text{ as } P \to \infty.
  \end{equation}

  If the assumptions of Theorem~\ref{res-oost} also hold, $b_T$ is
  restricted further so that $b_T \equiv \lfloor \gamma/\delta
  \rfloor$ for some positive scalar $\delta$, and we define
  \begin{equation}\label{eq:13}
    \kernelB{x} \equiv \kernelBDefn{x},
  \end{equation}
  then
  \begin{equation}
    \label{eq:12}
    \vtSum (Z_{1t} Z_{2t} - \E_R Z_{1t} Z_{2t}) \to^p 0.
  \end{equation}
  where
  \begin{gather}
    Z_{1t} = \varianceTermIIIa,%
    \intertext{and}%
    Z_{2t} = \varianceTermIVb.
  \end{gather}
\end{lem}
\begin{proof}
\newcommand{\UFiltration}[1]{\ensuremath{\mathcal{F}_{(#1)b_{T}+R-P}}}%
The first result,~\eqref{eq:10}, follows a similar argument to Lemma~5
of \citet{Jon:97} and~\eqref{eq:12} to Lemma~A.4 of \citet{JoD:00}.
Since these arguments are similar and our modification is the same for
both, we'll just present the more complicated version,~\eqref{eq:12}.

Note that $\{Z_{1t}^2 P\gamma_T/b_T\}$ and $\{Z_{2t}^2 P\gamma_T/b_T\}$
are uniformly integrable.  As in~\citet[Lemma A.4]{JoD:00}, we can
assume that there is a constant $C$ such that $Z_{1t}$ and $Z_{2t}$
are bounded in absolute value by $C\sqrt{b_T/P\gamma_T}$; uniform
integrability ensures that the difference between the unbounded random
variables and these truncated versions is negligible for large enough
values of $C$.

Let $r = \lfloor 3P/2b_T \rfloor$ and rewrite the summation as
\begin{align*}
  \vtSum \vtIIIsummand &= \vtSumr \vtSuma \vtIIIsummand \\
  &\quad+ \vtSumr \vtSumb \vtIIIsummand \\
  &\quad+ \sum_{t=r b_T - P + R + 1}^{2P+R} \vtIIIsummand \\
  &\equiv \vtSumr (U_i - \E_R U_i) + \vtSumr (U_i' - \E_R U_i') + o_{L_1}(1).
\end{align*}
The proof then holds if we can show that both $U_i$ and $U_i'$ obey
LLNs.  We'll do so by proving that $\{U_i -\E_R U_i,
\UFiltration{2i-1}\}$ and $\{U_i' - \E_R U_i', \UFiltration{2i}\}$ are
$L_2$-mixingales of size $-1/2$ and using the bound $\E(\vtSumr (U_i -
\E_R U_i))^2 = O(\vtSumr c_i^2)$ where the $c_i$ are the mixingale
magnitude indices \citep{Mcl:75}.

For non-negative $m$, we have
\begin{equation*}
U_i - \E_R U_i \in \UFiltration{2i+2m-1},
\end{equation*}
establishing half of the mixingale result trivially.  Now fix $i$ and $m >
0$ and use Lemma \ref{lem-basic-coupling} to define $D_{ts}^*$
for each $t =(2i-2)b_T-P+R+1,\dots,(2i-1)b_T-P+R$ and $s =
\max(t-b_T,R+1),\dots,\min(t+b_T,T-\h)$ such that
\begin{equation*}
 \E_R D_{ts}^* = \E_{(2i-2m-1)b_T+R-P} D_{ts}^* \quad a.s.
\end{equation*}
and
\begin{equation*}
  \lVert D_{ts}^* - D_s \rVert_2 \leq \couplingBound{s - (2i-2m-1)b_T+P}.
\end{equation*}
Also define
\begin{equation*}
  Z_{1t}^* = (P\gamma_T)^{-1/2} \sum_{l=\vttLower}^{\vttUpper}
  (D_{t,t+l}^* - \E_R D_{t,t+l}^*)\ W(l/\gamma_T), \\
\end{equation*}
and
\begin{equation*}
Z_{2t}^* = (P\gamma_T)^{-1/2} \sum_{j=\vttLower}^{\vttUpper}
    (D_{t,t+l}^* - \E_R D_{t,t+l}^*)\ \kernelB{j/\gamma_T}.
\end{equation*}

Now, we have the inequalities
\begin{align*}
  \lVert \E( U_i - \E_R U_i & \mid \UFiltration{2i-2m-1}) \rVert_2
  \\ &\leq
  \vtSuma \lVert \E(Z_{1t}Z_{2t} \mid \UFiltration{2i-2m-1}) - \E_R
  Z_{1t}Z_{2t} \rVert_2 \\
  &= \vtSuma \lVert \E(Z_{1t}Z_{2t} \mid \UFiltration{2i-2m-1}) \\
  &\quad - \E(Z_{1t}^* Z_{2t}^* \mid \UFiltration{2i-2m-1}) \\
  &\quad + \E(Z_{1t}^* Z_{2t}^* \mid \UFiltration{2i-2m-1})
  - \E_R Z_{1t}Z_{2t} \rVert_2 \\
%  \leq \vtSuma (\wall \lVert \wall \E(Z_{1t}Z_{2t} \mid \UFiltration{2i-2m-1}) \\
%  - \E(Z_{1t}^*Z_{2t}^* \mid \UFiltration{2i-2m-1}) \rVert_2\return
%  \\ + \lVert \E_R Z_{1t}Z_{2t} - \E_R Z_{1t}^*Z_{2t}^*
%  \rVert_2)\return \\
  &\leq 2 \vtSuma \lVert Z_{1t} Z_{2t} - Z_{1t}^* Z_{2t}^*
  \rVert_2 \\
  &\leq 2 \vtSuma (\lVert Z_{1t} - Z_{1t}^* \rVert_2 \lVert
  Z_{2t} \rVert_{\infty}
  + \lVert Z_{2t} - Z_{2t}^* \rVert_2 \lVert Z_{1t}^* \rVert_{\infty}) \\
  &\leq \frac{2 C b_T^{1/2}}{(P\gamma_T)^{1/2}} \vtSuma (\lVert Z_{1t} - Z_{1t}^* \rVert_2
  + \lVert Z_{2t} - Z_{2t}^* \rVert_2).
\end{align*}

And we can finish the proof with the following inequalities:
\begin{align*}
  \frac{2 C b_T^{1/2}}{(P \gamma_T)^{1/2}} &\vtSuma \lVert Z_{1t} - Z_{1t}^* \rVert_2 \\& \leq
  \frac{4C b_T^{1/2}}{P\gamma_T} \vtSuma \sum_{l=\vttLower}^{\vttUpper} \lVert
  D_{t+l} - D_{t,t+l}^* \rVert_2 W(l/\gamma_T) \\&
  \leq O\Bigg(\frac{b_T^{1/2}}{P\gamma_T}\Bigg) \vtSuma
  \sum_{l=\vttLower}^{\vttUpper} \couplingBeta{t+l-(2i-2m-1)b_T+P}\\&
  = O\Bigg(\frac{b_T^{1/2}}{P\gamma_T}\Bigg) O(b_T^{3/2-u}\, m^{-1/2-u})
\end{align*}
for some positive $u$.  The same argument holds for $Z_{2t}$.
As a result,
\[
E\Big(\vtSumr (U_i - \E_R U_i)\Big)^2 = o\Big(\vtSumr b_T^2/P\gamma_T\Big) =
 o(b_T/\gamma_T) \to 0,
\]
as required.
\end{proof}

\begin{lem}\label{lem:a6}
  Suppose the conditions of Theorem~\ref{res-oost} hold.
  Then
  \begin{equation*}
    \sh^2 - P^{-1} \sum_{s,t=R+1}^{T-h} (D_s - \E_R
    D_s) (D_t - \E_R D_t) W((t-s)/\gamma) \to^{L_1} 0.
  \end{equation*}
\end{lem}

\begin{proof}
   It follows from simple algebra that
\begin{multline*}
  \Big| \sh^2 -  P^{-1} \sum_{s,t=R+1}^{T-h} (D_s - \E_R
    D_s) (D_t - \E_R D_t) W((t-s)/\gamma) \Big| \leq\\
  \varianceDiffA \\ + P^{-1} \oosSum{s,t}{1} \lvert (D_s -
  \oosA)(\E_R D_t - \oosA) \rvert \vWeight + o_p(1).
\end{multline*}
We'll prove that these two sums are $o_p(1)$; uniform integrability
then implies convergence in $L_1$.  The arguments for each are almost
identical, so we'll only present the first.

Applying the Cauchy-Scwarz inequality twice and simplifying gives the
upper bound
\begin{multline*}
\varianceDiffA \\ \leq O(1) \Big[\varianceDiffAii\Big]^{1/2} \Big[\varianceDiffAi\Big]^{1/2}.
\end{multline*}
Now,
$P^{-1} \oosSum{s}{1}(D_s - \E_R D_s)^2 = O_p(1)$, and it suffices to prove
that \[\varianceDiffAi = o_p(1).\]  Observe that
\begin{align*}
  \varianceDiffAi & = O_p\Big(P^{-1} \oosSum{s}{1}(\E_{R}D_s -
  \E_R\oosA)^2\Big) + O_p(\oosA - \E_R\oosA)^{2} \\
  &= O_p\Big(P^{-1} \oosSum{s}{1} (\E_RD_s)^2 - (\E_R\oosA)^2\Big) +
  o_p(1),
\end{align*}
with the second term $o_p(1)$ by Lemma \ref{res-mixingale} and
\citepos{Dav:93} mixingale LLN.

Now define $D_s^*$, $s = R+1,\dots,T-\h$, as in Lemma
\ref{lem-basic-coupling} so that $\E_{s-1} D_s^* = \E_R D_s^*$ almost
surely.  Note that we also have the equality $\E_R D_s^* = \E_R
D_{R+1}^*$ almost surely for all $s\geq R+1$, and so
\[
P^{-1} \oosSum{s}{1} (\E_R D_s^*)^2 = \Big(P^{-1} \oosSum{s}{1} \E_R
D_s^*\Big)^2 \quad \text{a.s.}
\]
Consequently,
\begin{align*}
P^{-1} \oosSum{s}{1}& (\E_R D_s)^2 - (\E_R\oosA)^2 \\
&= P^{-1} \oosSum{s}{1} [(\E_R D_s)^2 - (\E_R D_s^*)^2]
 + \Big(P^{-1} \oosSum{s}{1} \E_R D_s^*\Big)^2 - (\E_R\oosA)^2 \quad \text{a.s}.\\
&= O_p\Big(P^{-1} \oosSum{s}{1} [(\E_R D_s)^2 - (\E_R D_s^*)^2]\Big)
 + O_p\Big(P^{-1} \oosSum{s}{1} \E_R (D_s - D_s^*)\Big).
\end{align*}
Finally,
\begin{align*}
\Big\lVert P^{-1} \oosSum{s}{1} [(\E_RD_s)^2 - (\E_R D_s^*)^2] \Big\rVert_1
&\leq P^{-1} \oosSum{s}{1} \lVert \E_R (D_s - D_s^*) \rVert_2 \lVert
\E_R (D_s + D_s^*) \rVert_2
\\ &\leq (4 B_L/P) \oosSum{s}{1} \lVert \E_R (D_s - D_s^*) \rVert_2,
\end{align*}
and this last term vanishes as in the proof of Lemma~\ref{lem:a2},
completing the proof.
\end{proof}

\subsection*{Proof of Lemma \ref{res-mixingale}}
We will start by proving that $\{D_t - \E_R D_t, \Fs_t\}$ is an
$L_2$-mixingale of size $-1/2$; $D_t$ is $\Fs_t$-measurable and so it
suffices to prove that
\begin{equation}\label{eq:24}
  \| \E_{t-l} D_t  - \E_R D_t \|_2 \leq 2^{2 + 1/\rho} B_L \, \couplingBeta{l}
\end{equation}
for $l = 1,\dots,t-R$, since $\couplingBeta{l} = O(l^{-1/2 - \delta})$
for some $\delta > 0$ by assumption. Fix $R$, $t$ and $l$ and use
Lemma \ref{lem-basic-coupling} to define $D_t^*$ so that
\[
\E_R D_t^* = \E_{t-l} D_t^* \quad a.s.
\]
and
\[
\lVert D_t - D_t^* \rVert_2 \leq \couplingBound{l}.
\]
Then
\begin{align*}
\lVert \E_{t-l} D_t - \E_R D_t \rVert_2 & \leq
\lVert \E_{t-l} D_t - \E_{t-l} D_t^* \rVert_2  + \lVert \E_{t-l} D_t^* - \E_R D_t \rVert_2 \\
&\leq 2 \lVert D_t - D_t^* \rVert_2 \\
&\leq 2^{2 + 1/\rho}\ B_L\ \couplingBeta{l}.\tag*{\qed}
\end{align*}

\noindent Asymptotic normality follows from~\eqref{eq:24} using a
modification of \citepos{Jon:97} CLTs for mixingale and NED arrays.
Define
\[\label{eq:z1}
  Z_i = \ZDef
\]
where $b_T$ is a sequence that satisfies $b_T\leq P$,
$b_T\to\infty$, and $b_T/P\to 0$.  The same arguments used in
\citepos{Jon:97} Theorem 1 show that
\begin{equation*}
   \SumOuterBlock{i} Z_i = P^{-1/2} \oosSum{s}{1} (D_t - \E_R D_t) + o_p(1).
\end{equation*}
and
\begin{equation*}
  \SumOuterBlock{i} Z_i = \SumOuterBlock{i} (Z_i - E_{R + (i-1)b_T}
  Z_i) + o_p(1).
\end{equation*}
Note that $\{Z_i - E_{R + (i-1)b_T} Z_i,\mathcal{F}_{R + i b_T}\}_i$ is an
MDS by construction, so \citepos{HaH:80} Theorem 3.2 and Corollary
3.1 ensure that $\sigma^{-1} \SumOuterBlock{i} Z_i \to^d N(0,1) $ as
long as
\begin{equation}\label{eq:cltvar2}
  \sigma^2 - \SumOuterBlock{i} \E_R Z_i^2 \to^p 0,
\end{equation}
and
\begin{equation}\label{eq:cltvar1}
  \SumOuterBlock{i} \E_R Z_i^2 - \SumOuterBlock{i} \ZSqCE \to^p 0.%
\footnote{Note that $\sigma^2 \in \mathcal{F}_t$ for
    all $t \geq R$, so Hall and Heyde's condition (3.21) is
    unnecessary---see the remarks after their result.} %
\end{equation}
Equation (\ref{eq:cltvar2}) holds as in \citet{Jon:97} (see the proof
of his Theorem 2); (\ref{eq:cltvar1}) is ensured by
Lemma~\ref{lem:a2}. \qed

\subsection*{Proof of Lemma \ref{res-convergence}}
\newcommand{\resConvgRHS}[1]{\ensuremath{\E(L(y^* - x_1^{*\prime}\bh{1#1}) - L(y^{*} -
x_2^{*\prime}\bh{2#1}) \mid \bh{#1})}}
\newcommand{\resConvgEstDiff}[1]{\ensuremath{\E(L(y^{*} -
    x_{#1}^{*\prime}\bh{#1R}) \mid \bh{R}) -
\E(L(y^{*} - x_{#1}^{*\prime}\bh{#1T}) \mid \bh{T})}}
\newcommand{\resConvgEstDiffRV}[1]{\ensuremath{L(y^{*} -
    x_{#1}^{*\prime}\bh{#1R}) - L(y^{*} - x_{#1}^{*\prime}\bh{#1T})}}

For the second part:
\begin{quotation}
  Notice that, if the observations are i.i.d. and $(y_{t+h}, x_{it})
  \sim N(0,I)$, $(\hat{\theta}_{iR} - \hat{\theta}_{iT}) \sim N(0,
  (X_{iR}'X_{iR})^{-1} - (X_{iT}'X_{iT})^{-1})$ given $X_{iT}$, and so
  $\var(x_{i,T+1}'\hat{\theta}_{iR} - x_{i,T+1}' \hat{\theta}_{iT})
  \gg P K_i / (R - K_2)(T - K_2)$.
\end{quotation}

Equation~\eqref{eq:7} holds if we show
\begin{gather}
\E_R \oosA = \resConvgRHS{R} + o_p(P^{-1/2}),\label{eq:18}\\
\E_T \oosB = \resConvgRHS{T} + o_p(Q^{-1/2}),\label{eq:19}
\intertext{and}
\resConvgEstDiff{i} = O_p(\sqrt{P/T}),\label{eq:20}
\end{gather}
where $\hat\theta_R = (\hat\theta_{1R}, \hat\theta_{2R})$,
$\hat\theta_T = (\hat\theta_{1T}, \hat\theta_{2T})$, and $y^{*}$,
$x_1^{*}$ and $x_2^{*}$ are random variables drawn from the joint
distribution of $(y_{t+\h},x_{1t},x_{2t})$ independently of
$\mathcal{F}_T$.

\begin{proof}[Proof of~\eqref{eq:18} and~\eqref{eq:19}]
For~\eqref{eq:18}, define $D_t^*$ for each $t=R+1,R+2,\dots,T-h$ so
that
\begin{equation*}
  \| D_t^* - D_t \|_2 \leq 2^{(1+\rho)/\rho} B_L \beta_{t-R}^{(\rho-2)/2\rho}
\end{equation*}
and
\begin{equation*}
  \E_R D_t^* = \E(L(y^* - x_1^{*\prime}\hat\theta_{1R}) -
  L(y^* - x_2^{*\prime}\hat\theta_{2R}) \mid \hat\theta_R) \qquad a.s.
\end{equation*}
Lemma~\ref{lem-basic-coupling} ensures that these $D_t^*$ exist.
Now,
\begin{align*}
  \Big\lVert \E_R \oosA - \E\Big(P^{-1} \sum_{t=R+1}^{T-h} D_t^* \mid
  \hat\theta_R\Big) \Big\rVert_2
  &= \Big\lVert \E_R\Big( \oosA - P^{-1} \sum_{t=R+1}^{T-h} D_t^*
  \Big) \Big\rVert_2 \\
  &\leq P^{-1} \sum_{t=R+1}^{T-h} \lVert D_t - D_t^* \rVert_2\\
  &= O(P^{-1}) \sum_{t=R+1}^{T-h} \beta_{t-R}^{(\rho-2)/2\rho}
\end{align*}
and this last term is $o(P^{-1/2})$ by assumption.
Essentially the same argument proves~\eqref{eq:19} as well.
\end{proof}

\begin{proof}[Proof of~\eqref{eq:20}]
Assumption~\ref{asmp-3} and the definition of the OLS estimator
ensure that
\begin{align*}
  \|L(y^{*} -  x_i^{*\prime} \bh{iR}) - L(y^{*} - x_i^{*\prime} \bh{iT}) \|_1
  &\leq B_{L} \| x_i^{*\prime} (\bh{iR} - \bh{iT}) \|_2 \\
  &\leq B_{L} \, \| x_i^{*\prime} [(X_{iT}'X_{iT})^{-1} -
  (X_{iR}'X_{iR})^{-1}] X_{iR}' \eb_{iR} \|_2 \\
  &\quad+ B_L \, \| x_i^{*\prime}
  (X_{iT}'X_{iT})^{-1}[X_{iT}'\eb_{iT} - X_{iR}
  \eb_{iR} ] \|_2 .
\end{align*}

Define $V = (X_{iT}'X_{iT})^{-1} - (X_{iR}'X_{iR})^{-1}$ and $W =
X_{iR}'\eb_{iR} \eb_{iR}X_{iR}$. The first term in the upper bound
satisfies
\begin{align}
  \| x_i^{*\prime} V X_{iR}' \eb_{iR} \|_2^2
  &= O(1) \cdot \E \tr\big( V^2 W \big) \\
  \label{eq:14}
  &= O(1) \big( \E \tr\{ V^2 \E(W \mid X_{iR}'X_{iR})\}
  + \E \tr\{ V^2 (W - \E(W \mid X_{iR}'X_{iR}))\}\big)
\end{align}
since $\eigen_{K_i}(\E x_i x_i') = O(1)$ by assumption.

To show that the first term in~\eqref{eq:14} converges to zero at the
right rate, observe that
\begin{equation}
  \E \tr\{ V^2 \E(W \mid X_{iR}'X_{iR})\}
  \leq \bigg\rVert
  \eigen_{K_i}(\E(W \mid X_{iR}'X_{iR})) \bigg\rVert_3
  \bigg\rVert \sum_{i=1}^{K_i} \eigen_i^2 \big(V\big) \bigg\rVert_{3/2}.
\end{equation}
The first term in this product is $O(R)$ by
Assumption~\ref{asmp-2}. For the second term, observe that
$(X_{iT}'X_{iT})^{-1} - (X_{iR}'X_{iR})^{-1}$ has rank $P$ and
each of its nonzero eigenvalues are bounded in absolute value by the
eigenvalues of $(X_{iR}'X_{iR})^{-1}$. The eivenvalues of
$(X_{iR}'X_{iR})^{-1}$ are $O_{L_3}(1/R)$ by Assumption~\ref{asmp-2}, so
\begin{equation*}
  \Big\|\sum_{j=1}^{K_i}
  \eigen_j^2(V)\Big\|_{3/2}
  \leq \Big\|\sum_{j=K_i - P+1}^{K_i} \eigen_j^2(V) \Big\|_{3/2}
  = O(P/R^2)
\end{equation*}
and, consequently,
\begin{equation*}
  \E \tr\bigg\{
  [(X_{iT}'X_{iT})^{-1} - (X_{iR}'X_{iR})^{-1}]^2
  \E(X_{iR}'\eb_{iR} \eb_{iR}X_{iR} \mid X_{iR}'X_{iR})\bigg\} = O(P/R).
\end{equation*}

For the second term in~\eqref{eq:14}, define for each $s=\h+1,\dots,R$,
\begin{equation*}
  w_{is} = x_{i,s-\h} x_{i,s-\h}' \e_{i,s}^2 + \1\{s > \h+1\}
  \sum_{t=\h+1}^{s-1} (x_{i,s-\h} x_{i,t-\h}' + x_{i,t-\h} x_{i,s-\h}')
  \e_{i,s} \e_{i,t}
\end{equation*}
and observe that $W = \sum_{s=\h+1}^{R} w_{is}$. Now use
Lemma~\ref{lem-berbee} to define new random variables $w_{is}^*$ for
each $s$ that are independent of $\sigma(x_{R+1},\dots,x_{T-\h})$ and
satisfy
\begin{equation*}
  \Pr[w_{is} \neq w_{is}^*] \leq \beta_{R -\h + 1- s}.
\end{equation*}
By construction,
\begin{equation*}
  \E\bigg(\sum_{t=\h+1}^R w_{it}^*\ \big|\ X_{iR}'X_{iR}, X_{iT}'X_{iT}\bigg)
  =  \E\bigg(\sum_{t=\h+1}^R w_{it}^*\ \big|\ X_{iR}'X_{iR}\bigg) \quad\text{a.s.}
\end{equation*}
and we can rewrite the second term in~\eqref{eq:14} as
\begin{align*}
  \E \tr&\{ V^2 (W - \E(W \mid X_{iR}'X_{iR}))\}\big) \\
  &= \E \tr\bigg\{V^2 \sum_{t=\h+1}^R \big(\E(w_{it} - w_{it}^* \mid
  X_{iR}'X_{iR}, X_{iT}'X_{iT}) - \E(w_{it} - w_{it}^* \mid
  X_{iR}'X_{iR})\big) \Bigg\} \\
  &= \E \sum_{t=\h+1}^R \big(\tr\{V^2 (w_{it} - w_{it}^*)\}
  - \E(\tr\{\E(V^2 \mid X_{iR}'X_{iR}) (w_{it} - w_{it}^*)\} \mid
  X_{iR}'X_{iR}) \big) \\
  &= \E \sum_{t=\h+1}^R \big(\tr\{(w_{it} - w_{it}^*)
  (V^2 - \E(V^2 \mid X_{iR}'X_{iR})\} \big)
\end{align*}
where these results come from repeated use of the LIE along with
linearity of the trace and expectation operators. By construction,
\begin{equation*}
  \Pr\big[\tr\{w_{it} (V^2 - \E(V^2 \mid X_{iR}'X_{iR}))\}
  \neq \tr\{w_{it}^* (V^2 - \E(V^2 \mid X_{iR}'X_{iR}))\}\big]
  \leq  \beta_{R -\h + 1- s}.
\end{equation*}

[NEED TO FINISH HERE]

A similar argument proves that
\begin{equation*}
  \big\| x_i^{*\prime} (X_{iR}'X_{iR})^{-1}
  [X_{iT}'\eb_{iT} - X_{iR} \eb_{iR} ] \big\|_2 =  O(\sqrt{P/R}),
\end{equation*}
completing the proof.
\end{proof}

\subsection*{Proof of Theorem \ref{res-oost}}
We can rewrite the centered \oost\ statistic as
\[
\sqrt{P}(\oosA - \E_T \oosB)/\sh
= \frac{\sigma}{\sh}\, \Big( \sqrt{P}(\oosA - \E_R \oosA)/\sigma
+ \sqrt{P}(\E_R \oosA - \E_T \oosB)/\sigma \Big)
\]
so Lemma \ref{res-mixingale} and \ref{res-convergence} ensure that
this term is asymptotically standard normal as long as $\sigma/\sh
\to^p 1$. Since $\sigma$ is almost surely positive, this convergence
is equivalent to $\sigma^2 - \sh^2 \to^p 0$.

The proof that $\sigma^2 - \sh^2 \to^p 0$ follows \citepos{JoD:00}
Theorem 2.1 closely.  We start by defining similar quantities to
theirs, borrowing their notation when possible to make the
similarities apparent. Let $b_T \equiv \lfloor \gamma/\delta \rfloor$,
define $\kernelB{x}$ as in \eqref{eq:13}, and define the following
terms as in \citet{JoD:00}:
\begin{align*}
  \varianceTermI &\equiv
  \varianceTermIDefn,\\ \varianceTermII &\equiv \vtSum
  \varianceTermIIa\\& \quad \times \varianceTermIIb,\\
  \varianceTermIII &\equiv \vtSum \varianceTermIIIa\\\displaybreak
  &\quad \times \varianceTermIIb,\\
  \varianceTermIV &\equiv \vtSum \varianceTermIIIa \\
  &\quad\times \varianceTermIVb.
\end{align*}
These definitions give the inequalities
\begin{align*}
  \lVert \sh^2 - \sigma^2 \rVert_1 &
  \leq \lVert \sh^2 - \varianceTermI \rVert_1
  + \lVert \varianceTermI - \varianceTermII \rVert_1
  + \lVert \varianceTermII - \varianceTermIII \rVert_1
  + \lVert \varianceTermIII - \varianceTermIV \rVert_1\\ & \quad
  + \lVert \varianceTermIV - \E_R \varianceTermIV \rVert_1
  + \lVert \E_R\varianceTermIII - \E_R\varianceTermIV \rVert_1
  + \lVert \E_R\varianceTermII - \E_R\varianceTermIII \rVert_1\\ & \quad
  + \lVert \E_R\varianceTermI - \E_R\varianceTermII \rVert_1
  + \lVert \E_R \varianceTermI - \sigma^2 \rVert_1
  \\ &
  \leq  \lVert \sh^2 - \varianceTermI \rVert_1
  + 2(\lVert \varianceTermI - \varianceTermII \rVert_1
      + \lVert \varianceTermII - \varianceTermIII \rVert_1
      + \lVert \varianceTermIII - \varianceTermIV \rVert_1) \\
  & \quad + \lVert \varianceTermIV - \E_R \varianceTermIV \rVert_1
  + \lVert \E_R \varianceTermI - \sigma^2 \rVert_1.
\end{align*}
De Jong and Davidson (2000) prove that
\begin{equation} \label{dd1}
\lim_{\delta\to0} \limsup_{T\to\infty} \lVert \varianceTermI -
\varianceTermII\rVert_1 = 0,
\end{equation}
\begin{equation} \label{dd2}
\lim_{\delta\to0} \limsup_{T\to\infty} \lVert \varianceTermII - \varianceTermIII
\rVert_1 = 0,
\end{equation}
\begin{equation} \label{dd3}
\lim_{\delta\to0} \limsup_{T\to\infty} \lVert \varianceTermIII - \varianceTermIV
\rVert_1 = 0,
\end{equation}
and
\begin{equation} \label{dd4}
\lim \lVert \E_R \varianceTermI - \sigma^2 \rVert_1 = 0.
\end{equation}
Their proofs of \eqref{dd1}--\eqref{dd4} use the fact that NED
functions of mixing processes are also mixingale processes and do not
use any other properties specific to NED processes, so their results
hold here as well.  We do need to modify their proofs that
\begin{equation}\label{eq:5}
  \lVert \varianceTermIV - \E_R \varianceTermIV \rVert_1 \to 0
\end{equation}
and
\begin{equation}\label{eq:4}
  \lVert \sh^2 - \varianceTermI \rVert_1 \to 0
\end{equation}
for all positive $\delta$, though, since those proofs exploit NED
properties. These results are presented as Lemmas~\ref{lem:a2}
and~\ref{lem:a6} respectively.  \qed


\subsection*{Proof of Theorem \ref{res:oostest}}
Coverage of the confidence intervals is immediate from
Theorem~\ref{res-oost}, so we will present a proof of~\eqref{eq:23}.
By Lemma~\ref{res-convergence}, we know that
\begin{equation*}
  \Pr[\E_R \oosA \leq 0] - \Pr[\E_T \oosB \leq 0] \to 0
\end{equation*}
and
\begin{equation*}
  \Pr[P^{1/2} \oosA / \sh > z_\alpha \text{ and } \E_R \oosA \leq 0]
  - \Pr[P^{1/2} \oosA / \sh > z_\alpha \text{ and }
  \E_T \oosB \leq 0] \to 0.
\end{equation*}
For large enough $T$, both $\Pr[\E_R \oosA \leq 0]$ and $\Pr[\E_T
\oosB \leq 0]$ are positive, so
\begin{equation*}
  \Pr[P^{1/2} \oosA / \sh > z_\alpha
  \mid \E_R \oosA \leq 0] =
  \frac{\Pr[P^{1/2} \oosA / \sh > z_\alpha \text{ and }
    \E_R \oosA \leq 0]}{\Pr[\E_R \oosA \leq 0]}
\end{equation*}
and
\begin{equation*}
  \Pr[P^{1/2} \oosA / \sh > z_\alpha
  \mid \E_T \oosB \leq 0] =
  \frac{\Pr[P^{1/2} \oosA / \sh > z_\alpha \text{ and }
    \E_T \oosB \leq 0]}{\Pr[\E_T \oosB \leq 0]}
\end{equation*}
almost surely and consequently
\begin{equation*}
  \Pr[P^{1/2} \oosA / \sh > z_\alpha
  \mid \E_T \oosB \leq 0]
  - \Pr[P^{1/2} \oosA / \sh > z_\alpha
  \mid \E_R \oosA \leq 0] \to^p 0.
\end{equation*}
Finally,
\begin{align*}
  \Pr[P^{1/2} \oosA / \sh > z_\alpha \mid \E_R \oosA \leq 0]
  &\leq \Pr[P^{1/2} (\oosA - \E_R \oosA) / \sh > z_\alpha
  \mid \E_R \oosA \leq 0] \\
  &= \E( \Pr[P^{1/2} (\oosA - \E_R \oosA) / \sh > z_\alpha
  \mid \mathcal{F}_R] \mid \E_R \oosA \leq 0) \\
  &= \E(\alpha \mid \E_R \oosA \leq 0) \\
  &= \alpha
\end{align*}
completing the proof.
\qed

\subsection*{Proof of Theorem~\ref{res:insample1}}

For both parts, let $(\e_{t+h}, x_{t}) \sim i.i.d. N(0,I)$ and let
$\theta_1 = 0$.

\noindent
\emph{Proof of (\ref{eq:11})}. $\E_T \oosB = 0$ implies that
\begin{equation*}
  \E_T (y_{T+\h+1} - x_{1T+1}'\bh{1T})^2
  = \E_T (y_{T+\h+1} - x_{2T+1}'\bh{2T})^2
\end{equation*}
almost surely, which can be expressed as
\begin{equation*}
  \bh{1T}'\bh{1T} = (\bh{2T} - \theta_2)'(\bh{2T} - \theta_2)
\end{equation*}
almost surely. So, given $\bh{1T}$,

, where $\tilde\theta_{1T}$ is the first $K_1$ elements
of $\bh{2T}$ and $\tilde\theta_{2T}$ is the remaining $K_2 - K_1$, and
$\theta_2^*$ is the corresponding elements of the pseudotrue $\theta_2$.


distribution of $\bh{2T}$ concentrates on and outside the sphere
defined by
\begin{equation*}
  (\bh{2T} - \theta_2)' A (\bh{2T} - \theta_2) = \theta_2' A \theta_2.
\end{equation*}
This sphere is centered at the

If $\theta_2$ and $V_T$ satisfy
\begin{equation*}
  \Pr[\theta_2' V_T \theta_2 > 2 c] \to 1
\end{equation*}
the acceptance region of $\Lambda$ is contained in a cube with

then
\begin{align*}
  \E(\Lambda \mid \E_T \oosB \leq 0)
  &\leq \Pr[ \bh{2T}' V_T \bh{2T} < c \mid \E_T \oosB \leq 0] \\
  &\leq (1/2)^{K_2},
\end{align*}
which converges to zero as $T \to \infty$.

\qed

\bibliographystyle{abbrvnat}
\bibliography{references}
\clearpage

\begin{table}[tb]
  \begin{tabularx}{\linewidth}{XX}
    \toprule
    Category & Variable \\
    \midrule
    Stock market variables
    & Dividend Price ratio (log) \\
    & Earnings Price ratio (log) \\
    & Stock Market Variance \\
    & Book to Market Ratio \\
    & Net Equity Expansion \\
    & Percent Equity Issuing \\\\
    Interest rate variables
    & Treasury Bill rate (3 month) \\
    & Long Term Yield \\
    & Long Term Rate \\
    & Default Return Spread \\
    & Default Yield Spread \\
    & Inflation \\
    \bottomrule
  \end{tabularx}
  \caption{Variables used to predict the equity premium.
    Please see Goyal and Welch's original paper \citep{GoW:08}
    for a detailed description of each variable.}
  \label{tab:equity}
\end{table}

\begin{table}[b]
 \begin{center}
 \begin{tabular}{lrrrr}\hline\hline
   \multicolumn{1}{l}{waldtest}&\multicolumn{1}{c}{Res.Df}&\multicolumn{1}{c}{Df}&\multicolumn{1}{c}{F}&\multicolumn{1}{c}{Pr(\textgreater
     F)}\tabularnewline
   \hline
   1&69&&&\\
   2&81&$-$12&3.4&0.00066 \\
   \hline
\end{tabular}
\caption{Wald test for the null hypothesis that all of the
  coefficients in Goyal and Welch's (2008) ``kitchen sink'' model are
  zero, except for the intercept, using the Newey-West
  variance-covariance matrix.  The p-value is calculated from the $F$
  distribution.\label{tab:gwinsample}}
\end{center}
\end{table}

\clearpage
\begin{figure}
  \centering {\large Simulated Coverage of One-Sided DMW Interval for
    $\E_R\oosA$} \tryinput{floats/mc-interval-testerror1.tex}
\caption{Simulated coverage of $\E_R \oosA$ at 90\% confidence using a
  one-sided interval based on the DMW
  OOS test, plotted as a function of the fraction of
  observations used in the test sample, $P/T$.  The solid horizontal
  line denotes the intervals' nominal coverage.}
 \label{fig:interval-R}
\end{figure}
\clearpage
\begin{figure}
  \centering {\large Simulated Coverage of One-Sided DMW Interval for
    $\E_T\oosB$} \tryinput{floats/mc-interval-generror1.tex}
\caption{Simulated coverage of $\E_T \oosB$ at 90\% confidence using a
  one-sided interval based on the DMW
  OOS, plotted as a function of the fraction of
  observations used in the test sample, $P/T$.  The solid horizontal
  line denotes the intervals' nominal coverage.}
\label{fig:interval-T}
\end{figure}
\clearpage

\begin{figure}
  \centering {\large Simulated Rejection Probability of $F$-test Under
    $\E_T \oosB \leq 0$}
  \tryinput{floats/mc-ftest.tex}
  \caption{Simulated rejection probabilities for the $F$-test given
    $\E_T \oosB \leq 0$ with nominal size 10\%.  Values greater than
    10\% indicate that the test rejects the benchmark model too often.
    See Section~\ref{sec:simulation-design} for a discussion of the
    simulation design.}
  \label{fig:ftest}
\end{figure}
\clearpage
\begin{figure}
  \centering {\large Simulated Rejection Probability of DMW Test Under
    $\E_T \oosB \leq 0$} \tryinput{floats/mc-dmwsize.tex}
  \caption{Simulated rejection probabilities for the DMW
    OOS $t$-test given $\E_T \oosB \leq 0$ with nominal
    size 10\%.  Values greater than 10\% indicate that the test
    rejects the benchmark model too often.  The solid horizontal line
    indicates the nominal rejection probability.  See
    Section~\ref{sec:simulation-design} for a discussion of the
    simulation design.}
  \label{fig:ttest-size}
\end{figure}

\begin{figure}
  \centering {\large Simulated Rejection Probability of Clark and West
    (2006, 2007) \\ Test Under $\E_T \oosB \leq 0$}
  \tryinput{floats/mc-clarkwestsize.tex}
  \caption{Simulated rejection probabilities for Clark and West's
    (2006, 2007) OOS test statistic given $\E_T \oosB
    \leq 0$ with nominal size 10\%.  Values greater than 10\% indicate
    that the test rejects the benchmark model too often.  The solid
    horizontal line indicates the nominal rejection probability.  See
    Section~\ref{sec:simulation-design} for a discussion of the
    simulation design.}
   \label{fig:clarkwest}
\end{figure}

\begin{figure}
  \centering {\large Simulated Rejection Probability of McCracken
    (2007) Test \\ Under $\E_T \oosB \leq 0$}
  \tryinput{floats/mc-mccrackensize.tex}
  \caption{Simulated rejection probabilities for McCracken's (2007)
    OOS test statistic given $\E_T \oosB \leq 0$ with
    nominal size 10\%.  Values greater than 10\% indicate that the
    test rejects the benchmark model too often.  The solid horizontal
    line indicates the nominal rejection probability.  See
    Section~\ref{sec:simulation-design} for a discussion of the
    simulation design.}
  \label{fig:mccracken}
\end{figure}

\begin{figure}
  \centering {\large Simulated Rejection Probability of DMW Test Under
    $\E_T \oosB > 0$} \tryinput{floats/mc-dmwpower.tex}
  \caption{Simulated rejection probabilities for the DMW
    OOS $t$-test given $\E_T \oosB > 0$ with nominal
    size 10\%.  Values greater than 10\% indicate that the test
    rejects the benchmark model too often.  The solid horizontal line
    indicates the nominal rejection probability.  See
    Section~\ref{sec:simulation-design} for a discussion of the
    simulation design.}
  \label{fig:ttest-power}
\end{figure}

\begin{figure}
\centering
\large{Difference in OOS MSE of Prevailing Mean and\\ Kitchen
    Sink Models of Equity Premium (OLS)}
\tryinput{floats/empirics-oos-mse-1.tex}
\tryinput{floats/empirics-oos-mse-1b.tex}
\caption{OOS difference in the MSE
  of the prevailing mean benchmark and the kitchen sink model as a
  function of the test sample size, $R$.  Both models forecast the
  equity premium using annual data from 1928--2008.  The solid line
  gives the OOS average, and the shaded region indicates the
  one-sided 95\% confidence interval implied by the
  DMW test.  The bottom panel is a detailed view of the top
  panel for $R \geq 50$.}
\label{fig:empirics1}
\end{figure}

\begin{figure}
\centering
\large{Difference in OOS MSE of Prevailing Mean and\\ Kitchen
    Sink Models of Equity Premium (CT)}
\tryinput{floats/empirics-oos-mse-2.tex}
\tryinput{floats/empirics-oos-mse-2b.tex}
\caption{OOS difference in the MSE
  of the prevailing mean benchmark and the kitchen sink model as a
  function of the test sample size, $R$.  Both models forecast the
  equity premium using annual data from 1928--2008.  The solid line
  gives the OOS average, and the shaded region indicates the
  one-sided 95\% confidence interval implied by the 
  DMW test.  The bottom panel is a detailed view of the top
  panel for $R \geq 50$.}
\label{fig:empirics2}
\end{figure}

\begin{figure}
\centering
\large{OOS MSE of Individual Forecasts of Equity Premium}
\tryinput{floats/empirics-oos-ind-ks.tex}
\tryinput{floats/empirics-oos-ind-pm.tex}
\caption{OOS MSE of the Prevailing Mean (PM) and
    Kitchen Sink (KS) models for equity premium prediction as
    a function of the size of the training sample, $R$.  Please note
    that the vertical scales are different in the two plots.}
\label{fig:empirics3}
\end{figure}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:

% LocalWords:  McCracken's kilian anatolyev rossi REStud JAppliedEconometrics
% LocalWords:  goyal welch rfs reputational calhoun Berbee's mixingales Rt tS
% LocalWords:  Heteroskedasticity Autocorrelation Tt iT th berbee iS de dx ixz
% LocalWords:  DMW mcleish Rs eq ftestreject lightgray veclen RSQLite zeileis
% LocalWords:  heteroskedasticity hothorn RNews JStatSoftware harrell campbell
% LocalWords:  thompson bivariate Welch's multicollinearity Newey newey tw vw
% LocalWords:  merlevede cltvar dR Scwarz davidson indices lrrrr waldtest Df jt
% LocalWords:  dmw McCracken MSE OLS mse inoue huber elemstatlearning efron sw
% LocalWords:  anova akritas jong normalpart coefpart Jong's rproject rsqlite
% LocalWords:  unrestrictive datasets Heyde's ib oos Mcc MeR StW InK DiM CCS lm
% LocalWords:  ClM CoS ClW GiW GiR GoW Cla HTF Efr BoB AkA AkP DeD Mcl Dej Rde
% LocalWords:  Sar Zeh Zei Har CaT NeW MeP HaH AllRefs Diebold jel Whi RoW brc
% LocalWords:  overrejection overreject fpe JoD iR PeT Giacomini WN Wishart T'V
% LocalWords:  premultiply const overrejects overrejecting BWB Gia Schwarz HHK
% LocalWords:  Haf Anatolyev's nonstochastic T'X Sweave tikzDevice pgfSweave

