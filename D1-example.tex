\section*{Supplemental Appendix (not for publication)}
\markright{Supplemental Appendix}

\noindent%
This appendix contains additional results to accompany
\citet{Cal:15b}. The first section presents a simple example with
i.i.d. observations that illustrates the main paper's theoretical
results. Many of these results can be proven directly under
independence and these proofs continue to demonstrate the key elements
of the main proofs. The second section presents additional Monte Carlo
simulations.

\section{Extended simple example}
\label{sec:example}

Throughout this section, let the DGP and alternative model both be
\[
  y_{t+1} = x_{2t}'\theta_2 + \e_{2,t+1}
\]
with $(x_{2t}, \e_{2,t+1}) \sim i.i.d.\ N(0,I)$ and assume that this
model nests the benchmark model. Also let $L(e) = e^2$
and $h = 1$; assume that $K_1/T \to c_1$ and $K_2/T \to c_t$; and
define
\[
  \sh^2 = (1/P) \oosSum{t}{1} (D_t - \oosA)^2.
\]

The first
subsection shows that the DMW \oost\ test is asymptotically normal. The
second directly shows that $\E_R \oosA$ converges to
$\E_T \oosB$ only when the test sample is small. And the last
subsection demonstrates that the $F$-test does not indicate which
model will be more accurate in the future, even when its finite sample
distribution is known.

\subsection{Asymptotic normality of the OOS average (Lemma 1)}

This subsection proves directly that the DMW $t$-test is
asymptotically normal and can be seen as a special case of
Lemma~\ref{res-mixingale}. Since the underlying observations are
independent, we can use asymptotic arguments for MDSes here instead of
those for mixingales.  This substitution simplifies the proof
considerably and highlights the role of the conditional expectation
and conditional variance in this setting.  In particular, we will show
that
\begin{equation}
  \sh^2 / \var_R(\sqrt{P} \oosA) \to^p 1
\end{equation}
and
\begin{equation}
  \sqrt{P} (\oosA - \E_R \oosA) / \sh \to^d N(0,1).
\end{equation}

By construction,
\begin{equation}\label{eq:3}
  D_t =
  \begin{cases}
    2 \e_{t+1} (x_{2t}'\bh{2R} - x_{1t}'\bh{1R})
    + (x_{2t}'\theta_2 - x_{1t}'\bh{1R})^2
    - (x_{2t}'\theta_2 - x_{2t}'\bh{2R})^2
    & \text{if\ } t < T \\
    2 \e_{t+1} (x_{2t}'\bh{2T} - x_{1t}'\bh{1T})
    + (x_{2t}'\theta_2 - x_{1t}'\bh{1T})^2
    - (x_{2t}'\theta_2 - x_{2t}'\bh{2T})^2
    & \text{if\ } t \geq T.
  \end{cases}
\end{equation}
Since $(y_{t+1}, x_{2t})$ is independent of $\bh{R}$ and $\bh{t}$,
\eqref{eq:3} ensures that
\begin{equation*}
  D_t - \E_{t-1} D_t = D_t - \E_R D_t
\end{equation*}
almost surely and, consequently, that
$\{D_t - \E_R D_t, \Fs_{t}; t=R+1,\dots,T-1\}$ is an MDS.

Moreover, independence also ensures that
$\{g(D_t) - \E_R g(D_t); \Fs_t; t = R+1,\dots,T-1\}$
is an MDS for any measurable $g$ as long as
$g(D_t)$ has finite mean.
As a result, $\oosA$ and $(1/P) \sum_{t=R+1}^{T-1} D_t^2$ both obey
MDS LLNs: $\oosA \to^p \E_R \oosA$ and
\begin{equation*}
  (1/P) \sum_{t=R+1}^{T-1} (D_t^2 - \E_R D_t^2) \to^p 0,
\end{equation*}
implying that $\sh^2/\var_R(\sqrt{P} \oosA) \to^p 0$.
Asymptotic normality,
\begin{equation*}
  \sqrt{P} (\oosA - \E_R \oosA )/\sh \to N(0,1),
\end{equation*}
holds by the MDS CLT.

\subsection{Convergence of $\E_R \oosA$ to the generalization error}

This subsection directly calculates the rate of convergence for
$\E_R \oosA - \E_T \oosB$. From \eqref{eq:3}, this difference can
be expressed as
\begin{align}
    \E_R \oosA - \E_T \oosB &= \Big\{(\bh{1R} - \theta_1)'(\bh{1R} - \theta_1) -
    (\bh{2R} - \theta_2)'(\bh{2R} - \theta_2)\Big\} \notag \\
    & \quad - \Big\{(\bh{1T} - \theta_1)'(\bh{1T} - \theta_1) -
    (\bh{2T} - \theta_2)'(\bh{2T} - \theta_2)\Big\} \\
    &=\ep{T}'\Big\{
      \tilde{X}_{1R}(X_{1R}'X_{1R})^{-2} \tilde{X}_{1R}'
      - X_{1T}(X_{1T}'X_{1T})^{-2} X_{1T}' \notag \\
      & \quad  - \tilde{X}_{2R}(X_{2R}'X_{2R})^{-2} \tilde{X}_{2R}'
      + X_{2T}(X_{2T}'X_{2T})^{-2} X_{2T}'
      \Big\}\ep{T} \label{eq:4}
\end{align}
with $\tilde{X}_{iR}$ the $T \times K_2$ matrix $[X_{iR}'\ 0]'$.  We
can directly calculate the rate that the first two moments of the RHS
of~\eqref{eq:4} converge to zero since~\eqref{eq:4} is a quadratic
form and all of the underlying random variables are normal.

The order of the expected value of this difference is
\begin{equation*}
  \begin{split}
  \E(\E_R \oosA - \E_T \oosB)
      &=O\Big(\max_i \E \tr \big\{
        \tilde{X}_{iR}(X_{iR}'X_{iR})^{-2} \tilde{X}_{iR}
        - X_{iT}(X_{iT}'X_{iT})^{-2} X_{iT} \big\} \Big) \\
      &=O\Big(\max_i \tr \big\{\E (X_{iR}'X_{iR})^{-1}
         - \E (X_{iT}'X_{iT})^{-1} \big\}\Big) \\
      &=O( K_1 P / (R-K_1)(T-K_1)) \\
      &=O(P/T).
    \end{split}
\end{equation*}
The first equality follows from well-known formulas for the
expectation of a quadratic form, the second from routine manipulations
of the trace operator, and the third from the moments of the inverse
Wishart distribution.
Similarly, the order of the variance of the difference is
\begin{align*}
  \var(\E_R & \oosA - \E_T \oosB) \\ &=
  O\Big(\max_i \E\big[\tr((X_{iR}'X_{iR})^{-1} - (X_{iT}'X_{iT})^{-1})\big]^2
  + 2 \tr \E\big((X_{iR}'X_{iR})^{-2} - (X_{iT}'X_{iT})^{-2}\big) \\
  &\qquad - \big[\tr \E((X_{iR}'X_{iR})^{-1} - (X_{iT}'X_{iT})^{-1}) \big]^2 \Big)\\
  &= O(P/T)^2.
\end{align*}
Consequently, $P^{1/2}(\E_R \oosA - \E_T \oosB) \to^p 0$ if
$P^3/T^2 \to 0$ for this DGP, which is slightly weaker than the
general requirement for Lemma~\ref{res-convergence} that
$P^2/T \to 0$. When this condition is violated, i.e. if
$\lim P^3/T^2 > 0$, the results from the previous subsection continue
to hold and the OOS average still obeys the MDS CLT. However, it is
centered on $\E_R \oosA$ and not $\E_T \oosB$.

\subsection{Behavior of the F-test}

This subsection shows that, in this example, the F-test fails to test
hypotheses about the generalization error.
To further simplify the presentation, assume that the full-sample
design matrix is orthogonal in-sample, not just in population, so
$X_{2T}'X_{2T} = T \cdot I_{K_2 \times K_2}$, and let $M_1$ and
$M_2$ be the selection matrices for the first $K_1$ and the last
$K_2-K_1$ elements of $\theta_2$.

Under these additional assumptions,
$M_2 \bh{2T} \sim N(M_2 \theta_2, (1/T) \, I_{K_2-K_1})$. More
importantly, given $\E_T \oosB = - c$ for any
$c > -\theta_2'M_2'M_2\theta_2$, the estimator $M_2 \bh{2T}$ is
distributed uniformly on the cylinder in $\Re^k$ defined by
\begin{equation*}
  \{\bh{2T} : (M_2 \bh{2T} - M_2 \theta_2)'(M_2 \bh{2T} - M_2 \theta_2) =
  \theta_2'M_2' M_2 \theta_2 + c\},
\end{equation*}
which is centered at $M_2\theta_2$. When $\E_T \oosB = 0$ this
cylinder passes through the origin.  Values of $\bh{2T}$ that satisfy
with the hypothesis $\E_T \oosB \leq 0$, i.e. that the smaller model
is more accurate than the larger model, lie outside that cylinder and
values of $\bh{2T}$ satisfying $\E_T \oosB > 0$ lie inside that
cylinder.

The F statistic for testing the null hypothesis $M_2'\theta = \eta$
can be expressed as
\begin{equation*}
  F = \tfrac{T}{s^2 (K_2 - K_1)} (\bh{2T}' M_2' - \eta') (M_2 \bh{2T} - \eta)
\end{equation*}
and $\sqrt{T}(F - 1)$ is asymptotically normal---its numerator is
Chi-squared with $K_2-K_1$ degrees of freedom and obeys the CLT and
its denominator obeys the LLN.
Consequently, this test will fail to reject the null hypothesis that
$M_2 \theta_2 = 0$ when it satisfies the inequality
\begin{equation}\label{eq:26}
  \tfrac{T}{s^2 (K_2 - K_1)} \bh{2T}' M_2' M_2 \bh{2T}
  \leq 1 + \delta / \sqrt{T},
\end{equation}
where $\delta$ is chosen to determine the size of the test. In other
words, the test fails to reject its implied null hypotheses whenever
$M_2 \bh{2T}$ falls in the sphere centered at the origin with radius
$s ((K_2 - K_1) / T)^{1/2} + O_p(1/\sqrt{T})$.

Consequently, when $M_2\theta_2$ is in the rejection region of the
F-test, the probability that the test will reject given
$\E_T \oosB \leq 0$ is necessarily greater than 50\%, with the
probability increasing as the length of $M_2\theta_2$ increases. So
$\theta_2$ can be chosen to make the rejection probability given
$\E_T \oosB \leq 0$ arbitrarily close to 1.

Similarly, when $2 M_2\theta_2$ is in the F-test's acceptance region,
then the acceptance region contains all of the parameter estimates
satisfying $\E_T \oosB > 0$---i.e. all of the parameter estimates that
would make the larger model forecast more accurately. So $\theta_2$
can also be chosen to make the conditional rejection probability given
$\E_T \oosB > 0$ have any value between $0$ and the nominal size of
the test.

\input{floats/circles}
\begin{figure}[t!]
  \centering
  \begin{tabular}{cc}
  \subfloat[]{\circlefigA{1}{2.5}{1.4}{4.5}\label{fig:circleA}} &
  \subfloat[]{\circlefigB{1}{2.5}{1.4}{4.5}\label{fig:circleB}}
  \end{tabular}
  \caption{Graphs indicating the rejection region and region of equal
    generalization error for the models discussed in Section
    \ref{sec:insample}.
    In Figure (a), the union of
    the two shaded areas shows the parameter estimates consistent
    with the null hypothesis $\E_T \oosB \leq 0$ and in Figure (b), the union of
    the two shaded areas shows the parameter estimates consistent
    with the alternative hypothesis $\E_T \oosB > 0$. The
    dark gray circles are contour lines for the sampling density of
    $M_2 \bh{2T}$, at increments of 0.01. In both figures, the lighter
    region shows the rejection region of the F-test and the darker
    region shows its acceptance region.
    Here $e_1 = (1,0)$ and $e_2 = (0,1)$ are the first and second
    selection vectors, so the horizontal axes represent changes in the
    first unique element of $\bh{2T}$ and the vertical axes represent
    changes in the second unique element.}
\label{fig:rreject}
\end{figure}

These relationships are perhaps best illustrated with a picture.
Figure~\ref{fig:rreject} (a) plots these quantities when $K_2 - K_1 =
2$. The black circle centered at the origin plots the threshold for the
$F$-test; when $M_2 \bh{iT}$ falls outside this circle, the $F$-test
rejects. The black circle centered at $M_2 \theta_2$ plots the set of points
for which $\E_T \oosB = 0$. The lighter shaded region in
Figure~\ref{fig:rreject}~(a) plots the rejection region given $\E_T
\oosB \leq 0$ and the darker shaded region in Figure~\ref{fig:rreject}~(b)
plots the acceptance region given $\E_T \oosB \leq 0$. The dark
gray lines in each figure represent contour lines for the sampling
density of $M_2 \bh{2T}$ given rejection or acceptance.

In the picture, the true coefficients $\theta_2$ satisfy
\begin{equation*}
\theta_2'M_2'M_2\theta_2 > \var(\e_t)^{1/2} ((K_2 - K_1) / T)^{1/2}
\end{equation*}
so the center of the conditional distribution of $\bh{2T}$ given
$\E_T \oosB \leq 0$ is outside the acceptance region of the
$F$-test. As can be seen from the figure, the F-test has a rejection
probability greater than 50\% when $\E_T \oosB \leq 0$ holds, since
most of the density of $M_2 \bh{T2}$ is not close to the origin. By
moving $M_2 \theta$ to other points outside the acceptance region of
the F-test, we can make its rejection probability under this null
arbitrarily close to one. Similarly, by moving $M_2 \theta_2$ to
points inside the F-test's acceptance region, we can make its
power to reject the hypothesis $\E_T \oosB \leq 0$ when it is
false arbitrarily close to zero.

In summary, the F-test can not be used for inference on the models'
forecasting performance. It does not control size for the null
$\E_T \oosB \leq 0$ and it has arbitrarily low power to detect the
alternative $\E_T \oosB > 0$ for some parameter values. Full-sample
model selection statistics like the AIC and BIC, which add or discard
covariates based on how far their coefficient estimates are from zero,
behave similarly.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
