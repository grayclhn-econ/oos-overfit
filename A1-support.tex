\section*{Appendix: mathematical details}
\markright{Appendix: mathematical details}
\setcounter{section}{1}
% Change lemma style to prepend 'A'
\setcounter{lem}{0}
\renewcommand{\thelem}{A\arabic{lem}}

\subsection{Supporting results}
The results in this paper rely heavily on a coupling argument for
absolutely regular sequences, Berbee's Lemma \citep{Ber:79}.  Many of
the results of this paper (Lemma \ref{res-mixingale} and
Theorem~\ref{res-oost}) use modifications of existing results
for NED functions of mixing processes by \citet{Jon:97} and
\citet{JoD:00}; this coupling argument is used to explicitly derive
inequalities that arise naturally for NED processes.  Lemma
\ref{lem-basic-coupling} establishes these inequalities, which are
based on a proposition of \citet{MeP:02}.

We present \citeauthor{MeP:02}'s
(\citeyear{MeP:02}) statement of Berbee's Lemma for
the reader's reference.  In the following Lemma, $\beta(X,Y)$ is the
coefficient of absolute regularity:
\begin{equation}
\beta(X,Y) = \sup_{A \in \sigma(Y)} \E \lvert \Pr(A \mid \sigma(X))
  - \Pr(A) \rvert.
\end{equation}

\begin{lem}\label{lem-berbee}\quad

\begin{quotation}\noindent
  Let $X$ and $Y$ be random variables defined on a probability space
  $(\Omega, \mathcal{T}, \Pr)$ with values in a Polish space
  $S$.  Let $\sigma(X)$ be a $\sigma$-algebra generated by $X$ and let
  $U$ be a random variable uniformly distributed on $[0,1]$
  independent of $(X,Y)$.  Then there exists a random variable $Y^{*}$
  measurable with respect to $\sigma(X) \vee \sigma(Y) \vee
  \sigma(U)$, independent of $X$ and distributed as $Y$, and such that
  $\Pr(Y \neq Y^{*}) = \beta(X,Y)$.

  \noindent\citep{MeP:02}
\end{quotation}
\end{lem}

The advantage of this result over coupling arguments that use other
forms of weak dependence is that the difference between the original
variable, $Y$, and the new variable, $Y^{*}$, does not depend on their
dimension.  Similar results for strong mixing sequences depend on the
dimension of $Y$, which makes them unsuitable for this paper.

\begin{lem}\label{lem-extend-mp}
  Suppose that $X$ and $X^*$ are $L_p$-bounded random variables, with
  $p > 2$, that satisfy ${\Pr[X \neq X^*] = c}$.  Then
  \begin{equation}
    \lVert X - X^* \rVert_2 \leq 2^{1/p} (\lVert X \rVert_p + \lVert
    X^* \rVert_p) c^{(p-2)/2p}
  \end{equation}
\end{lem}

The proof is virtually identical to the proof of Proposition 2.3 in
\citet{MeP:02} and is omitted.

\begin{lem}\label{lem-basic-coupling}
  Suppose Assumptions \ref{asmp-1}--\ref{asmp-3} hold.  Then, for any
  T, $s$, $t$, and $u$ with $s < t \leq u$, there exist random
  variables $D_t^*,\dots,D_u^*$ such that
  \begin{equation}\label{eq:coupling1}
    P[(D_{t}^*,\dots,D_u^*) \neq (D_{t},\dots,D_u)] \leq \beta_{t-s}
  \end{equation}
  and
  \begin{equation}\label{eq:coupling2}
    \E(\phi(D_{t}^*,\dots, D_u^*) \mid \mathcal{F}_s ) =
    \int
    \phi(D_{t},\dots,D_u) f(\mathbf{x}, \mathbf{y})\ d\mathbf{x}\ d\mathbf{y}
  \end{equation}
  almost surely for all measurable functions $\phi$ such that the
  expectations are finite, where
  \[ \mathbf{x} = (x_{t}', \dots, x_{u}')', \qquad
  \mathbf{y} = (y_{t+\h},\dots,y_{u+\h})',\] and $f$ is the
  joint density of $(\mathbf{x}, \mathbf{y})$.  Moreover,
 \begin{equation}\label{eq:coupling3}
   \| D_v^* - D_v \|_2 \leq 2^{1+1/\rho} B_L
   \beta_{t-s}^{\rhoExp}, \qquad v = t,\dots,u.
 \end{equation}
\end{lem}

\begin{proof}
  The proof follows as a consequence of Lemmas \ref{lem-berbee} and
  \ref{lem-extend-mp}.  Let $l = u-t$.  For any fixed values of $l$
  and $T$, the sequence of vectors
  \[ V_{t} = (y_{t+\h}, x_{t}', \dots, y_{t+l+\h}, x_{t+l}') \] is
  absolutely regular of size $\rho/(\rho-2)$.  Berbee's Lemma implies
  that there is a random vector $V^*$ that is independent of
  $\mathcal{F}_s$, equal to $V_{t}$ in distribution, and satisfies
  \[\Pr[V^* \neq V_{t}] = \beta_{t-s}.\]

  Now define
  \begin{equation*}
    D_v^* =
    \begin{cases}
      L(y_{v+\h}^* - x_{1v}^{*\prime} \hat{\theta}_{1R}) - L(y_{v+\h}^* -
      x_{2v}^{*\prime} \hat{\theta}_{2R}) & v \leq T \\
      L(y_{v+\h}^* - x_{1v}^{*\prime} \hat{\theta}_{1T}) - L(y_{v+\h}^* -
      x_{2v}^{*\prime} \hat{\theta}_{2T}) & v > T
    \end{cases}
  \end{equation*}
  with $y_{v+\h}^*$ and $x_{iv}^*$ denoting the elements of $V^*$
  corresponding to $y_{v+\h}$ and $x_{iv}$ in $V_{t}$.  Equations
  (\ref{eq:coupling1}) and (\ref{eq:coupling2}) are satisfied by
  construction, and (\ref{eq:coupling3}) follows from Lemma
  \ref{lem-extend-mp}.
\end{proof}

\begin{lem}\label{lem:a2}
  Suppose Assumptions~\ref{asmp-1}--\ref{asmp-3} hold.  Let $b_T$ be a
  sequence of integers such that $b_T \to \infty$ and $b_T = o(P)$ and define
  \begin{equation}
    Z_i = \ZDef.
  \end{equation}
  Then
  \begin{equation}
    \label{eq:10}
    \sum_{i=1}^{\lfloor P/b_T \rfloor} (\E_R Z_i^2 - \E_{R+(i-1) b_T}
    Z_i^2) \to^p 0 \quad\text{ as } P \to \infty.
  \end{equation}

  If the assumptions of Theorem~\ref{res-oost} also hold, $b_T$ is
  restricted further so that $b_T \equiv \lfloor \gamma/\delta
  \rfloor$ for some positive scalar $\delta$, and we define
  \begin{equation}\label{eq:13}
    \kernelB{x} \equiv \kernelBDefn{x},
  \end{equation}
  then
  \begin{equation}
    \label{eq:12}
    \vtSum (Z_{1t} Z_{2t} - \E_R Z_{1t} Z_{2t}) \to^p 0.
  \end{equation}
  where
  \begin{gather}
    Z_{1t} = \varianceTermIIIa,%
    \intertext{and}%
    Z_{2t} = \varianceTermIVb.
  \end{gather}
\end{lem}
\begin{proof}
\newcommand{\UFiltration}[1]{\ensuremath{\mathcal{F}_{(#1)b_{T}+R-P}}}%
The first result,~\eqref{eq:10}, follows a similar argument to Lemma~5
of \citet{Jon:97} and~\eqref{eq:12} to Lemma~A.4 of \citet{JoD:00}.
Since these arguments are similar and our modification is the same for
both, we'll just present the more complicated version,~\eqref{eq:12}.

Note that $\{Z_{1t}^2 P\gamma_T/b_T\}$ and $\{Z_{2t}^2 P\gamma_T/b_T\}$
are uniformly integrable.  As in~\citet[Lemma A.4]{JoD:00}, we can
assume that there is a constant $C$ such that $Z_{1t}$ and $Z_{2t}$
are bounded in absolute value by $C\sqrt{b_T/P\gamma_T}$; uniform
integrability ensures that the difference between the unbounded random
variables and these truncated versions is negligible for large enough
values of $C$.

Let $r = \lfloor 3P/2b_T \rfloor$ and rewrite the summation as
\begin{align*}
  \vtSum \vtIIIsummand &= \vtSumr \vtSuma \vtIIIsummand \\
  &\quad+ \vtSumr \vtSumb \vtIIIsummand \\
  &\quad+ \sum_{t=r b_T - P + R + 1}^{2P+R} \vtIIIsummand \\
  &\equiv \vtSumr (U_i - \E_R U_i) + \vtSumr (U_i' - \E_R U_i') + o_{L_1}(1).
\end{align*}
The proof then holds if we can show that both $U_i$ and $U_i'$ obey
LLNs.  We'll do so by proving that $\{U_i -\E_R U_i,
\UFiltration{2i-1}\}$ and $\{U_i' - \E_R U_i', \UFiltration{2i}\}$ are
$L_2$-mixingales of size $-1/2$ and using the bound $\E(\vtSumr (U_i -
\E_R U_i))^2 = O(\vtSumr c_i^2)$ where the $c_i$ are the mixingale
magnitude indices \citep{Mcl:75}.

For non-negative $m$, we have
\begin{equation*}
U_i - \E_R U_i \in \UFiltration{2i+2m-1},
\end{equation*}
establishing half of the mixingale result trivially.  Now fix $i$ and $m >
0$ and use Lemma \ref{lem-basic-coupling} to define $D_{ts}^*$
for each $t =(2i-2)b_T-P+R+1,\dots,(2i-1)b_T-P+R$ and $s =
\max(t-b_T,R+1),\dots,\min(t+b_T,T-\h)$ such that
\begin{equation*}
 \E_R D_{ts}^* = \E_{(2i-2m-1)b_T+R-P} D_{ts}^* \quad a.s.
\end{equation*}
and
\begin{equation*}
  \lVert D_{ts}^* - D_s \rVert_2 \leq \couplingBound{s - (2i-2m-1)b_T+P}.
\end{equation*}
Also define
\begin{equation*}
  Z_{1t}^* = (P\gamma_T)^{-1/2} \sum_{l=\vttLower}^{\vttUpper}
  (D_{t,t+l}^* - \E_R D_{t,t+l}^*)\ W(l/\gamma_T), \\
\end{equation*}
and
\begin{equation*}
Z_{2t}^* = (P\gamma_T)^{-1/2} \sum_{j=\vttLower}^{\vttUpper}
    (D_{t,t+l}^* - \E_R D_{t,t+l}^*)\ \kernelB{j/\gamma_T}.
\end{equation*}

Now, we have the inequalities
\begin{align*}
  \lVert \E( U_i - \E_R U_i & \mid \UFiltration{2i-2m-1}) \rVert_2
  \\ &\leq
  \vtSuma \lVert \E(Z_{1t}Z_{2t} \mid \UFiltration{2i-2m-1}) - \E_R
  Z_{1t}Z_{2t} \rVert_2 \\
  &= \vtSuma \lVert \E(Z_{1t}Z_{2t} \mid \UFiltration{2i-2m-1}) \\
  &\quad - \E(Z_{1t}^* Z_{2t}^* \mid \UFiltration{2i-2m-1}) \\
  &\quad + \E(Z_{1t}^* Z_{2t}^* \mid \UFiltration{2i-2m-1})
  - \E_R Z_{1t}Z_{2t} \rVert_2 \\
%  \leq \vtSuma (\wall \lVert \wall \E(Z_{1t}Z_{2t} \mid \UFiltration{2i-2m-1}) \\
%  - \E(Z_{1t}^*Z_{2t}^* \mid \UFiltration{2i-2m-1}) \rVert_2\return
%  \\ + \lVert \E_R Z_{1t}Z_{2t} - \E_R Z_{1t}^*Z_{2t}^*
%  \rVert_2)\return \\
  &\leq 2 \vtSuma \lVert Z_{1t} Z_{2t} - Z_{1t}^* Z_{2t}^*
  \rVert_2 \\
  &\leq 2 \vtSuma (\lVert Z_{1t} - Z_{1t}^* \rVert_2 \lVert
  Z_{2t} \rVert_{\infty}
  + \lVert Z_{2t} - Z_{2t}^* \rVert_2 \lVert Z_{1t}^* \rVert_{\infty}) \\
  &\leq \frac{2 C b_T^{1/2}}{(P\gamma_T)^{1/2}} \vtSuma (\lVert Z_{1t} - Z_{1t}^* \rVert_2
  + \lVert Z_{2t} - Z_{2t}^* \rVert_2).
\end{align*}

And we can finish the proof with the following inequalities:
\begin{align*}
  \frac{2 C b_T^{1/2}}{(P \gamma_T)^{1/2}} &\vtSuma \lVert Z_{1t} - Z_{1t}^* \rVert_2 \\& \leq
  \frac{4C b_T^{1/2}}{P\gamma_T} \vtSuma \sum_{l=\vttLower}^{\vttUpper} \lVert
  D_{t+l} - D_{t,t+l}^* \rVert_2 W(l/\gamma_T) \\&
  \leq O\Bigg(\frac{b_T^{1/2}}{P\gamma_T}\Bigg) \vtSuma
  \sum_{l=\vttLower}^{\vttUpper} \couplingBeta{t+l-(2i-2m-1)b_T+P}\\&
  = O\Bigg(\frac{b_T^{1/2}}{P\gamma_T}\Bigg) O(b_T^{3/2-u}\, m^{-1/2-u})
\end{align*}
for some positive $u$.  The same argument holds for $Z_{2t}$.
As a result,
\[
E\Big(\vtSumr (U_i - \E_R U_i)\Big)^2 = o\Big(\vtSumr b_T^2/P\gamma_T\Big) =
 o(b_T/\gamma_T) \to 0,
\]
as required.
\end{proof}

\begin{lem}\label{lem:a6}
  Suppose the conditions of Theorem~\ref{res-oost} hold.
  Then
  \begin{equation}
    \sh^2 - P^{-1} \sum_{s,t=R+1}^{T-h} (D_s - \E_R
    D_s) (D_t - \E_R D_t) W((t-s)/\gamma) \to^{L_1} 0.
  \end{equation}
\end{lem}

\begin{proof}
   It follows from simple algebra that
\begin{multline*}
  \Big| \sh^2 -  P^{-1} \sum_{s,t=R+1}^{T-h} (D_s - \E_R
    D_s) (D_t - \E_R D_t) W((t-s)/\gamma) \Big| \leq\\
  \varianceDiffA \\ + P^{-1} \oosSum{s,t}{1} \lvert (D_s -
  \oosA)(\E_R D_t - \oosA) \rvert \vWeight + o_p(1).
\end{multline*}
We'll prove that these two sums are $o_p(1)$; uniform integrability
then implies convergence in $L_1$.  The arguments for each are almost
identical, so we'll only present the first.

Applying the Cauchy-Schwarz inequality twice and simplifying gives the
upper bound
\begin{multline*}
\varianceDiffA \\ \leq O(1) \Big[\varianceDiffAii\Big]^{1/2} \Big[\varianceDiffAi\Big]^{1/2}.
\end{multline*}
Now,
$P^{-1} \oosSum{s}{1}(D_s - \E_R D_s)^2 = O_p(1)$, and it suffices to prove
that \[\varianceDiffAi = o_p(1).\]  Observe that
\begin{align*}
  \varianceDiffAi & = O_p\Big(P^{-1} \oosSum{s}{1}(\E_{R}D_s -
  \E_R\oosA)^2\Big) + O_p(\oosA - \E_R\oosA)^{2} \\
  &= O_p\Big(P^{-1} \oosSum{s}{1} (\E_RD_s)^2 - (\E_R\oosA)^2\Big) +
  o_p(1),
\end{align*}
with the second term $o_p(1)$ by Lemma \ref{res-mixingale} and
\citepos{Dav:93} mixingale LLN.

Now define $D_s^*$, $s = R+1,\dots,T-\h$, as in Lemma
\ref{lem-basic-coupling} so that $\E_{s-1} D_s^* = \E_R D_s^*$ almost
surely.  Note that we also have the equality $\E_R D_s^* = \E_R
D_{R+1}^*$ almost surely for all $s\geq R+1$, and so
\[
P^{-1} \oosSum{s}{1} (\E_R D_s^*)^2 = \Big(P^{-1} \oosSum{s}{1} \E_R
D_s^*\Big)^2 \quad \text{a.s.}
\]
Consequently,
\begin{align*}
P^{-1} \oosSum{s}{1}& (\E_R D_s)^2 - (\E_R\oosA)^2 \\
&= P^{-1} \oosSum{s}{1} [(\E_R D_s)^2 - (\E_R D_s^*)^2]
 + \Big(P^{-1} \oosSum{s}{1} \E_R D_s^*\Big)^2 - (\E_R\oosA)^2 \quad \text{a.s}.\\
&= O_p\Big(P^{-1} \oosSum{s}{1} [(\E_R D_s)^2 - (\E_R D_s^*)^2]\Big)
 + O_p\Big(P^{-1} \oosSum{s}{1} \E_R (D_s - D_s^*)\Big).
\end{align*}
Finally,
\begin{align*}
\Big\lVert P^{-1} \oosSum{s}{1} [(\E_RD_s)^2 - (\E_R D_s^*)^2] \Big\rVert_1
&\leq P^{-1} \oosSum{s}{1} \lVert \E_R (D_s - D_s^*) \rVert_2 \lVert
\E_R (D_s + D_s^*) \rVert_2
\\ &\leq (4 B_L/P) \oosSum{s}{1} \lVert \E_R (D_s - D_s^*) \rVert_2,
\end{align*}
and this last term vanishes as in the proof of Lemma~\ref{lem:a2},
completing the proof.
\end{proof}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
